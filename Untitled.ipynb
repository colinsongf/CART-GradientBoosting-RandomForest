{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intersection(s1, s2):\n",
    "    s = set(s2)\n",
    "    return filter(lambda x: x in s, s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, depth, max_depth, impurity, is_leaf=False):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.impurity = impurity\n",
    "        \n",
    "    def stopping_criteria(self, X, y):\n",
    "        if self.impurity == 'gini':\n",
    "            return self.depth >= self.max_depth or np.unique(y).shape[0] == 1\n",
    "        else:\n",
    "            return self.depth >= self.max_depth\n",
    "    \n",
    "    def compute_gini_impurity(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        prob = np.array(Counter(y).values()) / float(n)\n",
    "        \n",
    "        impurity = 1. - np.sum(prob**2)        \n",
    "        return impurity\n",
    "    \n",
    "    def compute_mse_impurity(self, X, y):\n",
    "        n = y.shape[0]\n",
    "        impurity = np.var(y) * n\n",
    "        return impurity\n",
    "    \n",
    "    def compute_impurity(self, X, y):\n",
    "        if self.impurity == 'gini':\n",
    "            impurity = self.compute_gini_impurity(X, y)\n",
    "        else:\n",
    "            impurity = self.compute_mse_impurity(X, y)\n",
    "        return impurity\n",
    "    \n",
    "    def get_predicate(self, X, y, table, indices):\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        best_pair = None\n",
    "        best_impurity = None\n",
    "        best_left = None\n",
    "        best_right = None\n",
    "        \n",
    "        for i in np.arange(m):\n",
    "            t = time.time()\n",
    "            table_i = intersection(table[:, i], indices)\n",
    "            \n",
    "            \n",
    "            new_impurity = 0\n",
    "            \n",
    "            ys_left = np.zeros(np.unique(y[indices]).shape)\n",
    "            ys_right = np.array(Counter(y[indices]).values())\n",
    "            \n",
    "            s_left = 0\n",
    "            s_right = indices.shape[0]\n",
    "            \n",
    "            sqs_left = 0\n",
    "            sqs_right = np.sum(ys_right**2)\n",
    "            \n",
    "            n = indices.shape[0]\n",
    "            \n",
    "            left_impurity = 0\n",
    "            right_impurity = self.compute_impurity(X[indices], y[indices])\n",
    "            \n",
    "            for ind, k in enumerate(table_i[:-1]):\n",
    "\n",
    "                s_left  += 1\n",
    "                s_right -= 1\n",
    "                sqs_left  += 2 * ys_left[y[k]] + 1\n",
    "                sqs_right -= 2 * ys_right[y[k]] - 1                \n",
    "            \n",
    "                ys_left [y[k]] += 1\n",
    "                ys_right[y[k]] -= 1\n",
    "                \n",
    "                if X[table_i[ind+1], i] > X[k, i]:\n",
    "                    if self.impurity == 'gini':\n",
    "                        #left_impurity  = 1. - (s_left**2 * (1. - left_impurity) + 2 * ys_left[y[k]] + 1) / (s_left + 1)**2\n",
    "                        #right_impurity = 1. - (s_right**2 * (1. - right_impurity) - 2 * ys_right[y[k]] + 1) / (s_right - 1)**2\n",
    "                        left_impurity  = 1. - float(sqs_left) / s_left**2\n",
    "                        right_impurity = 1. - float(sqs_right) / s_right**2\n",
    "\n",
    "                        #new_impurity = (s_left * left_impurity + s_right * right_impurity) / n\n",
    "                        new_impurity = (s_left * left_impurity + s_right * right_impurity)\n",
    "\n",
    "                    if best_pair is None or new_impurity < best_impurity:\n",
    "                        t = (X[k, i] + X[table_i[ind+1], i]) / 2\n",
    "                        best_pair = (i, t)\n",
    "                        best_impurity = new_impurity\n",
    "                        best_left = table_i[:ind+1]\n",
    "                        best_right = table_i[ind+1:]\n",
    "\n",
    "        return best_pair, np.array(best_left), np.array(best_right)\n",
    "        \n",
    "    def fit(self, X, y, table, indices):\n",
    "        \n",
    "        if self.stopping_criteria(X[indices], y[indices]):\n",
    "            self.is_leaf = True\n",
    "            if self.impurity == 'gini':\n",
    "                self.c = Counter(y[indices]).most_common()[0][0]\n",
    "            else:                \n",
    "                self.average = np.mean(y[indices])\n",
    "        else:\n",
    "            t = time.time()\n",
    "            predicate, best_left, best_right = self.get_predicate(X, y, table, indices)\n",
    "            print \"Split: \", time.time() - t\n",
    "            \n",
    "            if predicate is None:\n",
    "                self.is_leaf = True\n",
    "                if self.impurity == 'gini':\n",
    "                    self.c = Counter(y[indices]).most_common()[0][0]\n",
    "                else:\n",
    "                    self.average = np.mean(y[indices])\n",
    "                return\n",
    "            \n",
    "            (self.feature, self.threshold) = predicate\n",
    "                \n",
    "            self.left = Node(self.depth + 1, max_depth=self.max_depth, impurity=self.impurity)\n",
    "            self.left.fit(X, y, table, best_left)\n",
    "            \n",
    "            self.right = Node(self.depth + 1, max_depth=self.max_depth, impurity=self.impurity)\n",
    "            self.right.fit(X, y, table, best_right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.is_leaf:\n",
    "            if self.impurity == 'gini':\n",
    "                return [self.c for _ in X]\n",
    "            else:            \n",
    "                return [self.average for _ in X]\n",
    "        else:\n",
    "            pred = []\n",
    "            for x in X:      \n",
    "                if x[self.feature] <= self.threshold:\n",
    "                    pred.extend(self.left.predict(np.array([x])))\n",
    "                else:\n",
    "                    pred.extend(self.right.predict(np.array([x])))\n",
    "            return pred\n",
    "        \n",
    "class CART:\n",
    "    def __init__(self, impurity='gini', max_depth=5):\n",
    "        if impurity not in ['gini', 'mse']:\n",
    "            raise ValueError(\"Only gini and mse criteria are supported\")\n",
    "        self.impurity = impurity\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        table = X.argsort(axis=0) # Sort objects by each feature and return indices\n",
    "        indices = np.arange(X.shape[0])\n",
    "        \n",
    "        self.root = Node(1, max_depth=self.max_depth, impurity=self.impurity)\n",
    "        self.root.fit(X, y, table, indices)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.array(self.root.predict(X))\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td> 7.0</td>\n",
       "      <td> 3.2</td>\n",
       "      <td> 4.7</td>\n",
       "      <td> 1.4</td>\n",
       "      <td> Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td> 6.4</td>\n",
       "      <td> 3.2</td>\n",
       "      <td> 4.5</td>\n",
       "      <td> 1.5</td>\n",
       "      <td> Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td> 6.9</td>\n",
       "      <td> 3.1</td>\n",
       "      <td> 4.9</td>\n",
       "      <td> 1.5</td>\n",
       "      <td> Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td> 5.5</td>\n",
       "      <td> 2.3</td>\n",
       "      <td> 4.0</td>\n",
       "      <td> 1.3</td>\n",
       "      <td> Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td> 6.5</td>\n",
       "      <td> 2.8</td>\n",
       "      <td> 4.6</td>\n",
       "      <td> 1.5</td>\n",
       "      <td> Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal_length  sepal_width  petal_length  petal_width            class\n",
       "50           7.0          3.2           4.7          1.4  Iris-versicolor\n",
       "51           6.4          3.2           4.5          1.5  Iris-versicolor\n",
       "52           6.9          3.1           4.9          1.5  Iris-versicolor\n",
       "53           5.5          2.3           4.0          1.3  Iris-versicolor\n",
       "54           6.5          2.8           4.6          1.5  Iris-versicolor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('iris.data', delimiter=',')[50:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = {\n",
    "    \"Iris-setosa\": 0,\n",
    "    \"Iris-versicolor\": 1,\n",
    "    \"Iris-virginica\": 0\n",
    "}\n",
    "\n",
    "df[\"num_class\"] = df[\"class\"].apply(lambda s: classes[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>num_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td> 100.000000</td>\n",
       "      <td> 100.000000</td>\n",
       "      <td> 100.000000</td>\n",
       "      <td> 100.000000</td>\n",
       "      <td> 100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>   6.262000</td>\n",
       "      <td>   2.872000</td>\n",
       "      <td>   4.906000</td>\n",
       "      <td>   1.676000</td>\n",
       "      <td>   0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>   0.662834</td>\n",
       "      <td>   0.332751</td>\n",
       "      <td>   0.825578</td>\n",
       "      <td>   0.424769</td>\n",
       "      <td>   0.502519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>   4.900000</td>\n",
       "      <td>   2.000000</td>\n",
       "      <td>   3.000000</td>\n",
       "      <td>   1.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>   5.800000</td>\n",
       "      <td>   2.700000</td>\n",
       "      <td>   4.375000</td>\n",
       "      <td>   1.300000</td>\n",
       "      <td>   0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>   6.300000</td>\n",
       "      <td>   2.900000</td>\n",
       "      <td>   4.900000</td>\n",
       "      <td>   1.600000</td>\n",
       "      <td>   0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>   6.700000</td>\n",
       "      <td>   3.025000</td>\n",
       "      <td>   5.525000</td>\n",
       "      <td>   2.000000</td>\n",
       "      <td>   1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>   7.900000</td>\n",
       "      <td>   3.800000</td>\n",
       "      <td>   6.900000</td>\n",
       "      <td>   2.500000</td>\n",
       "      <td>   1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width   num_class\n",
       "count    100.000000   100.000000    100.000000   100.000000  100.000000\n",
       "mean       6.262000     2.872000      4.906000     1.676000    0.500000\n",
       "std        0.662834     0.332751      0.825578     0.424769    0.502519\n",
       "min        4.900000     2.000000      3.000000     1.000000    0.000000\n",
       "25%        5.800000     2.700000      4.375000     1.300000    0.000000\n",
       "50%        6.300000     2.900000      4.900000     1.600000    0.500000\n",
       "75%        6.700000     3.025000      5.525000     2.000000    1.000000\n",
       "max        7.900000     3.800000      6.900000     2.500000    1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(df, train_percent=0.8):\n",
    "    X = np.copy(df.values)\n",
    "    np.random.shuffle(X)\n",
    "    \n",
    "    X, y = X[:, :-1], X[:, -1]\n",
    "    \n",
    "    train_size = int(X.shape[0] * train_percent)\n",
    "    \n",
    "    X_train, y_train = X[:train_size, :], y[:train_size]\n",
    "    X_test, y_test = X[train_size:, :], y[train_size:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 60; Test size: 40;\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = shuffle(df.drop(\"class\", axis=1), train_percent=0.6)\n",
    "print \"Train size: {}; Test size: {};\".format(X_train.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57 58 59]\n",
      "1 57\n",
      "2 20\n",
      "3 8\n",
      "4 34\n",
      "5 37\n",
      "6 2\n",
      "7 30\n",
      "8 49\n",
      "9 44\n",
      "10 43\n",
      "11 56\n",
      "12 48\n",
      "13 58\n",
      "14 38\n",
      "15 50\n",
      "16 55\n",
      "17 59\n",
      "18 17\n",
      "19 28\n",
      "20 13\n",
      "21 31\n",
      "22 42\n",
      "23 24\n",
      "24 29\n",
      "25 16\n",
      "26 18\n",
      "27 41\n",
      "28 12\n",
      "29 35\n",
      "30 5\n",
      "31 45\n",
      "32 23\n",
      "33 1\n",
      "34 54\n",
      "35 52\n",
      "36 27\n",
      "37 46\n",
      "38 53\n",
      "39 21\n",
      "40 26\n",
      "41 47\n",
      "42 22\n",
      "43 14\n",
      "44 11\n",
      "45 0\n",
      "46 9\n",
      "47 25\n",
      "48 6\n",
      "49 32\n",
      "50 33\n",
      "51 36\n",
      "52 39\n",
      "53 10\n",
      "54 3\n",
      "55 40\n",
      "56 7\n",
      "57 51\n",
      "58 4\n",
      "59 15\n",
      "1 31\n",
      "2 37\n",
      "3 1\n",
      "4 34\n",
      "5 2\n",
      "6 57\n",
      "7 56\n",
      "8 5\n",
      "9 20\n",
      "10 38\n",
      "11 15\n",
      "12 30\n",
      "13 58\n",
      "14 41\n",
      "15 27\n",
      "16 59\n",
      "17 8\n",
      "18 50\n",
      "19 26\n",
      "20 55\n",
      "21 7\n",
      "22 52\n",
      "23 24\n",
      "24 18\n",
      "25 43\n",
      "26 23\n",
      "27 14\n",
      "28 44\n",
      "29 12\n",
      "30 13\n",
      "31 42\n",
      "32 51\n",
      "33 47\n",
      "34 40\n",
      "35 39\n",
      "36 48\n",
      "37 49\n",
      "38 0\n",
      "39 29\n",
      "40 25\n",
      "41 22\n",
      "42 36\n",
      "43 6\n",
      "44 11\n",
      "45 46\n",
      "46 3\n",
      "47 53\n",
      "48 17\n",
      "49 21\n",
      "50 32\n",
      "51 33\n",
      "52 45\n",
      "53 9\n",
      "54 35\n",
      "55 16\n",
      "56 28\n",
      "57 54\n",
      "58 10\n",
      "59 4\n",
      "1 20\n",
      "2 34\n",
      "3 8\n",
      "4 59\n",
      "5 2\n",
      "6 38\n",
      "7 24\n",
      "8 37\n",
      "9 31\n",
      "10 49\n",
      "11 48\n",
      "12 44\n",
      "13 30\n",
      "14 1\n",
      "15 22\n",
      "16 57\n",
      "17 28\n",
      "18 43\n",
      "19 13\n",
      "20 53\n",
      "21 26\n",
      "22 29\n",
      "23 14\n",
      "24 35\n",
      "25 33\n",
      "26 18\n",
      "27 17\n",
      "28 42\n",
      "29 6\n",
      "30 41\n",
      "31 56\n",
      "32 5\n",
      "33 21\n",
      "34 55\n",
      "35 50\n",
      "36 58\n",
      "37 23\n",
      "38 0\n",
      "39 27\n",
      "40 16\n",
      "41 46\n",
      "42 25\n",
      "43 52\n",
      "44 54\n",
      "45 11\n",
      "46 12\n",
      "47 32\n",
      "48 9\n",
      "49 39\n",
      "50 47\n",
      "51 36\n",
      "52 45\n",
      "53 3\n",
      "54 51\n",
      "55 10\n",
      "56 19\n",
      "57 40\n",
      "58 7\n",
      "59 4\n",
      "1 31\n",
      "2 34\n",
      "3 20\n",
      "4 2\n",
      "5 38\n",
      "6 59\n",
      "7 48\n",
      "8 43\n",
      "9 49\n",
      "10 1\n",
      "11 30\n",
      "12 24\n",
      "13 14\n",
      "14 44\n",
      "15 37\n",
      "16 22\n",
      "17 33\n",
      "18 29\n",
      "19 8\n",
      "20 53\n",
      "21 6\n",
      "22 23\n",
      "23 13\n",
      "24 26\n",
      "25 28\n",
      "26 35\n",
      "27 39\n",
      "28 57\n",
      "29 17\n",
      "30 12\n",
      "31 46\n",
      "32 42\n",
      "33 41\n",
      "34 3\n",
      "35 18\n",
      "36 27\n",
      "37 58\n",
      "38 5\n",
      "39 50\n",
      "40 56\n",
      "41 7\n",
      "42 19\n",
      "43 21\n",
      "44 36\n",
      "45 25\n",
      "46 40\n",
      "47 52\n",
      "48 47\n",
      "49 4\n",
      "50 16\n",
      "51 51\n",
      "52 15\n",
      "53 32\n",
      "54 0\n",
      "55 54\n",
      "56 55\n",
      "57 11\n",
      "58 45\n",
      "59 10\n",
      "Split:  0.0183849334717\n",
      "[20 34  8 59  2 38 24 37 31 49 48 44 30  1 22 57 28 43 13 53 26 29 14 35 33\n",
      " 18 17]\n",
      "1 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:74: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:75: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:77: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:78: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 57 is out of bounds for axis 0 with size 27",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-9114d0e687a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcart_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCART\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcart_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-109-8e141b49bd62>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimpurity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpurity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-109-8e141b49bd62>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, table, indices)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimpurity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpurity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimpurity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpurity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-109-8e141b49bd62>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, table, indices)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mpredicate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_right\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_predicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Split: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-109-8e141b49bd62>\u001b[0m in \u001b[0;36mget_predicate\u001b[1;34m(self, X, y, table, indices)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mX_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtable_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mX_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpurity\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'gini'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                         \u001b[1;31m#left_impurity  = 1. - (s_left**2 * (1. - left_impurity) + 2 * ys_left[y[k]] + 1) / (s_left + 1)**2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 57 is out of bounds for axis 0 with size 27"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "cart_clf = CART(max_depth=10)\n",
    "cart_clf.fit(X_train, y_train)\n",
    "print time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = cart_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 2.59042821955\n",
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "print \"Log loss: {}\".format(log_loss(y_test, pred))\n",
    "print \"Accuracy: {}\".format(cart_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00101804733276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=10)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 2.59042821955\n",
      "Accuracy: 0.925\n"
     ]
    }
   ],
   "source": [
    "#print \"Log loss: {}\".format(log_loss(y_test, sigmoid(pr)))\n",
    "print \"Log loss: {}\".format(log_loss(y_test, pr))\n",
    "print \"Accuracy: {}\".format(tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=10, poi=1.0, pof=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.poi = poi\n",
    "        self.pof = pof\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        print \"Fitting random forest with {} estimators\".format(self.n_estimators)\n",
    "        \n",
    "        self.estimators = []\n",
    "        \n",
    "        i = 0\n",
    "        for _ in xrange(self.n_estimators):\n",
    "            \n",
    "            i += 1\n",
    "            print \"Fitting estimator number {}\".format(i)\n",
    "            \n",
    "            n, m = X.shape\n",
    "            items    = np.sort(np.random.choice(n, n * self.poi))\n",
    "            features = np.sort(np.random.choice(m, m * self.pof, replace=False))\n",
    "                            \n",
    "            estimator = CART(max_depth=self.max_depth)\n",
    "            estimator.fit(X[:, features][items, :], y[items])\n",
    "            \n",
    "            self.estimators.append(estimator)            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        answer = 0\n",
    "        for e in self.estimators:\n",
    "            p = e.predict(X)\n",
    "            p[p == 0] = -1\n",
    "            answer += p\n",
    "        answer /= self.n_estimators\n",
    "        answer = np.sign(answer)        \n",
    "        answer[answer == -1] = 0\n",
    "        return answer\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)        \n",
    "        return np.mean(pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest with 100 estimators\n",
      "Fitting estimator number 1\n",
      "Fitting estimator number 2\n",
      "Fitting estimator number 3\n",
      "Fitting estimator number 4\n",
      "Fitting estimator number 5\n",
      "Fitting estimator number 6\n",
      "Fitting estimator number 7\n",
      "Fitting estimator number 8\n",
      "Fitting estimator number 9\n",
      "Fitting estimator number 10\n",
      "Fitting estimator number 11\n",
      "Fitting estimator number 12\n",
      "Fitting estimator number 13\n",
      "Fitting estimator number 14\n",
      "Fitting estimator number 15\n",
      "Fitting estimator number 16\n",
      "Fitting estimator number 17\n",
      "Fitting estimator number 18\n",
      "Fitting estimator number 19\n",
      "Fitting estimator number 20\n",
      "Fitting estimator number 21\n",
      "Fitting estimator number 22\n",
      "Fitting estimator number 23\n",
      "Fitting estimator number 24\n",
      "Fitting estimator number 25\n",
      "Fitting estimator number 26\n",
      "Fitting estimator number 27\n",
      "Fitting estimator number 28\n",
      "Fitting estimator number 29\n",
      "Fitting estimator number 30\n",
      "Fitting estimator number 31\n",
      "Fitting estimator number 32\n",
      "Fitting estimator number 33\n",
      "Fitting estimator number 34\n",
      "Fitting estimator number 35\n",
      "Fitting estimator number 36\n",
      "Fitting estimator number 37\n",
      "Fitting estimator number 38\n",
      "Fitting estimator number 39\n",
      "Fitting estimator number 40\n",
      "Fitting estimator number 41\n",
      "Fitting estimator number 42\n",
      "Fitting estimator number 43\n",
      "Fitting estimator number 44\n",
      "Fitting estimator number 45\n",
      "Fitting estimator number 46\n",
      "Fitting estimator number 47\n",
      "Fitting estimator number 48\n",
      "Fitting estimator number 49\n",
      "Fitting estimator number 50\n",
      "Fitting estimator number 51\n",
      "Fitting estimator number 52\n",
      "Fitting estimator number 53\n",
      "Fitting estimator number 54\n",
      "Fitting estimator number 55\n",
      "Fitting estimator number 56\n",
      "Fitting estimator number 57\n",
      "Fitting estimator number 58\n",
      "Fitting estimator number 59\n",
      "Fitting estimator number 60\n",
      "Fitting estimator number 61\n",
      "Fitting estimator number 62\n",
      "Fitting estimator number 63\n",
      "Fitting estimator number 64\n",
      "Fitting estimator number 65\n",
      "Fitting estimator number 66\n",
      "Fitting estimator number 67\n",
      "Fitting estimator number 68\n",
      "Fitting estimator number 69\n",
      "Fitting estimator number 70\n",
      "Fitting estimator number 71\n",
      "Fitting estimator number 72\n",
      "Fitting estimator number 73\n",
      "Fitting estimator number 74\n",
      "Fitting estimator number 75\n",
      "Fitting estimator number 76\n",
      "Fitting estimator number 77\n",
      "Fitting estimator number 78\n",
      "Fitting estimator number 79\n",
      "Fitting estimator number 80\n",
      "Fitting estimator number 81\n",
      "Fitting estimator number 82\n",
      "Fitting estimator number 83\n",
      "Fitting estimator number 84\n",
      "Fitting estimator number 85\n",
      "Fitting estimator number 86\n",
      "Fitting estimator number 87\n",
      "Fitting estimator number 88\n",
      "Fitting estimator number 89\n",
      "Fitting estimator number 90\n",
      "Fitting estimator number 91\n",
      "Fitting estimator number 92\n",
      "Fitting estimator number 93\n",
      "Fitting estimator number 94\n",
      "Fitting estimator number 95\n",
      "Fitting estimator number 96\n",
      "Fitting estimator number 97\n",
      "Fitting estimator number 98\n",
      "Fitting estimator number 99\n",
      "Fitting estimator number 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:21: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:22: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "forest_clf = RandomForest(n_estimators=100, max_depth=10)\n",
    "forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92500000000000004"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92500000000000004"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=10, max_depth=5, mu=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth        \n",
    "        self.mu = mu\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        print \"Fitting gradient boosting with {} estimators\".format(self.n_estimators)\n",
    "        \n",
    "        self.estimators = []\n",
    "        self.weigths = []\n",
    "        \n",
    "        estimator = CART(max_depth=self.max_depth, impurity='gini')\n",
    "        estimator.fit(X, y)\n",
    "        \n",
    "        h = estimator.predict(X)\n",
    "        \n",
    "        i = 0\n",
    "        for _ in xrange(self.n_estimators):\n",
    "            \n",
    "            i += 1\n",
    "            print \"Fitting estimator number {}\".format(i)\n",
    "            \n",
    "            g = y - sigmoid(h)\n",
    "            \n",
    "            estimator = CART(max_depth=self.max_depth, impurity='mse')\n",
    "            estimator.fit(X, g)\n",
    "            \n",
    "            self.estimators.append(estimator)\n",
    "            \n",
    "            #p = estimator.predict(X)            \n",
    "            #func = lambda b: log_loss(y, h + b * p)\n",
    "            #b = scipy.optimize.minimize(func, np.zeros((y.shape[0],))).x\n",
    "            #print b\n",
    "            #b = 1            \n",
    "            b = self.mu\n",
    "            \n",
    "            self.weigths.append(b)\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        answer = 0\n",
    "        for (b, a) in zip(self.weigths, self.estimators):            \n",
    "            answer += b * a.predict(X)\n",
    "        answer /= self.n_estimators\n",
    "        answer = np.sign(answer)\n",
    "        answer[answer == -1] = 0\n",
    "        return answer\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return np.mean(pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting gradient boosting with 100 estimators\n",
      "Fitting estimator number 1\n",
      "Fitting estimator number 2\n",
      "Fitting estimator number 3\n",
      "Fitting estimator number 4\n",
      "Fitting estimator number 5\n",
      "Fitting estimator number 6\n",
      "Fitting estimator number 7\n",
      "Fitting estimator number 8\n",
      "Fitting estimator number 9\n",
      "Fitting estimator number 10\n",
      "Fitting estimator number 11\n",
      "Fitting estimator number 12\n",
      "Fitting estimator number 13\n",
      "Fitting estimator number 14\n",
      "Fitting estimator number 15\n",
      "Fitting estimator number 16\n",
      "Fitting estimator number 17\n",
      "Fitting estimator number 18\n",
      "Fitting estimator number 19\n",
      "Fitting estimator number 20\n",
      "Fitting estimator number 21\n",
      "Fitting estimator number 22\n",
      "Fitting estimator number 23\n",
      "Fitting estimator number 24\n",
      "Fitting estimator number 25\n",
      "Fitting estimator number 26\n",
      "Fitting estimator number 27\n",
      "Fitting estimator number 28\n",
      "Fitting estimator number 29\n",
      "Fitting estimator number 30\n",
      "Fitting estimator number 31\n",
      "Fitting estimator number 32\n",
      "Fitting estimator number 33\n",
      "Fitting estimator number 34\n",
      "Fitting estimator number 35\n",
      "Fitting estimator number 36\n",
      "Fitting estimator number 37\n",
      "Fitting estimator number 38\n",
      "Fitting estimator number 39\n",
      "Fitting estimator number 40\n",
      "Fitting estimator number 41\n",
      "Fitting estimator number 42\n",
      "Fitting estimator number 43\n",
      "Fitting estimator number 44\n",
      "Fitting estimator number 45\n",
      "Fitting estimator number 46\n",
      "Fitting estimator number 47\n",
      "Fitting estimator number 48\n",
      "Fitting estimator number 49\n",
      "Fitting estimator number 50\n",
      "Fitting estimator number 51\n",
      "Fitting estimator number 52\n",
      "Fitting estimator number 53\n",
      "Fitting estimator number 54\n",
      "Fitting estimator number 55\n",
      "Fitting estimator number 56\n",
      "Fitting estimator number 57\n",
      "Fitting estimator number 58\n",
      "Fitting estimator number 59\n",
      "Fitting estimator number 60\n",
      "Fitting estimator number 61\n",
      "Fitting estimator number 62\n",
      "Fitting estimator number 63\n",
      "Fitting estimator number 64\n",
      "Fitting estimator number 65\n",
      "Fitting estimator number 66\n",
      "Fitting estimator number 67\n",
      "Fitting estimator number 68\n",
      "Fitting estimator number 69\n",
      "Fitting estimator number 70\n",
      "Fitting estimator number 71\n",
      "Fitting estimator number 72\n",
      "Fitting estimator number 73\n",
      "Fitting estimator number 74\n",
      "Fitting estimator number 75\n",
      "Fitting estimator number 76\n",
      "Fitting estimator number 77\n",
      "Fitting estimator number 78\n",
      "Fitting estimator number 79\n",
      "Fitting estimator number 80\n",
      "Fitting estimator number 81\n",
      "Fitting estimator number 82\n",
      "Fitting estimator number 83\n",
      "Fitting estimator number 84\n",
      "Fitting estimator number 85\n",
      "Fitting estimator number 86\n",
      "Fitting estimator number 87\n",
      "Fitting estimator number 88\n",
      "Fitting estimator number 89\n",
      "Fitting estimator number 90\n",
      "Fitting estimator number 91\n",
      "Fitting estimator number 92\n",
      "Fitting estimator number 93\n",
      "Fitting estimator number 94\n",
      "Fitting estimator number 95\n",
      "Fitting estimator number 96\n",
      "Fitting estimator number 97\n",
      "Fitting estimator number 98\n",
      "Fitting estimator number 99\n",
      "Fitting estimator number 100\n"
     ]
    }
   ],
   "source": [
    "grad_clf = GradientBoosting(n_estimators=100)\n",
    "grad_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92500000000000004"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
       "              max_depth=5, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "grad = GradientBoostingClassifier(n_estimators=100, max_depth=5)\n",
    "grad.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92500000000000004"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VotingClassifier:\n",
    "    def __init__(self, estimators, weigths):\n",
    "        self.estimators = estimators\n",
    "        self.weights = weigths\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for estimator in self.estimators:\n",
    "            estimator.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        answer = 0\n",
    "        for weight, estimator in zip(self.weights, self.estimators):\n",
    "            p = estimator.predict(X)\n",
    "            p[p == 0] = -1\n",
    "            answer += weight * p\n",
    "        answer = np.sign(answer / sum(self.weights))\n",
    "        answer[answer == -1] = 0\n",
    "        return answer\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return np.mean(pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest with 100 estimators\n",
      "Fitting estimator number 1\n",
      "Fitting estimator number 2\n",
      "Fitting estimator number 3\n",
      "Fitting estimator number 4\n",
      "Fitting estimator number 5\n",
      "Fitting estimator number 6\n",
      "Fitting estimator number 7\n",
      "Fitting estimator number 8\n",
      "Fitting estimator number 9\n",
      "Fitting estimator number 10\n",
      "Fitting estimator number 11\n",
      "Fitting estimator number 12\n",
      "Fitting estimator number 13\n",
      "Fitting estimator number 14\n",
      "Fitting estimator number 15\n",
      "Fitting estimator number 16\n",
      "Fitting estimator number 17\n",
      "Fitting estimator number 18\n",
      "Fitting estimator number 19\n",
      "Fitting estimator number 20\n",
      "Fitting estimator number 21\n",
      "Fitting estimator number 22\n",
      "Fitting estimator number 23\n",
      "Fitting estimator number 24\n",
      "Fitting estimator number 25\n",
      "Fitting estimator number 26\n",
      "Fitting estimator number 27\n",
      "Fitting estimator number 28\n",
      "Fitting estimator number 29\n",
      "Fitting estimator number 30\n",
      "Fitting estimator number 31\n",
      "Fitting estimator number 32\n",
      "Fitting estimator number 33\n",
      "Fitting estimator number 34\n",
      "Fitting estimator number 35\n",
      "Fitting estimator number 36\n",
      "Fitting estimator number 37\n",
      "Fitting estimator number 38\n",
      "Fitting estimator number 39\n",
      "Fitting estimator number 40\n",
      "Fitting estimator number 41\n",
      "Fitting estimator number 42\n",
      "Fitting estimator number 43\n",
      "Fitting estimator number 44\n",
      "Fitting estimator number 45\n",
      "Fitting estimator number 46\n",
      "Fitting estimator number 47\n",
      "Fitting estimator number 48\n",
      "Fitting estimator number 49\n",
      "Fitting estimator number 50\n",
      "Fitting estimator number 51\n",
      "Fitting estimator number 52\n",
      "Fitting estimator number 53\n",
      "Fitting estimator number 54\n",
      "Fitting estimator number 55\n",
      "Fitting estimator number 56\n",
      "Fitting estimator number 57\n",
      "Fitting estimator number 58\n",
      "Fitting estimator number 59\n",
      "Fitting estimator number 60\n",
      "Fitting estimator number 61\n",
      "Fitting estimator number 62\n",
      "Fitting estimator number 63\n",
      "Fitting estimator number 64\n",
      "Fitting estimator number 65\n",
      "Fitting estimator number 66\n",
      "Fitting estimator number 67\n",
      "Fitting estimator number 68\n",
      "Fitting estimator number 69\n",
      "Fitting estimator number 70\n",
      "Fitting estimator number 71\n",
      "Fitting estimator number 72\n",
      "Fitting estimator number 73\n",
      "Fitting estimator number 74\n",
      "Fitting estimator number 75\n",
      "Fitting estimator number 76\n",
      "Fitting estimator number 77\n",
      "Fitting estimator number 78\n",
      "Fitting estimator number 79\n",
      "Fitting estimator number 80\n",
      "Fitting estimator number 81\n",
      "Fitting estimator number 82\n",
      "Fitting estimator number 83\n",
      "Fitting estimator number 84\n",
      "Fitting estimator number 85\n",
      "Fitting estimator number 86\n",
      "Fitting estimator number 87\n",
      "Fitting estimator number 88\n",
      "Fitting estimator number 89\n",
      "Fitting estimator number 90\n",
      "Fitting estimator number 91\n",
      "Fitting estimator number 92\n",
      "Fitting estimator number 93\n",
      "Fitting estimator number 94\n",
      "Fitting estimator number 95\n",
      "Fitting estimator number 96\n",
      "Fitting estimator number 97\n",
      "Fitting estimator number 98\n",
      "Fitting estimator number 99\n",
      "Fitting estimator number 100\n",
      "Fitting gradient boosting with 100 estimators\n",
      "Fitting estimator number 1\n",
      "Fitting estimator number 2\n",
      "Fitting estimator number 3\n",
      "Fitting estimator number 4\n",
      "Fitting estimator number 5\n",
      "Fitting estimator number 6\n",
      "Fitting estimator number 7\n",
      "Fitting estimator number 8\n",
      "Fitting estimator number 9\n",
      "Fitting estimator number 10\n",
      "Fitting estimator number 11\n",
      "Fitting estimator number 12\n",
      "Fitting estimator number 13\n",
      "Fitting estimator number 14\n",
      "Fitting estimator number 15\n",
      "Fitting estimator number 16\n",
      "Fitting estimator number 17\n",
      "Fitting estimator number 18\n",
      "Fitting estimator number 19\n",
      "Fitting estimator number 20\n",
      "Fitting estimator number 21\n",
      "Fitting estimator number 22\n",
      "Fitting estimator number 23\n",
      "Fitting estimator number 24\n",
      "Fitting estimator number 25\n",
      "Fitting estimator number 26\n",
      "Fitting estimator number 27\n",
      "Fitting estimator number 28\n",
      "Fitting estimator number 29\n",
      "Fitting estimator number 30\n",
      "Fitting estimator number 31\n",
      "Fitting estimator number 32\n",
      "Fitting estimator number 33\n",
      "Fitting estimator number 34\n",
      "Fitting estimator number 35\n",
      "Fitting estimator number 36\n",
      "Fitting estimator number 37\n",
      "Fitting estimator number 38\n",
      "Fitting estimator number 39\n",
      "Fitting estimator number 40\n",
      "Fitting estimator number 41\n",
      "Fitting estimator number 42\n",
      "Fitting estimator number 43\n",
      "Fitting estimator number 44\n",
      "Fitting estimator number 45\n",
      "Fitting estimator number 46\n",
      "Fitting estimator number 47\n",
      "Fitting estimator number 48\n",
      "Fitting estimator number 49\n",
      "Fitting estimator number 50\n",
      "Fitting estimator number 51\n",
      "Fitting estimator number 52\n",
      "Fitting estimator number 53\n",
      "Fitting estimator number 54\n",
      "Fitting estimator number 55\n",
      "Fitting estimator number 56\n",
      "Fitting estimator number 57\n",
      "Fitting estimator number 58\n",
      "Fitting estimator number 59\n",
      "Fitting estimator number 60\n",
      "Fitting estimator number 61\n",
      "Fitting estimator number 62\n",
      "Fitting estimator number 63\n",
      "Fitting estimator number 64\n",
      "Fitting estimator number 65\n",
      "Fitting estimator number 66\n",
      "Fitting estimator number 67\n",
      "Fitting estimator number 68\n",
      "Fitting estimator number 69\n",
      "Fitting estimator number 70\n",
      "Fitting estimator number 71\n",
      "Fitting estimator number 72\n",
      "Fitting estimator number 73\n",
      "Fitting estimator number 74\n",
      "Fitting estimator number 75\n",
      "Fitting estimator number 76\n",
      "Fitting estimator number 77\n",
      "Fitting estimator number 78\n",
      "Fitting estimator number 79\n",
      "Fitting estimator number 80\n",
      "Fitting estimator number 81\n",
      "Fitting estimator number 82\n",
      "Fitting estimator number 83\n",
      "Fitting estimator number 84\n",
      "Fitting estimator number 85\n",
      "Fitting estimator number 86\n",
      "Fitting estimator number 87\n",
      "Fitting estimator number 88\n",
      "Fitting estimator number 89\n",
      "Fitting estimator number 90\n",
      "Fitting estimator number 91\n",
      "Fitting estimator number 92\n",
      "Fitting estimator number 93\n",
      "Fitting estimator number 94\n",
      "Fitting estimator number 95\n",
      "Fitting estimator number 96\n",
      "Fitting estimator number 97\n",
      "Fitting estimator number 98\n",
      "Fitting estimator number 99\n",
      "Fitting estimator number 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:21: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:22: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "vote_clf = VotingClassifier([forest_clf, grad_clf], [1.0, 1.0])\n",
    "vote_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92500000000000004"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.349723</td>\n",
       "      <td> 0.658872</td>\n",
       "      <td> 0.341822</td>\n",
       "      <td> 0.369098</td>\n",
       "      <td> 0.448115</td>\n",
       "      <td> 0.327417</td>\n",
       "      <td> 0.517556</td>\n",
       "      <td> 0.393646</td>\n",
       "      <td> 0.430504</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.470654</td>\n",
       "      <td> 0.410545</td>\n",
       "      <td> 0.590262</td>\n",
       "      <td> 0.450158</td>\n",
       "      <td> 0.443301</td>\n",
       "      <td> 0.467284</td>\n",
       "      <td> 0.438850</td>\n",
       "      <td> 0.413235</td>\n",
       "      <td> 0.433625</td>\n",
       "      <td> 0.575691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.320356</td>\n",
       "      <td> 0.214419</td>\n",
       "      <td> 0.796892</td>\n",
       "      <td> 0.283771</td>\n",
       "      <td> 0.429499</td>\n",
       "      <td> 0.336705</td>\n",
       "      <td> 0.209530</td>\n",
       "      <td> 0.411694</td>\n",
       "      <td> 0.620735</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.470654</td>\n",
       "      <td> 0.410545</td>\n",
       "      <td> 0.454687</td>\n",
       "      <td> 0.450158</td>\n",
       "      <td> 0.443301</td>\n",
       "      <td> 0.467284</td>\n",
       "      <td> 0.438850</td>\n",
       "      <td> 0.413235</td>\n",
       "      <td> 0.433625</td>\n",
       "      <td> 0.354707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.575150</td>\n",
       "      <td> 0.658872</td>\n",
       "      <td> 0.341822</td>\n",
       "      <td> 0.541797</td>\n",
       "      <td> 0.430258</td>\n",
       "      <td> 0.575468</td>\n",
       "      <td> 0.509843</td>\n",
       "      <td> 0.518629</td>\n",
       "      <td> 0.383852</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.470654</td>\n",
       "      <td> 0.410545</td>\n",
       "      <td> 0.454687</td>\n",
       "      <td> 0.450158</td>\n",
       "      <td> 0.443301</td>\n",
       "      <td> 0.467284</td>\n",
       "      <td> 0.992203</td>\n",
       "      <td> 0.768557</td>\n",
       "      <td> 0.433625</td>\n",
       "      <td> 0.391791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.349723</td>\n",
       "      <td> 0.658872</td>\n",
       "      <td> 0.341822</td>\n",
       "      <td> 0.440102</td>\n",
       "      <td> 0.455950</td>\n",
       "      <td> 0.327417</td>\n",
       "      <td> 0.560001</td>\n",
       "      <td> 0.398133</td>\n",
       "      <td> 0.376336</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.470654</td>\n",
       "      <td> 0.410545</td>\n",
       "      <td> 0.607055</td>\n",
       "      <td> 0.450158</td>\n",
       "      <td> 0.443301</td>\n",
       "      <td> 0.467284</td>\n",
       "      <td> 0.438850</td>\n",
       "      <td> 0.413235</td>\n",
       "      <td> 0.433625</td>\n",
       "      <td> 0.550478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.320356</td>\n",
       "      <td> 0.658872</td>\n",
       "      <td> 0.341822</td>\n",
       "      <td> 0.385197</td>\n",
       "      <td> 0.437169</td>\n",
       "      <td> 0.709301</td>\n",
       "      <td> 0.419971</td>\n",
       "      <td> 0.288835</td>\n",
       "      <td> 0.382394</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.470654</td>\n",
       "      <td> 0.410545</td>\n",
       "      <td> 0.454687</td>\n",
       "      <td> 0.450158</td>\n",
       "      <td> 0.443301</td>\n",
       "      <td> 0.467284</td>\n",
       "      <td> 0.438850</td>\n",
       "      <td> 0.413235</td>\n",
       "      <td> 0.433625</td>\n",
       "      <td> 0.741449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7    \\\n",
       "0    1  0.349723  0.658872  0.341822  0.369098  0.448115  0.327417  0.517556   \n",
       "1    1  0.320356  0.214419  0.796892  0.283771  0.429499  0.336705  0.209530   \n",
       "2    1  0.575150  0.658872  0.341822  0.541797  0.430258  0.575468  0.509843   \n",
       "3    1  0.349723  0.658872  0.341822  0.440102  0.455950  0.327417  0.560001   \n",
       "4    1  0.320356  0.658872  0.341822  0.385197  0.437169  0.709301  0.419971   \n",
       "\n",
       "        8         9      ...          93        94        95        96   \\\n",
       "0  0.393646  0.430504    ...     0.470654  0.410545  0.590262  0.450158   \n",
       "1  0.411694  0.620735    ...     0.470654  0.410545  0.454687  0.450158   \n",
       "2  0.518629  0.383852    ...     0.470654  0.410545  0.454687  0.450158   \n",
       "3  0.398133  0.376336    ...     0.470654  0.410545  0.607055  0.450158   \n",
       "4  0.288835  0.382394    ...     0.470654  0.410545  0.454687  0.450158   \n",
       "\n",
       "        97        98        99        100       101       102  \n",
       "0  0.443301  0.467284  0.438850  0.413235  0.433625  0.575691  \n",
       "1  0.443301  0.467284  0.438850  0.413235  0.433625  0.354707  \n",
       "2  0.443301  0.467284  0.992203  0.768557  0.433625  0.391791  \n",
       "3  0.443301  0.467284  0.438850  0.413235  0.433625  0.550478  \n",
       "4  0.443301  0.467284  0.438850  0.413235  0.433625  0.741449  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_train_df = pd.read_csv('spam.train.txt', header=None, delimiter=\" \")\n",
    "spam_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_train_df[\"class\"] = spam_train_df[0].copy()\n",
    "spam_X_train, spam_y_train, _, _ = shuffle(spam_train_df.drop(0, axis=1), train_percent=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.445622</td>\n",
       "      <td> 0.632602</td>\n",
       "      <td> 0.368683</td>\n",
       "      <td> 0.728974</td>\n",
       "      <td> 0.479510</td>\n",
       "      <td> 0.471833</td>\n",
       "      <td> 0.571882</td>\n",
       "      <td> 0.579280</td>\n",
       "      <td> 0.604702</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.462762</td>\n",
       "      <td> 0.448051</td>\n",
       "      <td> 0.446374</td>\n",
       "      <td> 0.424879</td>\n",
       "      <td> 0.472828</td>\n",
       "      <td> 0.450815</td>\n",
       "      <td> 0.427943</td>\n",
       "      <td> 0.416184</td>\n",
       "      <td> 0.446429</td>\n",
       "      <td> 0.667609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.296747</td>\n",
       "      <td> 0.632602</td>\n",
       "      <td> 0.368683</td>\n",
       "      <td> 0.330429</td>\n",
       "      <td> 0.374055</td>\n",
       "      <td> 0.451231</td>\n",
       "      <td> 0.249144</td>\n",
       "      <td> 0.431370</td>\n",
       "      <td> 0.344248</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.462762</td>\n",
       "      <td> 0.448051</td>\n",
       "      <td> 0.916790</td>\n",
       "      <td> 0.424879</td>\n",
       "      <td> 0.472828</td>\n",
       "      <td> 0.368804</td>\n",
       "      <td> 0.464272</td>\n",
       "      <td> 0.400264</td>\n",
       "      <td> 0.547989</td>\n",
       "      <td> 0.580435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.296747</td>\n",
       "      <td> 0.632602</td>\n",
       "      <td> 0.368683</td>\n",
       "      <td> 0.309466</td>\n",
       "      <td> 0.392561</td>\n",
       "      <td> 0.458103</td>\n",
       "      <td> 0.289492</td>\n",
       "      <td> 0.595944</td>\n",
       "      <td> 0.574234</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.462762</td>\n",
       "      <td> 0.448051</td>\n",
       "      <td> 0.798868</td>\n",
       "      <td> 0.424879</td>\n",
       "      <td> 0.472828</td>\n",
       "      <td> 0.368804</td>\n",
       "      <td> 0.472316</td>\n",
       "      <td> 0.415417</td>\n",
       "      <td> 0.446429</td>\n",
       "      <td> 0.781863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.296747</td>\n",
       "      <td> 0.184437</td>\n",
       "      <td> 0.831801</td>\n",
       "      <td> 0.239714</td>\n",
       "      <td> 0.494042</td>\n",
       "      <td> 0.716275</td>\n",
       "      <td> 0.295862</td>\n",
       "      <td> 0.349747</td>\n",
       "      <td> 0.434139</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.462762</td>\n",
       "      <td> 0.448051</td>\n",
       "      <td> 0.446374</td>\n",
       "      <td> 0.424879</td>\n",
       "      <td> 0.472828</td>\n",
       "      <td> 0.368804</td>\n",
       "      <td> 0.427943</td>\n",
       "      <td> 0.854188</td>\n",
       "      <td> 0.543903</td>\n",
       "      <td> 0.562639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.368046</td>\n",
       "      <td> 0.632602</td>\n",
       "      <td> 0.368683</td>\n",
       "      <td> 0.280957</td>\n",
       "      <td> 0.380190</td>\n",
       "      <td> 0.445567</td>\n",
       "      <td> 0.339384</td>\n",
       "      <td> 0.534049</td>\n",
       "      <td> 0.522703</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.462762</td>\n",
       "      <td> 0.448051</td>\n",
       "      <td> 0.446374</td>\n",
       "      <td> 0.533793</td>\n",
       "      <td> 0.472828</td>\n",
       "      <td> 0.812761</td>\n",
       "      <td> 0.473772</td>\n",
       "      <td> 0.400264</td>\n",
       "      <td> 0.446429</td>\n",
       "      <td> 0.659760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7    \\\n",
       "0    1  0.445622  0.632602  0.368683  0.728974  0.479510  0.471833  0.571882   \n",
       "1    1  0.296747  0.632602  0.368683  0.330429  0.374055  0.451231  0.249144   \n",
       "2    1  0.296747  0.632602  0.368683  0.309466  0.392561  0.458103  0.289492   \n",
       "3    1  0.296747  0.184437  0.831801  0.239714  0.494042  0.716275  0.295862   \n",
       "4    1  0.368046  0.632602  0.368683  0.280957  0.380190  0.445567  0.339384   \n",
       "\n",
       "        8         9      ...          93        94        95        96   \\\n",
       "0  0.579280  0.604702    ...     0.462762  0.448051  0.446374  0.424879   \n",
       "1  0.431370  0.344248    ...     0.462762  0.448051  0.916790  0.424879   \n",
       "2  0.595944  0.574234    ...     0.462762  0.448051  0.798868  0.424879   \n",
       "3  0.349747  0.434139    ...     0.462762  0.448051  0.446374  0.424879   \n",
       "4  0.534049  0.522703    ...     0.462762  0.448051  0.446374  0.533793   \n",
       "\n",
       "        97        98        99        100       101       102  \n",
       "0  0.472828  0.450815  0.427943  0.416184  0.446429  0.667609  \n",
       "1  0.472828  0.368804  0.464272  0.400264  0.547989  0.580435  \n",
       "2  0.472828  0.368804  0.472316  0.415417  0.446429  0.781863  \n",
       "3  0.472828  0.368804  0.427943  0.854188  0.543903  0.562639  \n",
       "4  0.472828  0.812761  0.473772  0.400264  0.446429  0.659760  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_test_df = pd.read_csv('spam.test.txt', header=None, delimiter=\" \")\n",
    "spam_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_test_df[\"class\"] = spam_test_df[0].copy()\n",
    "spam_X_test, spam_y_test = spam_test_df.drop(0, axis=1).values[:, :-1], spam_test_df.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  1.6292579174\n",
      "Split:  1.1741271019\n",
      "Split:  1.18071699142\n",
      "Split:  0.298894882202\n",
      "Split:  0.303210020065\n",
      "4.59608912468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:69: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:70: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:72: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:73: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "cart_clf = CART(max_depth=4)\n",
    "#cart_clf = DecisionTreeClassifier(max_depth=3)\n",
    "cart_clf.fit(spam_X_train[:1000, :], spam_y_train[:1000])\n",
    "print time.time() - t\n",
    "#forest_clf = RandomForest(n_estimators=100, max_depth=10, poi=0.7, pof=0.3)\n",
    "#grad_clf = GradientBoosting(n_estimators=100)\n",
    "\n",
    "#vote_clf = VotingClassifier([forest_clf, grad_clf], [1.0, 1.0])\n",
    "#vote_clf.fit(spam_X_train, spam_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72305091487669049"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_clf.score(spam_X_test, spam_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0393750667572\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "tree = DecisionTreeClassifier(max_depth=4)\n",
    "tree.fit(spam_X_train[:1000, :], spam_y_train[:1000])\n",
    "print time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72464200477326968"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(spam_X_test, spam_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
