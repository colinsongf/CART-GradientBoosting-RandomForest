{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss, mean_squared_error, f1_score, precision_recall_fscore_support\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from collections import Counter\n",
    "import pprint\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intersection(s1, s2):\n",
    "    s = set(s2)\n",
    "    return filter(lambda x: x in s, s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    hist = Counter(y)\n",
    "    return 1. - np.sum(np.array(hist.values()) / float(len(y)))**2\n",
    "\n",
    "def mse(y):\n",
    "    return np.var(y) * len(y)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, depth, max_depth, impurity, is_leaf=False):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.impurity = impurity\n",
    "        \n",
    "    def stopping_criteria(self, X, y):\n",
    "        if self.max_depth is not None and self.depth >= self.max_depth:\n",
    "            return True\n",
    "                \n",
    "        if self.impurity == 'gini':        \n",
    "            return np.unique(y).shape[0] == 1\n",
    "        \n",
    "    def get_answer(self, y):\n",
    "        if self.impurity == 'gini':\n",
    "            return Counter(y).most_common()[0][0]\n",
    "        else:                \n",
    "            return np.mean(y)\n",
    "    \n",
    "    def get_impurity_change(self, y, y_left, y_right):\n",
    "        if self.impurity == 'gini':\n",
    "            return gini(y) - gini(y_left) * len(y_left) / len(y) - gini(y_right) * len(y_right) / len(y)\n",
    "        else:\n",
    "            return mse(y) - mse(y_left) * len(y_left) / len(y) - mse(y_right) * len(y_right) / len(y)\n",
    "        \n",
    "    def get_predicate(self, X, y, table, indices):\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_impurity = None\n",
    "        best_index = None\n",
    "    \n",
    "        for feature in np.arange(m):\n",
    "            if self.impurity == 'gini':\n",
    "                hist_left = Counter([])\n",
    "                hist_right = Counter(y[indices])\n",
    "                            \n",
    "                square_sum_left, square_sum_right = 0, np.sum(np.array(hist_right.values())**2)\n",
    "            else:\n",
    "                sum_left, sum_right = 0, np.sum(y[indices])                \n",
    "            \n",
    "            table_feature = intersection(table[feature], indices)\n",
    "            \n",
    "            n = len(table_feature)\n",
    "            \n",
    "            for ind, k in enumerate(table_feature[:-1]):\n",
    "\n",
    "                if self.impurity == 'gini':\n",
    "                    square_sum_left += 2 * hist_left[y[k]] + 1\n",
    "                    square_sum_right -= 2 * hist_right[y[k]] - 1\n",
    "\n",
    "                    hist_left[y[k]] += 1\n",
    "                    hist_right[y[k]] -= 1\n",
    "\n",
    "                    new_impurity = -float(square_sum_left) / (ind+1) - float(square_sum_right) / (n-ind-1)\n",
    "                else:\n",
    "                    sum_left += y[k]\n",
    "                    sum_right -= y[k]\n",
    "                    \n",
    "                    new_impurity = -sum_left**2 / float(ind+1) - sum_right**2 / float(n-ind-1)\n",
    "\n",
    "                if X[table_feature[ind+1], feature] > X[k, feature]:\n",
    "                    \n",
    "                    if best_feature is None or new_impurity < best_impurity:\n",
    "                        threshold = (X[k, feature] + X[table_feature[ind+1], feature]) / 2.\n",
    "\n",
    "                        best_feature = feature\n",
    "                        best_threshold = threshold\n",
    "                        best_impurity = new_impurity\n",
    "                        best_index = ind+1                        \n",
    "                \n",
    "        if best_feature is not None:\n",
    "            table_feature = intersection(table[best_feature], indices)\n",
    "\n",
    "            best_left  = table_feature[:best_index]\n",
    "            best_right = table_feature[best_index:]\n",
    "                        \n",
    "            self.var_importance = len(indices) * self.get_impurity_change(y[indices], y[best_left], y[best_right]) / X.shape[0]\n",
    "        else:\n",
    "            best_left  = None\n",
    "            best_right = None        \n",
    "                \n",
    "        return best_feature, best_threshold, best_left, best_right\n",
    "        \n",
    "    def fit(self, X, y, table, indices):\n",
    "        \n",
    "        if self.stopping_criteria(X[indices], y[indices]):\n",
    "            self.is_leaf = True\n",
    "            self.answer = self.get_answer(y[indices])\n",
    "            return\n",
    "        \n",
    "        #t = time.time()\n",
    "        self.feature, self.threshold, left, right = self.get_predicate(X, y, table, indices)\n",
    "        #print \"Split: \", time.time() - t\n",
    "\n",
    "        if self.feature is None:\n",
    "            self.is_leaf = True\n",
    "            self.answer = self.get_answer(y[indices])            \n",
    "            return\n",
    "\n",
    "        self.left = Node(self.depth + 1, max_depth=self.max_depth, impurity=self.impurity)\n",
    "        self.left.fit(X, y, table, left)\n",
    "\n",
    "        self.right = Node(self.depth + 1, max_depth=self.max_depth, impurity=self.impurity)\n",
    "        self.right.fit(X, y, table, right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.is_leaf:\n",
    "            return self.answer\n",
    "        else:\n",
    "            if X[self.feature] <= self.threshold:\n",
    "                return self.left.predict(X)\n",
    "            else:\n",
    "                return self.right.predict(X)\n",
    "            \n",
    "    def get_feature_scores(self):\n",
    "        if self.is_leaf:\n",
    "            return Counter([])\n",
    "        \n",
    "        left_scores = self.left.get_feature_scores()\n",
    "        right_scores = self.right.get_feature_scores()\n",
    "        \n",
    "        scores = left_scores + right_scores\n",
    "        scores[self.feature] += self.var_importance\n",
    "        \n",
    "        return scores\n",
    "        \n",
    "class CART:\n",
    "    def __init__(self, impurity='gini', max_depth=None):\n",
    "        if impurity not in ['gini', 'mse']:\n",
    "            raise ValueError(\"Only gini and mse criteria are supported\")\n",
    "        self.impurity = impurity\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        table = X.argsort(axis=0).T\n",
    "        indices = np.arange(X.shape[0])\n",
    "        \n",
    "        self.root = Node(0, max_depth=self.max_depth, impurity=self.impurity)                \n",
    "        self.root.fit(X, y, table, indices)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        answer = []\n",
    "        for x in X:\n",
    "            answer.append(self.root.predict(x))\n",
    "        return np.array(answer)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "    \n",
    "    def get_feature_scores(self):\n",
    "        return self.root.get_feature_scores()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(df, train_percent=0.8):\n",
    "    X = np.copy(df.values)\n",
    "    np.random.shuffle(X)\n",
    "    \n",
    "    X, y = X[:, :-1], X[:, -1]\n",
    "    \n",
    "    train_size = int(X.shape[0] * train_percent)\n",
    "    \n",
    "    X_train, y_train = X[:train_size, :], y[:train_size]\n",
    "    X_test, y_test = X[train_size:, :], y[train_size:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=10, poi=1.0, pof=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.poi = poi\n",
    "        self.pof = pof\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        print \"Fitting random forest with {} estimators\".format(self.n_estimators)\n",
    "        \n",
    "        self.estimators = []\n",
    "        self.estimators_features = []\n",
    "        \n",
    "        self.oob_score = []\n",
    "           \n",
    "        #np.random.seed(1543*1543)\n",
    "        np.random.seed(1)\n",
    "    \n",
    "        i = 0\n",
    "        for _ in xrange(self.n_estimators):\n",
    "            \n",
    "            i += 1\n",
    "            print \"Fitting estimator number {}\".format(i)\n",
    "            \n",
    "            n, m = X.shape\n",
    "            items    = np.sort(np.random.choice(n, n * self.poi))\n",
    "            features = np.sort(np.random.choice(m, m * self.pof, replace=False))\n",
    "            \n",
    "            mask = np.ones((X.shape[0],), dtype=bool)\n",
    "            mask[items] = 0\n",
    "            \n",
    "            #estimator = CART(max_depth=self.max_depth)\n",
    "            estimator = CART()\n",
    "            #estimator = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            #estimator = DecisionTreeClassifier()\n",
    "            estimator.fit(X[:, features][items, :], y[items])\n",
    "            \n",
    "            self.estimators.append(estimator)\n",
    "            self.estimators_features.append(features)\n",
    "                        \n",
    "            self.oob_score.append(estimator.score(X[:, features][mask, :], y[mask]))\n",
    "        #print \"OOB score: {}\".format(np.mean(self.oob_score))\n",
    "        print \"OOB score: {}\".format(np.mean(self.oob_score))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        answer = 0\n",
    "        \n",
    "        prob = [Counter([])] * X.shape[0]\n",
    "        \n",
    "        for e, f, s in zip(self.estimators, self.estimators_features, self.oob_score):\n",
    "            p = e.predict(X[:, f])\n",
    "            answer += p\n",
    "\n",
    "            for i, a in enumerate(p):\n",
    "                prob[i][a] += 1\n",
    "                \n",
    "        answer /= self.n_estimators\n",
    "        answer = np.sign(answer)\n",
    "        #answer[answer == -1] = 0\n",
    "                    \n",
    "        #pprint.pprint(map(lambda c: np.array(c.values()) / float(X.shape[0] * self.n_estimators), prob))\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)        \n",
    "        return np.mean(pred == y)\n",
    "    \n",
    "    def get_feature_scores(self):\n",
    "        feature_scores = Counter([])\n",
    "        for e, ef in zip(self.estimators, self.estimators_features):\n",
    "            scores = e.get_feature_scores()\n",
    "            for f, s in scores.items():\n",
    "                feature_scores[ef[f]] += s            \n",
    "        return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binarize(y):\n",
    "    return np.array([-1 if x < 0.0 else 1 for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=10, max_depth=5, mu=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth        \n",
    "        self.mu = mu\n",
    "    \n",
    "    def loss(self, y_true, y_pred):\n",
    "        return log_loss((y_true+1) / 2., (y_pred + 1) / 2.)\n",
    "    \n",
    "    def get_learning_rate(self, X, y, estimator):\n",
    "        \n",
    "        old = np.array(self.predict(X))\n",
    "        new = np.array(estimator.predict(X))\n",
    "\n",
    "        best_b = self.mu\n",
    "        best_loss = float(\"+inf\")\n",
    "        \n",
    "        b = 0.1\n",
    "        while b <= 10.0:\n",
    "            p = old + b * self.mu * new\n",
    "            p = binarize(p)\n",
    "            \n",
    "            loss = self.loss(y, p)\n",
    "            \n",
    "            if loss <= best_loss:                \n",
    "                best_b = b\n",
    "                best_loss = loss\n",
    "                \n",
    "            b += 0.1\n",
    "        \n",
    "        b = 10.0\n",
    "        while b <= 100.0:\n",
    "            p = old + b * self.mu * new\n",
    "            p = binarize(p)\n",
    "            \n",
    "            loss = self.loss(y, p)\n",
    "            if loss <= best_loss:\n",
    "                \n",
    "                best_b = b\n",
    "                best_loss = loss\n",
    "                \n",
    "            b += 1.0\n",
    "        \n",
    "        return self.mu * b    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        print \"Fitting gradient boosting with {} estimators\".format(self.n_estimators)\n",
    "        \n",
    "        self.estimators = []\n",
    "        self.weigths = []\n",
    "        \n",
    "        estimator = CART(max_depth=1, impurity='mse')\n",
    "        estimator.fit(X, y)\n",
    "        \n",
    "        #sk_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "        #sk_estimator = DecisionTreeRegressor(max_depth=1)\n",
    "        #sk_estimator.fit(X, y)\n",
    "        #print \"SK LOG LOSS: \", log_loss(y, sk_estimator.predict(X))\n",
    "        \n",
    "        #h = estimator.predict(X)\n",
    "        \n",
    "        #print \"LOG LOSS: \", log_loss(y, estimator.predict(X))\n",
    "        \n",
    "        self.estimators.append(estimator)\n",
    "        #self.estimators.append(sk_estimator)\n",
    "        self.weigths.append(1.)\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        old_loss = 0\n",
    "        \n",
    "        depth = self.max_depth        \n",
    "        for i in xrange(self.n_estimators):\n",
    "                    \n",
    "            print \"Fitting estimator number {}\".format(i+1)\n",
    "            \n",
    "            h = self.predict(X)\n",
    "            \n",
    "            g = (y+1)/2. - sigmoid((h+1)/2.)\n",
    "             \n",
    "            loss = self.loss(y_true=y, y_pred=binarize(h))\n",
    "            print \"Score: \", np.mean(y == binarize(h))\n",
    "            #loss = self.loss(y, h)        \n",
    "            print \"Loss: {}\\ndiff: {}\".format(loss, loss - old_loss)                    \n",
    "                        \n",
    "            old_loss = loss            \n",
    "                    \n",
    "            estimator = CART(max_depth=depth, impurity='mse')\n",
    "            estimator.fit(X, g)\n",
    "            \n",
    "            sk_estimator = DecisionTreeRegressor(max_depth=depth)\n",
    "            sk_estimator.fit(X, g)\n",
    "            \n",
    "            self.estimators.append(estimator)\n",
    "            #self.estimators.append(sk_estimator)\n",
    "            \n",
    "            print \"MY MSE: \", mean_squared_error(g, estimator.predict(X))\n",
    "            print \"SK MSE: \", mean_squared_error(g, sk_estimator.predict(X))\n",
    "            \n",
    "            b = self.get_learning_rate(X, y, estimator)\n",
    "            #b = self.get_learning_rate(X, y, sk_estimator)\n",
    "            self.weigths.append(b)\n",
    "\n",
    "    def predict(self, X):\n",
    "        answer = 0\n",
    "        for b, a in zip(self.weigths, self.estimators):            \n",
    "            answer += b * a.predict(X)\n",
    "        return answer\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return np.mean(np.sign(pred) == y)\n",
    "    \n",
    "    def get_feature_scores(self):        \n",
    "        feature_scores = Counter([])\n",
    "        for w, e in zip(self.weights, self.estimators):\n",
    "            scores = e.get_feature_scores()\n",
    "            for k in scores.keys():\n",
    "                scores[k] *= w\n",
    "            feature_scores += scores\n",
    "        return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VotingClassifier:\n",
    "    def __init__(self, estimators, weigths=None):\n",
    "        self.estimators = estimators\n",
    "        self.weights = weigths\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.weights is not None:\n",
    "            return\n",
    "        \n",
    "        self.weights = np.zeros((len(self.estimators),))\n",
    "        \n",
    "        predicts = np.concatenate(map(lambda e: [e.predict(X)], self.estimators)).T\n",
    "                \n",
    "        alpha = 0.0001\n",
    "        \n",
    "        eps = 0.01\n",
    "        \n",
    "        print \"Fitting voting classifier\"\n",
    "        \n",
    "        h = np.dot(predicts, self.weights)\n",
    "        old_err = float('+inf')\n",
    "        err = log_loss((y+1)/2, (binarize(h)+1)/2)\n",
    "        i = 0\n",
    "        while abs(err - old_err) > eps:\n",
    "            \n",
    "            grad = np.dot(predicts.T, sigmoid(h) - y) / X.shape[0]\n",
    "            self.weights -= alpha * grad\n",
    "            \n",
    "            h = np.dot(predicts, self.weights)\n",
    "            old_err = err\n",
    "            err = log_loss((y+1)/2, (binarize(h)+1)/2)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        answer = 0\n",
    "        for weight, estimator in zip(self.weights, self.estimators):\n",
    "            p = estimator.predict(X)            \n",
    "            answer += weight * p\n",
    "        answer = np.sign(answer / len(self.estimators))\n",
    "        return answer\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return np.mean(pred == y)\n",
    "    \n",
    "    def get_feature_scores(self):\n",
    "        feature_scores = Counter([])\n",
    "        for w, e in zip(self.weights, self.estimators):\n",
    "            scores = e.get_feature_scores()\n",
    "            for k in scores.keys():\n",
    "                scores[k] *= w\n",
    "            feature_scores += scores\n",
    "        return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CFS(X, y):    \n",
    "    r = np.corrcoef(X, y.reshape(-1, 1), rowvar=0)\n",
    "    ry = r[:, -1]\n",
    "    r = r[:-1, :-1]\n",
    "    \n",
    "    best_all_score = None\n",
    "    best_features = None\n",
    "    all_features = []\n",
    "    \n",
    "    features = set([])\n",
    "    \n",
    "    ysum = 0.0\n",
    "    pairsum = 0.0\n",
    "    d = 0\n",
    "    \n",
    "    features_scores = Counter([])\n",
    "    for i in xrange(X.shape[1]):\n",
    "        best_score = None\n",
    "        for f in xrange(X.shape[1]):\n",
    "            if f not in features:\n",
    "                score = (ysum + ry[f]) / math.sqrt(d + 1 + 2 * (pairsum + np.sum(r[f, list(features)])))\n",
    "                if best_score is None or score > best_score:\n",
    "                    best_score = score\n",
    "                    best_feature = f\n",
    "        \n",
    "        d += 1\n",
    "        pairsum += 2 * np.sum(r[best_feature, list(features)])\n",
    "        ysum += ry[best_feature]\n",
    "        \n",
    "        features.add(best_feature)\n",
    "        all_features.append((best_feature, best_score))\n",
    "                    \n",
    "        if best_all_score < best_score:\n",
    "            best_all_score = best_score\n",
    "            best_features = features\n",
    "            \n",
    "    return best_features, best_all_score, all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def J(X, y, feature_set, model):\n",
    "    f1 = []\n",
    "    for train_index, test_index in KFold(y.shape[0], n_folds=3, shuffle=True):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "                \n",
    "        model.fit(X_train, y_train)        \n",
    "        p = model.predict(X_test)        \n",
    "        \n",
    "        f1.append(f1_score(np.array((y_test+1)/2, dtype=int), np.array((p+1)/2, dtype=int)))\n",
    "    return np.mean(f1)\n",
    "\n",
    "def forward_selection(X, y, n, model):\n",
    "    features = []\n",
    "    scores = []\n",
    "    \n",
    "    best_features = None\n",
    "    best_all_score = None\n",
    "    \n",
    "    current_features = set([])\n",
    "    \n",
    "    for j in xrange(X.shape[1]):\n",
    "        best_score = None\n",
    "        best_feature = None\n",
    "        \n",
    "        t = time.time()\n",
    "        \n",
    "        for f in xrange(X.shape[1]): # Find best feature\n",
    "            if f in current_features:\n",
    "                continue\n",
    "\n",
    "            #print \"Trying feature number \", f\n",
    "                \n",
    "            current_set = current_features | set([f])\n",
    "            score = J(X[:, list(current_set)], y, current_set, model)\n",
    "            if best_score is None or score > best_score:\n",
    "                best_score = score\n",
    "                best_feature = f\n",
    "        \n",
    "        features.append(best_feature)\n",
    "        scores.append(best_score)\n",
    "        \n",
    "        print best_score\n",
    "        \n",
    "        current_features.add(best_feature) # Add new feature\n",
    "\n",
    "#         if best_all_score is not None and best_score >= eta * best_all_score:\n",
    "#             return best_features\n",
    "        \n",
    "        if best_all_score is None or best_all_score < best_score:\n",
    "            best_all_score = best_score\n",
    "            best_features = current_features\n",
    "            best_j = j\n",
    "            \n",
    "        print \"[Iteration number {}]\".format(j)\n",
    "        print \"Best features so far: {}\".format(best_features)\n",
    "        print \"Time elapsed on iteration: {}\".format(time.time() - t)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        #if j - best_j >= n:\n",
    "            #return best_features\n",
    "    \n",
    "    return best_features, best_all_score, features, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849393279131\n",
      "[Iteration number 0]\n",
      "Best features so far: set([48])\n",
      "Time elapsed on iteration: 115.462303162\n",
      "0.880265755608\n",
      "[Iteration number 1]\n",
      "Best features so far: set([48, 42])\n",
      "Time elapsed on iteration: 100.477928162\n",
      "0.904757764959\n",
      "[Iteration number 2]\n",
      "Best features so far: set([48, 42, 70])\n",
      "Time elapsed on iteration: 119.885170937\n",
      "0.923801101009\n",
      "[Iteration number 3]\n",
      "Best features so far: set([48, 41, 42, 70])\n",
      "Time elapsed on iteration: 124.949954987\n",
      "0.932619867349\n",
      "[Iteration number 4]\n",
      "Best features so far: set([48, 41, 42, 101, 70])\n",
      "Time elapsed on iteration: 135.90101099\n",
      "0.948181877008\n",
      "[Iteration number 5]\n",
      "Best features so far: set([101, 70, 41, 42, 15, 48])\n",
      "Time elapsed on iteration: 159.726443052\n",
      "0.952783705844\n",
      "[Iteration number 6]\n",
      "Best features so far: set([101, 70, 41, 42, 15, 48, 53])\n",
      "Time elapsed on iteration: 161.849126101\n",
      "0.956596655402\n",
      "[Iteration number 7]\n",
      "Best features so far: set([101, 70, 41, 42, 15, 48, 53, 9])\n",
      "Time elapsed on iteration: 160.080096006\n",
      "0.958655563789\n",
      "[Iteration number 8]\n",
      "Best features so far: set([101, 70, 41, 42, 15, 48, 53, 9, 92])\n",
      "Time elapsed on iteration: 195.405498028\n",
      "0.959183723275\n",
      "[Iteration number 9]\n",
      "Best features so far: set([101, 70, 41, 42, 15, 48, 53, 9, 92, 5])\n",
      "Time elapsed on iteration: 188.058015823\n",
      "0.960508491718\n",
      "[Iteration number 10]\n",
      "Best features so far: set([101, 70, 41, 42, 15, 48, 53, 9, 79, 92, 5])\n",
      "Time elapsed on iteration: 203.821041107\n",
      "0.960626042595\n",
      "[Iteration number 11]\n",
      "Best features so far: set([101, 70, 38, 41, 42, 15, 48, 53, 9, 79, 92, 5])\n",
      "Time elapsed on iteration: 204.133595943\n",
      "0.962872616463\n",
      "[Iteration number 12]\n",
      "Best features so far: set([101, 70, 38, 41, 42, 55, 15, 48, 53, 9, 79, 92, 5])\n",
      "Time elapsed on iteration: 212.798886061\n",
      "0.964257755765\n",
      "[Iteration number 13]\n",
      "Best features so far: set([101, 70, 7, 38, 41, 42, 55, 15, 48, 53, 9, 79, 92, 5])\n",
      "Time elapsed on iteration: 211.683436871\n",
      "0.96390619492\n",
      "[Iteration number 14]\n",
      "Best features so far: set([101, 70, 7, 38, 41, 42, 55, 15, 48, 53, 9, 71, 79, 92, 5])\n",
      "Time elapsed on iteration: 217.971770048\n",
      "0.964631296239\n",
      "[Iteration number 15]\n",
      "Best features so far: set([101, 70, 7, 38, 41, 42, 55, 15, 48, 53, 9, 71, 79, 92, 29, 5])\n",
      "Time elapsed on iteration: 237.576892138\n",
      "0.965968878491\n",
      "[Iteration number 16]\n",
      "Best features so far: set([101, 70, 7, 38, 41, 42, 55, 60, 15, 48, 53, 9, 71, 79, 92, 29, 5])\n",
      "Time elapsed on iteration: 242.494128942\n",
      "0.96584020585\n",
      "[Iteration number 17]\n",
      "Best features so far: set([101, 70, 7, 38, 41, 42, 55, 60, 15, 48, 53, 9, 71, 79, 92, 29, 62, 5])\n",
      "Time elapsed on iteration: 238.540933847\n",
      "0.966668779766\n",
      "[Iteration number 18]\n",
      "Best features so far: set([101, 70, 7, 38, 41, 42, 55, 60, 15, 48, 53, 73, 9, 71, 79, 92, 29, 62, 5])\n",
      "Time elapsed on iteration: 236.43012619\n",
      "0.968303583438\n",
      "[Iteration number 19]\n",
      "Best features so far: set([101, 70, 7, 38, 41, 42, 55, 60, 15, 48, 53, 73, 9, 24, 71, 79, 92, 29, 62, 5])\n",
      "Time elapsed on iteration: 236.991391897\n",
      "0.967877605168\n",
      "[Iteration number 20]\n",
      "Best features so far: set([101, 70, 7, 38, 41, 42, 55, 60, 15, 48, 95, 53, 73, 9, 24, 71, 79, 92, 29, 62, 5])\n",
      "Time elapsed on iteration: 239.373544931\n",
      "0.967745455457\n",
      "[Iteration number 21]\n",
      "Best features so far: set([5, 7, 9, 15, 24, 29, 38, 41, 42, 47, 48, 53, 55, 60, 62, 70, 71, 73, 79, 92, 95, 101])\n",
      "Time elapsed on iteration: 239.446030855\n",
      "0.968160678073\n",
      "[Iteration number 22]\n",
      "Best features so far: set([5, 7, 9, 15, 24, 29, 38, 41, 42, 47, 48, 53, 55, 60, 62, 70, 71, 73, 79, 92, 94, 95, 101])\n",
      "Time elapsed on iteration: 243.693776131\n",
      "0.968432046944\n",
      "[Iteration number 23]\n",
      "Best features so far: set([5, 7, 9, 15, 24, 29, 38, 41, 42, 47, 48, 53, 55, 60, 62, 70, 71, 73, 79, 80, 92, 94, 95, 101])\n",
      "Time elapsed on iteration: 249.55713892\n",
      "0.968486346964\n",
      "[Iteration number 24]\n",
      "Best features so far: set([5, 7, 9, 15, 24, 25, 29, 38, 41, 42, 47, 48, 53, 55, 60, 62, 70, 71, 73, 79, 80, 92, 94, 95, 101])\n",
      "Time elapsed on iteration: 264.692312956\n",
      "0.967973962339\n",
      "[Iteration number 25]\n",
      "Best features so far: set([5, 7, 9, 15, 24, 25, 29, 35, 38, 41, 42, 47, 48, 53, 55, 60, 62, 70, 71, 73, 79, 80, 92, 94, 95, 101])\n",
      "Time elapsed on iteration: 269.20686698\n",
      "0.968917885898\n",
      "[Iteration number 26]\n",
      "Best features so far: set([5, 7, 9, 10, 15, 24, 25, 29, 35, 38, 41, 42, 47, 48, 53, 55, 60, 62, 70, 71, 73, 79, 80, 92, 94, 95, 101])\n",
      "Time elapsed on iteration: 275.020501137\n",
      "0.968112565429\n",
      "[Iteration number 27]\n",
      "Best features so far: set([5, 7, 8, 9, 10, 15, 24, 25, 29, 35, 38, 41, 42, 47, 48, 53, 55, 60, 62, 70, 71, 73, 79, 80, 92, 94, 95, 101])\n",
      "Time elapsed on iteration: 293.656612873\n",
      "0.968701658465\n",
      "[Iteration number 28]\n",
      "Best features so far: set([5, 7, 8, 9, 10, 15, 24, 25, 29, 35, 38, 41, 42, 47, 48, 53, 55, 60, 62, 70, 71, 73, 79, 80, 88, 92, 94, 95, 101])\n",
      "Time elapsed on iteration: 295.285323143\n",
      "0.968320778528\n",
      "[Iteration number 29]\n",
      "Best features so far: set([5, 7, 8, 9, 10, 15, 24, 25, 29, 35, 38, 41, 42, 47, 48, 52, 53, 55, 60, 62, 70, 71, 73, 79, 80, 88, 92, 94, 95, 101])\n",
      "Time elapsed on iteration: 293.943177938\n",
      "0.968922930489\n",
      "[Iteration number 30]\n",
      "Best features so far: set([5, 7, 8, 9, 10, 15, 24, 25, 29, 35, 38, 41, 42, 47, 48, 52, 53, 55, 60, 62, 64, 70, 71, 73, 79, 80, 88, 92, 94, 95, 101])\n",
      "Time elapsed on iteration: 280.088696003\n",
      "0.96935319046\n",
      "[Iteration number 31]\n",
      "Best features so far: set([5, 7, 8, 9, 10, 15, 24, 25, 29, 35, 38, 41, 42, 47, 48, 52, 53, 55, 60, 62, 64, 70, 71, 73, 79, 80, 88, 92, 94, 95, 100, 101])\n",
      "Time elapsed on iteration: 266.250435114\n",
      "0.969660986703\n",
      "[Iteration number 32]\n",
      "Best features so far: set([5, 7, 8, 9, 10, 15, 24, 25, 29, 35, 38, 41, 42, 47, 48, 52, 53, 55, 58, 60, 62, 64, 70, 71, 73, 79, 80, 88, 92, 94, 95, 100, 101])\n",
      "Time elapsed on iteration: 264.519531965\n",
      "0.968794598259\n",
      "[Iteration number 33]\n",
      "Best features so far: set([5, 7, 8, 9, 10, 15, 24, 25, 29, 35, 38, 41, 42, 47, 48, 52, 53, 55, 58, 60, 62, 64, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 100, 101])\n",
      "Time elapsed on iteration: 262.338778973\n",
      "0.968929507428\n",
      "[Iteration number 34]\n",
      "Best features so far: set([5, 7, 8, 9, 10, 15, 18, 24, 25, 29, 35, 38, 41, 42, 47, 48, 52, 53, 55, 58, 60, 62, 64, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 100, 101])\n",
      "Time elapsed on iteration: 262.889780045\n",
      "0.968687673217\n",
      "[Iteration number 35]\n",
      "Best features so far: set([5, 7, 8, 9, 10, 15, 18, 24, 25, 29, 35, 38, 41, 42, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 100, 101])\n",
      "Time elapsed on iteration: 281.405380964\n",
      "0.969169462239\n",
      "[Iteration number 36]\n",
      "Best features so far: set([5, 7, 8, 9, 10, 15, 18, 24, 25, 26, 29, 35, 38, 41, 42, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 100, 101])\n",
      "Time elapsed on iteration: 282.148638964\n",
      "0.969433041849\n",
      "[Iteration number 37]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 24, 25, 26, 29, 35, 38, 41, 42, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 100, 101])\n",
      "Time elapsed on iteration: 278.760056973\n",
      "0.969069423473\n",
      "[Iteration number 38]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 24, 25, 26, 29, 35, 38, 41, 42, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 275.618983984\n",
      "0.969001413377\n",
      "[Iteration number 39]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 24, 25, 26, 28, 29, 35, 38, 41, 42, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 276.709900856\n",
      "0.969293711211\n",
      "[Iteration number 40]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 24, 25, 26, 28, 29, 32, 35, 38, 41, 42, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 273.003298998\n",
      "0.9700094906\n",
      "[Iteration number 41]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 24, 25, 26, 28, 29, 32, 35, 38, 40, 41, 42, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 270.974925041\n",
      "0.969408353385\n",
      "[Iteration number 42]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 24, 25, 26, 28, 29, 32, 35, 38, 40, 41, 42, 43, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 274.116023064\n",
      "0.969699749321\n",
      "[Iteration number 43]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 24, 25, 26, 28, 29, 32, 35, 38, 40, 41, 42, 43, 45, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 271.937996864\n",
      "0.970161989623\n",
      "[Iteration number 44]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 23, 24, 25, 26, 28, 29, 32, 35, 38, 40, 41, 42, 43, 45, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 275.235450029\n",
      "0.97028610946\n",
      "[Iteration number 45]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 23, 24, 25, 26, 28, 29, 32, 35, 38, 40, 41, 42, 43, 45, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 279.448750019\n",
      "0.970494495052\n",
      "[Iteration number 46]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 23, 24, 25, 26, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 70, 71, 73, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 271.96548295\n",
      "0.970463676005\n",
      "[Iteration number 47]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 23, 24, 25, 26, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 69, 70, 71, 73, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 271.289412975\n",
      "0.970049591812\n",
      "[Iteration number 48]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 23, 24, 25, 26, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 69, 70, 71, 73, 76, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 276.27402401\n",
      "0.97000473377\n",
      "[Iteration number 49]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 23, 24, 25, 26, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 73, 76, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 271.290872097\n",
      "0.970728960915\n",
      "[Iteration number 50]\n",
      "Best features so far: set([0, 5, 7, 8, 9, 10, 15, 18, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 73, 76, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 272.74546814\n",
      "0.971597003402\n",
      "[Iteration number 51]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 15, 18, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 73, 76, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 264.956842899\n",
      "0.97034125498\n",
      "[Iteration number 52]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 15, 18, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 73, 76, 77, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 270.003602982\n",
      "0.971521324295\n",
      "[Iteration number 53]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 15, 18, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 49, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 73, 76, 77, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 263.823827982\n",
      "0.97147833038\n",
      "[Iteration number 54]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 15, 18, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 49, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 266.137653112\n",
      "0.971576772542\n",
      "[Iteration number 55]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 15, 18, 21, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 49, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 274.026966095\n",
      "0.971075089386\n",
      "[Iteration number 56]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 14, 15, 18, 21, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 49, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 274.853124142\n",
      "0.971033368171\n",
      "[Iteration number 57]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 14, 15, 18, 21, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 38, 40, 41, 42, 43, 45, 47, 48, 49, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 269.853160143\n",
      "0.971327632857\n",
      "[Iteration number 58]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 14, 15, 18, 21, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 40, 41, 42, 43, 45, 47, 48, 49, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 87, 88, 90, 92, 94, 95, 96, 100, 101])\n",
      "Time elapsed on iteration: 265.417900085\n",
      "0.972136405796\n",
      "[Iteration number 59]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 14, 15, 18, 21, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 40, 41, 42, 43, 45, 47, 48, 49, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 87, 88, 90, 92, 94, 95, 96, 98, 100, 101])\n",
      "Time elapsed on iteration: 260.952775955\n",
      "0.972119844535\n",
      "[Iteration number 60]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 14, 15, 18, 21, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 87, 88, 90, 92, 94, 95, 96, 98, 100, 101])\n",
      "Time elapsed on iteration: 252.896040916\n",
      "0.971031299912\n",
      "[Iteration number 61]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 14, 15, 18, 20, 21, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 87, 88, 90, 92, 94, 95, 96, 98, 100, 101])\n",
      "Time elapsed on iteration: 245.860454082\n",
      "0.97083123553\n",
      "[Iteration number 62]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 14, 15, 18, 20, 21, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 87, 88, 90, 92, 94, 95, 96, 98, 100, 101])\n",
      "Time elapsed on iteration: 244.818893909\n",
      "0.971368424507\n",
      "[Iteration number 63]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 14, 15, 18, 20, 21, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 100, 101])\n",
      "Time elapsed on iteration: 253.73291111\n",
      "0.970848347793\n",
      "[Iteration number 64]\n",
      "Best features so far: set([0, 4, 5, 7, 8, 9, 10, 14, 15, 16, 18, 20, 21, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 100, 101])\n",
      "Time elapsed on iteration: 244.409634113\n",
      "0.970966490645\n",
      "[Iteration number 65]\n",
      "Best features so far: set([0, 2, 4, 5, 7, 8, 9, 10, 14, 15, 16, 18, 20, 21, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 100, 101])\n",
      "Time elapsed on iteration: 243.758829832\n",
      "0.970237571164\n",
      "[Iteration number 66]\n",
      "Best features so far: set([0, 2, 4, 5, 7, 8, 9, 10, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 100, 101])\n",
      "Time elapsed on iteration: 246.390058994\n",
      "0.971839910052\n",
      "[Iteration number 67]\n",
      "Best features so far: set([0, 2, 4, 5, 7, 8, 9, 10, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 100, 101])\n",
      "Time elapsed on iteration: 247.372740984\n",
      "0.971698480303\n",
      "[Iteration number 68]\n",
      "Best features so far: set([0, 2, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 100, 101])\n",
      "Time elapsed on iteration: 234.899163008\n",
      "0.970428807862\n",
      "[Iteration number 69]\n",
      "Best features so far: set([0, 2, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 83, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 100, 101])\n",
      "Time elapsed on iteration: 232.059253931\n",
      "0.970997393703\n",
      "[Iteration number 70]\n",
      "Best features so far: set([0, 2, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 83, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 227.477548122\n",
      "0.970098598181\n",
      "[Iteration number 71]\n",
      "Best features so far: set([0, 2, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 83, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 219.212450027\n",
      "0.970277491284\n",
      "[Iteration number 72]\n",
      "Best features so far: set([0, 2, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 83, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 214.933003902\n",
      "0.971819228369\n",
      "[Iteration number 73]\n",
      "Best features so far: set([0, 2, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 83, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 208.080060959\n",
      "0.971245838322\n",
      "[Iteration number 74]\n",
      "Best features so far: set([0, 2, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 79, 80, 83, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 204.233115196\n",
      "0.970249128606\n",
      "[Iteration number 75]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 79, 80, 83, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 194.202528954\n",
      "0.971075544131\n",
      "[Iteration number 76]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 190.990918159\n",
      "0.970144018543\n",
      "[Iteration number 77]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 184.817607164\n",
      "0.971071084244\n",
      "[Iteration number 78]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 181.902523994\n",
      "0.971078342096\n",
      "[Iteration number 79]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 172.804296017\n",
      "0.971232963212\n",
      "[Iteration number 80]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 175.890362024\n",
      "0.969651420448\n",
      "[Iteration number 81]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 168.028237104\n",
      "0.970042643097\n",
      "[Iteration number 82]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 156.181957006\n",
      "0.970211350888\n",
      "[Iteration number 83]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 164.37279892\n",
      "0.970107458798\n",
      "[Iteration number 84]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 154.862375975\n",
      "0.970741902255\n",
      "[Iteration number 85]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 1493.81071305\n",
      "0.970670357214\n",
      "[Iteration number 86]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 131.270883083\n",
      "0.970369400914\n",
      "[Iteration number 87]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 86, 87, 88, 89, 90, 92, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 124.969609022\n",
      "0.970807902622\n",
      "[Iteration number 88]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 117.408333063\n",
      "0.970839117813\n",
      "[Iteration number 89]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 109.086338043\n",
      "0.970248743294\n",
      "[Iteration number 90]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 104.588605881\n",
      "0.970871128987\n",
      "[Iteration number 91]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 93.63667202\n",
      "0.969859572095\n",
      "[Iteration number 92]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 86.2960078716\n",
      "0.969068141735\n",
      "[Iteration number 93]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 79.2057259083\n",
      "0.970153824337\n",
      "[Iteration number 94]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 75.0260031223\n",
      "0.969266953936\n",
      "[Iteration number 95]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 63.1761779785\n",
      "0.968507913453\n",
      "[Iteration number 96]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 56.4753479958\n",
      "0.969474248371\n",
      "[Iteration number 97]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 46.1526150703\n",
      "0.970004730291\n",
      "[Iteration number 98]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 38.8905398846\n",
      "0.970131179324\n",
      "[Iteration number 99]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 29.1461470127\n",
      "0.969609814889\n",
      "[Iteration number 100]\n",
      "Best features so far: set([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 19.2989060879\n",
      "0.966506998469\n",
      "[Iteration number 101]\n",
      "Best features so far: set([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101])\n",
      "Time elapsed on iteration: 9.7848470211\n",
      "22459.689934\n"
     ]
    }
   ],
   "source": [
    "model = VotingClassifier([\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=50, learning_rate=0.03, max_depth=3)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=50))\n",
    "    ], voting='soft')\n",
    "\n",
    "t = time.time()\n",
    "wrapper_best_features, wrapper_best_score, wrapper_all_features, wrapper_all_scores = forward_selection(spam_X_train, spam_y_train, 2, model)\n",
    "print time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tosave = (wrapper_best_features, wrapper_best_score, wrapper_all_features, wrapper_all_scores)\n",
    "wrapper_file = open('wrapper.pkl', 'wb')\n",
    "pickle.dump(tosave, wrapper_file)\n",
    "wrapper_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
    "model.fit(spam_X_train[:, list(best_features)], spam_y_train)\n",
    "\n",
    "model_pred = model.predict(spam_X_test[:, list(best_features)])\n",
    "\n",
    "print f1_score(np.array((spam_y_test+1)/2, dtype=int), np.array((model_pred+1)/2, dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedded_features, embedded_scores = zip(*voting_clf.get_feature_scores().most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 70, 101, 49, 8, 71, 10, 9, 29, 35, 52, 53, 4, 32, 75, 41, 17, 56, 19, 14, 76, 73, 47, 31, 3, 15, 43, 55, 16, 42, 45, 50, 83, 80, 33, 78, 51, 72, 87, 38, 100, 37, 11, 39, 79, 20, 24, 30, 22, 54, 27, 21, 44, 12, 13, 74, 96, 82, 62, 0, 85, 46, 28, 23, 18, 61, 59, 81, 88, 95, 36, 6, 89, 90, 7, 93, 92, 84, 40, 86, 25, 26, 5, 91, 63, 57, 99, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "print embedded_features\n",
    "\n",
    "embedded_f1_scores = []\n",
    "\n",
    "for i in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "    embed_clf = VotingClassifier([\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=300, learning_rate=0.01, max_depth=3)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=300, max_features=1.0))\n",
    "    ], voting='soft')\n",
    "    features = sorted(embedded_features[:i])\n",
    "    embed_clf.fit(spam_X_train[:, features], spam_y_train)\n",
    "    pred = embed_clf.predict(spam_X_test[:, features])\n",
    "    embedded_f1_scores.append(f1_score(np.array((spam_y_test+1)/2, dtype=int), np.array((pred+1)/2, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfs_features, cfs_all_score, cfs_all_features = CFS(spam_X_train, spam_y_train)\n",
    "_, cfs_scores = zip(*cfs_all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_f1_scores = []\n",
    "\n",
    "filter_features = map(lambda x: x[0], cfs_all_features)\n",
    "\n",
    "for i in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "    filter_clf = VotingClassifier([\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=300, learning_rate=0.01, max_depth=3)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=300, max_features=1.0))\n",
    "    ], voting='soft')\n",
    "    features = sorted(filter_features[:i])\n",
    "    filter_clf.fit(spam_X_train[:, features], spam_y_train)\n",
    "    pred = filter_clf.predict(spam_X_test[:, features])\n",
    "    filter_f1_scores.append(f1_score(np.array((spam_y_test+1)/2, dtype=int), np.array((pred+1)/2, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ef466a610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], embedded_f1_scores, label='embedded', color='g')\n",
    "plt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], filter_f1_scores, label='filter CFS', color='r')\n",
    "plt.plot(wrapper_all_scores, label='Wrapper', color='b')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = []\n",
    "with open('wrapper.time', 'r') as f:\n",
    "    times = map(float, f.readlines())\n",
    "    \n",
    "plt.plot(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.data', delimiter=',')[50:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = {\n",
    "    \"Iris-setosa\": 0,\n",
    "    \"Iris-versicolor\": 1,\n",
    "    \"Iris-virginica\": 0\n",
    "}\n",
    "\n",
    "df[\"num_class\"] = df[\"class\"].apply(lambda s: classes[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = shuffle(df.drop(\"class\", axis=1), train_percent=0.6)\n",
    "print \"Train size: {}; Test size: {};\".format(X_train.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cart_clf = CART(max_depth=10, impurity='mse')\n",
    "cart_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = cart_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print \"Log loss: {}\".format(log_loss(y_test, pred))\n",
    "#print \"Accuracy: {}\".format(cart_clf.score(X_test, y_test))\n",
    "print \"MSE: {}\".format(mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tree = DecisionTreeClassifier(max_depth=10, random_state=1)\n",
    "tree = DecisionTreeRegressor(max_depth=10, random_state=1)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print \"Log loss: {}\".format(log_loss(y_test, sigmoid(pr)))\n",
    "#print \"Log loss: {}\".format(log_loss(y_test, pr))\n",
    "#print \"Accuracy: {}\".format(tree.score(X_test, y_test))\n",
    "print \"MSE: {}\".format(mean_squared_error(y_test, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train[y_train == 0] = -1\n",
    "y_test[y_test == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_clf_1 = RandomForest(n_estimators=100, max_depth=10)\n",
    "forest_clf_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest_clf_1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, max_depth=10, max_features=1.0)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_clf_1 = GradientBoosting(n_estimators=100, mu=0.1)\n",
    "grad_clf_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad_clf_1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad = GradientBoostingClassifier(n_estimators=100, max_depth=5)\n",
    "#grad = GradientBoostingRegressor(n_estimators=100, max_depth=5)\n",
    "grad.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print log_loss(y_test, grad.predict(X_test))\n",
    "#grad.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.349723</td>\n",
       "      <td> 0.658872</td>\n",
       "      <td> 0.341822</td>\n",
       "      <td> 0.369098</td>\n",
       "      <td> 0.448115</td>\n",
       "      <td> 0.327417</td>\n",
       "      <td> 0.517556</td>\n",
       "      <td> 0.393646</td>\n",
       "      <td> 0.430504</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.470654</td>\n",
       "      <td> 0.410545</td>\n",
       "      <td> 0.590262</td>\n",
       "      <td> 0.450158</td>\n",
       "      <td> 0.443301</td>\n",
       "      <td> 0.467284</td>\n",
       "      <td> 0.438850</td>\n",
       "      <td> 0.413235</td>\n",
       "      <td> 0.433625</td>\n",
       "      <td> 0.575691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.320356</td>\n",
       "      <td> 0.214419</td>\n",
       "      <td> 0.796892</td>\n",
       "      <td> 0.283771</td>\n",
       "      <td> 0.429499</td>\n",
       "      <td> 0.336705</td>\n",
       "      <td> 0.209530</td>\n",
       "      <td> 0.411694</td>\n",
       "      <td> 0.620735</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.470654</td>\n",
       "      <td> 0.410545</td>\n",
       "      <td> 0.454687</td>\n",
       "      <td> 0.450158</td>\n",
       "      <td> 0.443301</td>\n",
       "      <td> 0.467284</td>\n",
       "      <td> 0.438850</td>\n",
       "      <td> 0.413235</td>\n",
       "      <td> 0.433625</td>\n",
       "      <td> 0.354707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.575150</td>\n",
       "      <td> 0.658872</td>\n",
       "      <td> 0.341822</td>\n",
       "      <td> 0.541797</td>\n",
       "      <td> 0.430258</td>\n",
       "      <td> 0.575468</td>\n",
       "      <td> 0.509843</td>\n",
       "      <td> 0.518629</td>\n",
       "      <td> 0.383852</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.470654</td>\n",
       "      <td> 0.410545</td>\n",
       "      <td> 0.454687</td>\n",
       "      <td> 0.450158</td>\n",
       "      <td> 0.443301</td>\n",
       "      <td> 0.467284</td>\n",
       "      <td> 0.992203</td>\n",
       "      <td> 0.768557</td>\n",
       "      <td> 0.433625</td>\n",
       "      <td> 0.391791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.349723</td>\n",
       "      <td> 0.658872</td>\n",
       "      <td> 0.341822</td>\n",
       "      <td> 0.440102</td>\n",
       "      <td> 0.455950</td>\n",
       "      <td> 0.327417</td>\n",
       "      <td> 0.560001</td>\n",
       "      <td> 0.398133</td>\n",
       "      <td> 0.376336</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.470654</td>\n",
       "      <td> 0.410545</td>\n",
       "      <td> 0.607055</td>\n",
       "      <td> 0.450158</td>\n",
       "      <td> 0.443301</td>\n",
       "      <td> 0.467284</td>\n",
       "      <td> 0.438850</td>\n",
       "      <td> 0.413235</td>\n",
       "      <td> 0.433625</td>\n",
       "      <td> 0.550478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.320356</td>\n",
       "      <td> 0.658872</td>\n",
       "      <td> 0.341822</td>\n",
       "      <td> 0.385197</td>\n",
       "      <td> 0.437169</td>\n",
       "      <td> 0.709301</td>\n",
       "      <td> 0.419971</td>\n",
       "      <td> 0.288835</td>\n",
       "      <td> 0.382394</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.470654</td>\n",
       "      <td> 0.410545</td>\n",
       "      <td> 0.454687</td>\n",
       "      <td> 0.450158</td>\n",
       "      <td> 0.443301</td>\n",
       "      <td> 0.467284</td>\n",
       "      <td> 0.438850</td>\n",
       "      <td> 0.413235</td>\n",
       "      <td> 0.433625</td>\n",
       "      <td> 0.741449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7    \\\n",
       "0    1  0.349723  0.658872  0.341822  0.369098  0.448115  0.327417  0.517556   \n",
       "1    1  0.320356  0.214419  0.796892  0.283771  0.429499  0.336705  0.209530   \n",
       "2    1  0.575150  0.658872  0.341822  0.541797  0.430258  0.575468  0.509843   \n",
       "3    1  0.349723  0.658872  0.341822  0.440102  0.455950  0.327417  0.560001   \n",
       "4    1  0.320356  0.658872  0.341822  0.385197  0.437169  0.709301  0.419971   \n",
       "\n",
       "        8         9      ...          93        94        95        96   \\\n",
       "0  0.393646  0.430504    ...     0.470654  0.410545  0.590262  0.450158   \n",
       "1  0.411694  0.620735    ...     0.470654  0.410545  0.454687  0.450158   \n",
       "2  0.518629  0.383852    ...     0.470654  0.410545  0.454687  0.450158   \n",
       "3  0.398133  0.376336    ...     0.470654  0.410545  0.607055  0.450158   \n",
       "4  0.288835  0.382394    ...     0.470654  0.410545  0.454687  0.450158   \n",
       "\n",
       "        97        98        99        100       101       102  \n",
       "0  0.443301  0.467284  0.438850  0.413235  0.433625  0.575691  \n",
       "1  0.443301  0.467284  0.438850  0.413235  0.433625  0.354707  \n",
       "2  0.443301  0.467284  0.992203  0.768557  0.433625  0.391791  \n",
       "3  0.443301  0.467284  0.438850  0.413235  0.433625  0.550478  \n",
       "4  0.443301  0.467284  0.438850  0.413235  0.433625  0.741449  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_train_df = pd.read_csv('spam.train.txt', header=None, delimiter=\" \")\n",
    "spam_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_X_train = np.copy(spam_train_df.values)\n",
    "np.random.shuffle(spam_X_train)\n",
    "spam_X_train, spam_y_train = spam_X_train[:, 1:], spam_X_train[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.445622</td>\n",
       "      <td> 0.632602</td>\n",
       "      <td> 0.368683</td>\n",
       "      <td> 0.728974</td>\n",
       "      <td> 0.479510</td>\n",
       "      <td> 0.471833</td>\n",
       "      <td> 0.571882</td>\n",
       "      <td> 0.579280</td>\n",
       "      <td> 0.604702</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.462762</td>\n",
       "      <td> 0.448051</td>\n",
       "      <td> 0.446374</td>\n",
       "      <td> 0.424879</td>\n",
       "      <td> 0.472828</td>\n",
       "      <td> 0.450815</td>\n",
       "      <td> 0.427943</td>\n",
       "      <td> 0.416184</td>\n",
       "      <td> 0.446429</td>\n",
       "      <td> 0.667609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.296747</td>\n",
       "      <td> 0.632602</td>\n",
       "      <td> 0.368683</td>\n",
       "      <td> 0.330429</td>\n",
       "      <td> 0.374055</td>\n",
       "      <td> 0.451231</td>\n",
       "      <td> 0.249144</td>\n",
       "      <td> 0.431370</td>\n",
       "      <td> 0.344248</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.462762</td>\n",
       "      <td> 0.448051</td>\n",
       "      <td> 0.916790</td>\n",
       "      <td> 0.424879</td>\n",
       "      <td> 0.472828</td>\n",
       "      <td> 0.368804</td>\n",
       "      <td> 0.464272</td>\n",
       "      <td> 0.400264</td>\n",
       "      <td> 0.547989</td>\n",
       "      <td> 0.580435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.296747</td>\n",
       "      <td> 0.632602</td>\n",
       "      <td> 0.368683</td>\n",
       "      <td> 0.309466</td>\n",
       "      <td> 0.392561</td>\n",
       "      <td> 0.458103</td>\n",
       "      <td> 0.289492</td>\n",
       "      <td> 0.595944</td>\n",
       "      <td> 0.574234</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.462762</td>\n",
       "      <td> 0.448051</td>\n",
       "      <td> 0.798868</td>\n",
       "      <td> 0.424879</td>\n",
       "      <td> 0.472828</td>\n",
       "      <td> 0.368804</td>\n",
       "      <td> 0.472316</td>\n",
       "      <td> 0.415417</td>\n",
       "      <td> 0.446429</td>\n",
       "      <td> 0.781863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.296747</td>\n",
       "      <td> 0.184437</td>\n",
       "      <td> 0.831801</td>\n",
       "      <td> 0.239714</td>\n",
       "      <td> 0.494042</td>\n",
       "      <td> 0.716275</td>\n",
       "      <td> 0.295862</td>\n",
       "      <td> 0.349747</td>\n",
       "      <td> 0.434139</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.462762</td>\n",
       "      <td> 0.448051</td>\n",
       "      <td> 0.446374</td>\n",
       "      <td> 0.424879</td>\n",
       "      <td> 0.472828</td>\n",
       "      <td> 0.368804</td>\n",
       "      <td> 0.427943</td>\n",
       "      <td> 0.854188</td>\n",
       "      <td> 0.543903</td>\n",
       "      <td> 0.562639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.368046</td>\n",
       "      <td> 0.632602</td>\n",
       "      <td> 0.368683</td>\n",
       "      <td> 0.280957</td>\n",
       "      <td> 0.380190</td>\n",
       "      <td> 0.445567</td>\n",
       "      <td> 0.339384</td>\n",
       "      <td> 0.534049</td>\n",
       "      <td> 0.522703</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.462762</td>\n",
       "      <td> 0.448051</td>\n",
       "      <td> 0.446374</td>\n",
       "      <td> 0.533793</td>\n",
       "      <td> 0.472828</td>\n",
       "      <td> 0.812761</td>\n",
       "      <td> 0.473772</td>\n",
       "      <td> 0.400264</td>\n",
       "      <td> 0.446429</td>\n",
       "      <td> 0.659760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7    \\\n",
       "0    1  0.445622  0.632602  0.368683  0.728974  0.479510  0.471833  0.571882   \n",
       "1    1  0.296747  0.632602  0.368683  0.330429  0.374055  0.451231  0.249144   \n",
       "2    1  0.296747  0.632602  0.368683  0.309466  0.392561  0.458103  0.289492   \n",
       "3    1  0.296747  0.184437  0.831801  0.239714  0.494042  0.716275  0.295862   \n",
       "4    1  0.368046  0.632602  0.368683  0.280957  0.380190  0.445567  0.339384   \n",
       "\n",
       "        8         9      ...          93        94        95        96   \\\n",
       "0  0.579280  0.604702    ...     0.462762  0.448051  0.446374  0.424879   \n",
       "1  0.431370  0.344248    ...     0.462762  0.448051  0.916790  0.424879   \n",
       "2  0.595944  0.574234    ...     0.462762  0.448051  0.798868  0.424879   \n",
       "3  0.349747  0.434139    ...     0.462762  0.448051  0.446374  0.424879   \n",
       "4  0.534049  0.522703    ...     0.462762  0.448051  0.446374  0.533793   \n",
       "\n",
       "        97        98        99        100       101       102  \n",
       "0  0.472828  0.450815  0.427943  0.416184  0.446429  0.667609  \n",
       "1  0.472828  0.368804  0.464272  0.400264  0.547989  0.580435  \n",
       "2  0.472828  0.368804  0.472316  0.415417  0.446429  0.781863  \n",
       "3  0.472828  0.368804  0.427943  0.854188  0.543903  0.562639  \n",
       "4  0.472828  0.812761  0.473772  0.400264  0.446429  0.659760  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_test_df = pd.read_csv('spam.test.txt', header=None, delimiter=\" \")\n",
    "spam_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_X_test = np.copy(spam_test_df.values)\n",
    "np.random.shuffle(spam_X_test)\n",
    "spam_X_test, spam_y_test = spam_X_test[:, 1:], spam_X_test[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_y_train[spam_y_train == 0] = -1\n",
    "spam_y_test[spam_y_test == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest with 300 estimators\n",
      "Fitting estimator number 1\n",
      "Fitting estimator number 2\n",
      "Fitting estimator number 3\n",
      "Fitting estimator number 4\n",
      "Fitting estimator number 5\n",
      "Fitting estimator number 6\n",
      "Fitting estimator number 7\n",
      "Fitting estimator number 8\n",
      "Fitting estimator number 9\n",
      "Fitting estimator number 10\n",
      "Fitting estimator number 11\n",
      "Fitting estimator number 12\n",
      "Fitting estimator number 13\n",
      "Fitting estimator number 14\n",
      "Fitting estimator number 15\n",
      "Fitting estimator number 16\n",
      "Fitting estimator number 17\n",
      "Fitting estimator number 18\n",
      "Fitting estimator number 19\n",
      "Fitting estimator number 20\n",
      "Fitting estimator number 21\n",
      "Fitting estimator number 22\n",
      "Fitting estimator number 23\n",
      "Fitting estimator number 24\n",
      "Fitting estimator number 25\n",
      "Fitting estimator number 26\n",
      "Fitting estimator number 27\n",
      "Fitting estimator number 28\n",
      "Fitting estimator number 29\n",
      "Fitting estimator number 30\n",
      "Fitting estimator number 31\n",
      "Fitting estimator number 32\n",
      "Fitting estimator number 33\n",
      "Fitting estimator number 34\n",
      "Fitting estimator number 35\n",
      "Fitting estimator number 36\n",
      "Fitting estimator number 37\n",
      "Fitting estimator number 38\n",
      "Fitting estimator number 39\n",
      "Fitting estimator number 40\n",
      "Fitting estimator number 41\n",
      "Fitting estimator number 42\n",
      "Fitting estimator number 43\n",
      "Fitting estimator number 44\n",
      "Fitting estimator number 45\n",
      "Fitting estimator number 46\n",
      "Fitting estimator number 47\n",
      "Fitting estimator number 48\n",
      "Fitting estimator number 49\n",
      "Fitting estimator number 50\n",
      "Fitting estimator number 51\n",
      "Fitting estimator number 52\n",
      "Fitting estimator number 53\n",
      "Fitting estimator number 54\n",
      "Fitting estimator number 55\n",
      "Fitting estimator number 56\n",
      "Fitting estimator number 57\n",
      "Fitting estimator number 58\n",
      "Fitting estimator number 59\n",
      "Fitting estimator number 60\n",
      "Fitting estimator number 61\n",
      "Fitting estimator number 62\n",
      "Fitting estimator number 63\n",
      "Fitting estimator number 64\n",
      "Fitting estimator number 65\n",
      "Fitting estimator number 66\n",
      "Fitting estimator number 67\n",
      "Fitting estimator number 68\n",
      "Fitting estimator number 69\n",
      "Fitting estimator number 70\n",
      "Fitting estimator number 71\n",
      "Fitting estimator number 72\n",
      "Fitting estimator number 73\n",
      "Fitting estimator number 74\n",
      "Fitting estimator number 75\n",
      "Fitting estimator number 76\n",
      "Fitting estimator number 77\n",
      "Fitting estimator number 78\n",
      "Fitting estimator number 79\n",
      "Fitting estimator number 80\n",
      "Fitting estimator number 81\n",
      "Fitting estimator number 82\n",
      "Fitting estimator number 83\n",
      "Fitting estimator number 84\n",
      "Fitting estimator number 85\n",
      "Fitting estimator number 86\n",
      "Fitting estimator number 87\n",
      "Fitting estimator number 88\n",
      "Fitting estimator number 89\n",
      "Fitting estimator number 90\n",
      "Fitting estimator number 91\n",
      "Fitting estimator number 92\n",
      "Fitting estimator number 93\n",
      "Fitting estimator number 94\n",
      "Fitting estimator number 95\n",
      "Fitting estimator number 96\n",
      "Fitting estimator number 97\n",
      "Fitting estimator number 98\n",
      "Fitting estimator number 99\n",
      "Fitting estimator number 100\n",
      "Fitting estimator number 101\n",
      "Fitting estimator number 102\n",
      "Fitting estimator number 103\n",
      "Fitting estimator number 104\n",
      "Fitting estimator number 105\n",
      "Fitting estimator number 106\n",
      "Fitting estimator number 107\n",
      "Fitting estimator number 108\n",
      "Fitting estimator number 109\n",
      "Fitting estimator number 110\n",
      "Fitting estimator number 111\n",
      "Fitting estimator number 112\n",
      "Fitting estimator number 113\n",
      "Fitting estimator number 114\n",
      "Fitting estimator number 115\n",
      "Fitting estimator number 116\n",
      "Fitting estimator number 117\n",
      "Fitting estimator number 118\n",
      "Fitting estimator number 119\n",
      "Fitting estimator number 120\n",
      "Fitting estimator number 121\n",
      "Fitting estimator number 122\n",
      "Fitting estimator number 123\n",
      "Fitting estimator number 124\n",
      "Fitting estimator number 125\n",
      "Fitting estimator number 126\n",
      "Fitting estimator number 127\n",
      "Fitting estimator number 128\n",
      "Fitting estimator number 129\n",
      "Fitting estimator number 130\n",
      "Fitting estimator number 131\n",
      "Fitting estimator number 132\n",
      "Fitting estimator number 133\n",
      "Fitting estimator number 134\n",
      "Fitting estimator number 135\n",
      "Fitting estimator number 136\n",
      "Fitting estimator number 137\n",
      "Fitting estimator number 138\n",
      "Fitting estimator number 139\n",
      "Fitting estimator number 140\n",
      "Fitting estimator number 141\n",
      "Fitting estimator number 142\n",
      "Fitting estimator number 143\n",
      "Fitting estimator number 144\n",
      "Fitting estimator number 145\n",
      "Fitting estimator number 146\n",
      "Fitting estimator number 147\n",
      "Fitting estimator number 148\n",
      "Fitting estimator number 149\n",
      "Fitting estimator number 150\n",
      "Fitting estimator number 151\n",
      "Fitting estimator number 152\n",
      "Fitting estimator number 153\n",
      "Fitting estimator number 154\n",
      "Fitting estimator number 155\n",
      "Fitting estimator number 156\n",
      "Fitting estimator number 157\n",
      "Fitting estimator number 158\n",
      "Fitting estimator number 159\n",
      "Fitting estimator number 160\n",
      "Fitting estimator number 161\n",
      "Fitting estimator number 162\n",
      "Fitting estimator number 163\n",
      "Fitting estimator number 164\n",
      "Fitting estimator number 165\n",
      "Fitting estimator number 166\n",
      "Fitting estimator number 167\n",
      "Fitting estimator number 168\n",
      "Fitting estimator number 169\n",
      "Fitting estimator number 170\n",
      "Fitting estimator number 171\n",
      "Fitting estimator number 172\n",
      "Fitting estimator number 173\n",
      "Fitting estimator number 174\n",
      "Fitting estimator number 175\n",
      "Fitting estimator number 176\n",
      "Fitting estimator number 177\n",
      "Fitting estimator number 178\n",
      "Fitting estimator number 179\n",
      "Fitting estimator number 180\n",
      "Fitting estimator number 181\n",
      "Fitting estimator number 182\n",
      "Fitting estimator number 183\n",
      "Fitting estimator number 184\n",
      "Fitting estimator number 185\n",
      "Fitting estimator number 186\n",
      "Fitting estimator number 187\n",
      "Fitting estimator number 188\n",
      "Fitting estimator number 189\n",
      "Fitting estimator number 190\n",
      "Fitting estimator number 191\n",
      "Fitting estimator number 192\n",
      "Fitting estimator number 193\n",
      "Fitting estimator number 194\n",
      "Fitting estimator number 195\n",
      "Fitting estimator number 196\n",
      "Fitting estimator number 197\n",
      "Fitting estimator number 198\n",
      "Fitting estimator number 199\n",
      "Fitting estimator number 200\n",
      "Fitting estimator number 201\n",
      "Fitting estimator number 202\n",
      "Fitting estimator number 203\n",
      "Fitting estimator number 204\n",
      "Fitting estimator number 205\n",
      "Fitting estimator number 206\n",
      "Fitting estimator number 207\n",
      "Fitting estimator number 208\n",
      "Fitting estimator number 209\n",
      "Fitting estimator number 210\n",
      "Fitting estimator number 211\n",
      "Fitting estimator number 212\n",
      "Fitting estimator number 213\n",
      "Fitting estimator number 214\n",
      "Fitting estimator number 215\n",
      "Fitting estimator number 216\n",
      "Fitting estimator number 217\n",
      "Fitting estimator number 218\n",
      "Fitting estimator number 219\n",
      "Fitting estimator number 220\n",
      "Fitting estimator number 221\n",
      "Fitting estimator number 222\n",
      "Fitting estimator number 223\n",
      "Fitting estimator number 224\n",
      "Fitting estimator number 225\n",
      "Fitting estimator number 226\n",
      "Fitting estimator number 227\n",
      "Fitting estimator number 228\n",
      "Fitting estimator number 229\n",
      "Fitting estimator number 230\n",
      "Fitting estimator number 231\n",
      "Fitting estimator number 232\n",
      "Fitting estimator number 233\n",
      "Fitting estimator number 234\n",
      "Fitting estimator number 235\n",
      "Fitting estimator number 236\n",
      "Fitting estimator number 237\n",
      "Fitting estimator number 238\n",
      "Fitting estimator number 239\n",
      "Fitting estimator number 240\n",
      "Fitting estimator number 241\n",
      "Fitting estimator number 242\n",
      "Fitting estimator number 243\n",
      "Fitting estimator number 244\n",
      "Fitting estimator number 245\n",
      "Fitting estimator number 246\n",
      "Fitting estimator number 247\n",
      "Fitting estimator number 248\n",
      "Fitting estimator number 249\n",
      "Fitting estimator number 250\n",
      "Fitting estimator number 251\n",
      "Fitting estimator number 252\n",
      "Fitting estimator number 253\n",
      "Fitting estimator number 254\n",
      "Fitting estimator number 255\n",
      "Fitting estimator number 256\n",
      "Fitting estimator number 257\n",
      "Fitting estimator number 258\n",
      "Fitting estimator number 259\n",
      "Fitting estimator number 260\n",
      "Fitting estimator number 261\n",
      "Fitting estimator number 262\n",
      "Fitting estimator number 263\n",
      "Fitting estimator number 264\n",
      "Fitting estimator number 265\n",
      "Fitting estimator number 266\n",
      "Fitting estimator number 267\n",
      "Fitting estimator number 268\n",
      "Fitting estimator number 269\n",
      "Fitting estimator number 270\n",
      "Fitting estimator number 271\n",
      "Fitting estimator number 272\n",
      "Fitting estimator number 273\n",
      "Fitting estimator number 274\n",
      "Fitting estimator number 275\n",
      "Fitting estimator number 276\n",
      "Fitting estimator number 277\n",
      "Fitting estimator number 278\n",
      "Fitting estimator number 279\n",
      "Fitting estimator number 280\n",
      "Fitting estimator number 281\n",
      "Fitting estimator number 282\n",
      "Fitting estimator number 283\n",
      "Fitting estimator number 284\n",
      "Fitting estimator number 285\n",
      "Fitting estimator number 286\n",
      "Fitting estimator number 287\n",
      "Fitting estimator number 288\n",
      "Fitting estimator number 289\n",
      "Fitting estimator number 290\n",
      "Fitting estimator number 291\n",
      "Fitting estimator number 292\n",
      "Fitting estimator number 293\n",
      "Fitting estimator number 294\n",
      "Fitting estimator number 295\n",
      "Fitting estimator number 296\n",
      "Fitting estimator number 297\n",
      "Fitting estimator number 298\n",
      "Fitting estimator number 299\n",
      "Fitting estimator number 300\n",
      "OOB score: 0.92186377779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:27: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:28: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "forest_clf = RandomForest(n_estimators=300, poi=1.0, pof=1.0)\n",
    "forest_clf.fit(spam_X_train, spam_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "\n",
    "forest_loss = []\n",
    "forest_f1 = []\n",
    "\n",
    "for e in forest_clf.estimators:\n",
    "    pr = e.predict(spam_X_test)\n",
    "    s += pr\n",
    "    p = np.sign(s)\n",
    "    forest_loss.append(log_loss(spam_y_test, p))    \n",
    "    forest_f1.append(f1_score(np.array((spam_y_test+1)/2, dtype=int),\n",
    "                              np.array((p+1)/2, dtype=int)))\n",
    "\n",
    "forest_sk_loss = []\n",
    "forest_sk_f1 = []\n",
    "\n",
    "forest_tree = RandomForestClassifier(random_state=1, max_features=forest_clf.pof, warm_start=True)\n",
    "for i in xrange(3, forest_clf.n_estimators):\n",
    "    print i\n",
    "    forest_tree.set_params(n_estimators=i)\n",
    "    forest_tree.fit(spam_X_train, spam_y_train)\n",
    "    p = forest_tree.predict(spam_X_test)    \n",
    "    \n",
    "    forest_sk_loss.append(log_loss(spam_y_test, p))\n",
    "    forest_sk_f1.append(f1_score(np.array((spam_y_test+1)/2, dtype=int), \n",
    "                                 np.array((p+1)/2, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd4VNXWxt+dSe+VEAih9yaIdCGCgICKiheliL3rtVxF\n/WxwvRbsegUVFAuIoBcsCApSgkjvBAQpoSYkISG9TKbs7493TiaEJECSGSCs3/PMMzPn7LPbmXn3\n2muXo7TWEARBEC5+PM53BgRBEITaQQRdEAShjiCCLgiCUEcQQRcEQagjiKALgiDUEUTQBUEQ6ggi\n6EK1UEolKKXuPt/5uJhQSuUppZqc73wIdRcR9DqIUuqQUmqgi5PRjpdQARU1eFrrIK31IRekdYdS\nalVtxytcfIig101EbM8/F039K6VM5zsPQu0ggn4JoZTyUUq9r5RKdrzeU0p5lzk/QSmVopQ6ppS6\nRyllV0o1O4t4lVLqBUfPIE0p9ZVSKthxzlcpNUsplaGUylJKbVBK1XOcu0MpdUAplauUSlJKjakg\n7gZKqUKlVFiZY12UUieUUialVAul1EqlVLbj2JxK8tjEUZ7xSqnDjrD/d5Zle1Yptd9RhrlGXior\nm1LqVQBXAvjI4Wb50BG+tD6VUl8qpaYqpRY5wqxSStVXSn3giGu3UuqyMvkw8pCrlNqllLrBcbwt\ngI8B9HLEc9JxPEQp9bVSKt1xX55XSqky9b5aKfWuUioDwMtnW4/CBY7WWl517AXgIIABFRz/N4A1\nACIdr9UA/u04dw2A4wDaAvADMAuADUCzStJYAeAux+e7AOwD0ARAAIB5AL52nLsfwM8AfAEoAF0A\nBDnC5QBo6QgXDaBdJWktA3BPme9vAZjq+PwtgOccn70B9K4kjiYA7AA+BeADoBOAYgBtzlCXjznq\nrAEALwCfAJhdVdnK10+ZuOxGfQL4EsAJxzU+jjIeAjDOEdcrAJaXufZmAPUdn0cByAcQ7fh+O4BV\n5dL6GsAPjnpuDODvMvfrDgAWAA+DRp3v2dajvC7sl1jolxZjQAHP0FpnAJgE4DbHuVEAZmitd2ut\niwC8DArL2TAWwDta60Na6wIAzwG41dGVLwEQAQq31lpv1VrnOa6zA+iolPLTWqdprf+qJP7ZAEYD\ntJgB3OI4Bkf8TZRSDbXWJVrrNWfI6ySttVlrvQPAdgCdzxD+fgAvaK1TtNYWsM5uPouyAVXXnwYw\n33GNGRTfAq31LK21BvAdKPYMrPX/tNapjs/fgQ1oj4rSceTtFlCgC7TWhwG8A+e9BoAUrfUUrbVd\na12Mc69H4QJEBP3SogGAw2W+H3EcA4AYAEfLnDt2DvHGVBCvJ4B6AGYCWAxgjsPNM1kp5ekQ/lsA\nPAAgRSn1i1KqdSXxzwddCvUB9ANg11r/6Tg3ARS0DUqpnUqpO8+Q19QynwtBC7YqmgD4weEGyQLw\nFwBrVWUrc+2Z/OjpZT4Xl/teBCDQ+OJwFW0tk48OYGNSEZFgb6L8PWlY5vtRnMq51qNwASKCfmmR\nAgqUQRyAZMfn4wAalTlX9nN14rUCSNNaW7XW/9ZatwfQG8C1AMYDgNZ6idZ6MID6APYAmF5R5Frr\nLABLwAZgDOgeMM6laa3v01o3BK3pqWfj9z8HjgC4RmsdVublr7U+XlXZUIuDokqpxgCmgS6ScK11\nGICdcFrm5dPKAF0qTcoci8OpjfQp17ihHgU3IIJed/F2DNoZL09QCF9QSkUqpSIBvAT6ygF28e9U\nSrVRSvkDePEc0voWwBOOgcdAAK8BmKO1tiul4pVSHR1ugDxQaGyOwcMRSqkAx7EC0GdfGbNBX/FI\nON0tUEr9QykV6/iaDQqV/Rzyfia30icAXlNKxTnSi1JKXe/4XGHZHNelAWheg3TLEgCWKwOAh8N6\n7lDmfBqAWKWUFwBorW3g/XxVKRXoaBCegPNen56ZmtejcAEggl53WQS6FIzXSwD+A2ATgB2O1ybH\nMWitfwPwITiYtxfAWkc85rNIawbofvgDQJIjvUcd5+oD+B4cAP0LQIIjrAcoMskAMsFZIQ9WkcbP\nAFoAOK61TixzvBuAdUqpPAA/Afinrnyud0VW85ks6Q8caS9RSuWC9dL9DGUzrrtZKXVSKfV+Jenq\nKr6X5s0xtvCOI+1UUMz/LBNuGYBdAFKVUobb5lGwkUwCsArANwC+qCKtc6lH4QJFcfylkpNKzQAw\nHEC61rqj49g/AEwE0AbAFVrrLW7Ip+BmHNPhEgF4a63FUhOEi4AzWehfgNPZypII4EbQGhPqEEqp\nGxXnqocBmAzgZxFzQbh4qFLQtdarAGSVO7ZHa73XpbkSzhf3gf7Y/aA/uCoXSJ1BKfWrY1FO+dez\n5ztvgnAueJ45iHCpoLUeer7zcD64VMst1D1kUFQQBKGO4DILXSl10WxOJAiCcCGhtT6Xaa2l1NRC\nrzLR872vgStfL7/88nnPg5RPyiblq3uvmlCloCulvgU3JmqtlDqqlLpLKXWDUuoogJ4AFiqlfq1R\nDgRBEIRaoUqXi9Z6dCWnfnRBXgRBEIQaIIOi1SQ+Pv58Z8Gl1OXy1eWyAVK+S5kqV4rWKGKltKvi\nFgRBqKsopaCrOSgq89AFQagSx4OOBBdQ20avCLogCGdEetu1jysaSvGhC4Ig1BFE0AVBEOoIIuiC\nIAh1BBF0QRCEOoIIuiAIQh3BpYL+yspXXBm9IAiCUAaXCvqGlA2ujF4QhEucJk2a4O2330anTp0Q\nFBSEu+++G2lpaRg6dChCQkIwaNAgZGdnY/jw4fjoo49OubZTp0746aefzlPOXYNLBd0uTy8TBMGF\nKKUwf/58LFu2DH///Td++eUXDB06FG+88QbS09Nht9vx4Ycf4o477sCsWbNKr9u+fTtSUlIwfPjw\n85j72selC4tE0AXh0kBNqp1FMvrlc1/A9OijjyIqKgoAcOWVVyI6OhqdO3cGANx4441YtmwZnnnm\nGdx///04cOAAmjdvjpkzZ+LWW2+Fp2fdWlspgi4IQo2pjhDXFtHR0aWf/fz8Tvnu6+uL/Px8+Pj4\nYNSoUZg5cyZefvllzJkzB/PmzTsf2XUpLhV0m93myugFQRBOo7JtCm6//XaMHz8effr0gb+/P3r0\n6OHmnLke8aELgnBJ0KtXLyil8NRTT2H8+PHnOzsuQQRdEIQ6RdlNr5RSp3wfP348EhMTMW7cuPOR\nNZfj0v3Q+87oi1V3rnJJ/IIguAfH/tznOxu1wsyZMzF9+nT88ccf5zsrldZrTfZDFwtdEIRLgsLC\nQkyZMgX33Xff+c6KyzjTQ6JnKKXSlFKJZY6FK6V+V0rtVUotUUqFVna9CLogCBcCixcvRr169RAT\nE4MxY8ac7+y4jDNZ6F8AuKbcsWcB/K61bgVgmeN7hcgsF0EQLgSGDBmC/Px8/PDDD/DwqLtbWFVZ\nMq31KgBZ5Q5fD+Arx+evANxQ2fVioQuCILiP6jRV0VrrNMfnNADRlQUUQRcEQXAfNep7aA7RVjr8\nLYIuCILgPqqzUjRNKVVfa52qlIoBkF5ZwJQFKZiYOhEAEB8fj/j4+GplUhAEoa6SkJCAhISEWonr\njPPQlVJNACzQWnd0fH8TQKbWerJS6lkAoVrr0wZGlVK63ZR22PXQrlrJqCAI54e6NA/9QsLt89CV\nUt8CWAOgtVLqqFLqTgBvABiklNoLYIDje4WIy0UQhPPBxIkTcdttt53zuYudKl0uWuvRlZy6+mwi\nl2mLgiCcD8ou9z+Xcxc7slJUEIQ6R1UuIle5j6xWq0viPRdE0AVBuKiZPHkyYmNjERwcjDZt2mD5\n8uWnWOEWiwWjR4/GP/7xD1gsltOuX7duHXr37o2wsDBcdtllWLlyZem5L774Au3atUNwcDCaN2+O\nadOmlZ5LSEhAbGws3nzzTcTExOCuu+7CpEmTMGrUKNx+++0IDg5Ghw4dsHnzZtdWQBlE0AVBuGj5\n+++/MWXKFGzatAm5ublYsmQJmjRpUnq+uLgYN9xwA/z8/PDdd9/By8vrlOuTk5Nx7bXX4qWXXkJW\nVhbefvttjBw5EpmZmQD48IyFCxciNzcXX3zxBZ544gls3bq19Pq0tDRkZWXhyJEjmDZtGrTWWLBg\nAUaPHo2cnBxcf/31eOSRR9xSF4CLBd2mxYcuCJcEStXO6xwxmUwwm83YtWsXLBYL4uLi0KxZMwBA\nbm4uhgwZgpYtW2LGjBkV+s5nzZqFYcOG4ZpruMPJ1VdfjW7dumHhwoUAgGHDhqFp06YAgH79+mHw\n4MFYtcq5g6yHhwcmTZoELy8v+Pr6AuBj8K655hoopTBu3Dhs3779nMtVXcRCFwSh5mhdO69zpEWL\nFnj//fcxceJEREdHY/To0Th+/Di01li3bh127tyJZ555ptLrDx8+jO+//x5hYWGlr9WrVyM1NRUA\n8Ouvv6Jnz56IiIhAWFgYFi1aVGq9A0BUVBS8vb1PibPsI/D8/f1RXFwMu909WiiCLgjCRc3o0aOx\natUqHD58GEopPPPMM1BKYfDgwXj22WcxcOBApKdXvP4xLi4Ot912G7KyskpfeXl5mDBhAsxmM0aO\nHIkJEyYgPT0dWVlZGDZs2CmDquWt/vM9g8a1LheZtigIggvZu3cvli9fDrPZDB8fH/j6+sJkMpWe\nf/rppzFmzBgMHDjwFMvaYNy4cViwYAGWLFkCm82G4uJiJCQkIDk5GSUlJSgpKUFkZCQ8PDzw66+/\nYsmSJVXm53wvwBILXRCEixaz2YznnnsOUVFRiImJQUZGBl5//XUATmv5hRdewA033IBBgwYhKyvr\nlMfSxcbG4qeffsJrr72GevXqIS4uDu+88w601ggKCsKHH36IUaNGITw8HN9++y1GjBhxSvoVWejn\n02p36SPowt4Iw8lnTrokfkEQ3IMs/XcN8gg6QRAEoVJk2qIgCEIdQSx0QRCEOoIIuiAIQh1Bpi0K\ngiDUEcRCFwRBqCOIoAuCINQRXCroGlrmrwqCILgJlwo6IFa6IAiCuxBBFwShTjNu3DjExMQgODgY\nzZo1w6uvvlp67ujRo6W7KT711FOnXDd06FBs2bLF3dmtEdUWdKXUY0qpRKXUTqXUY5WFE0EXBMEd\nTJw4EZMmTTrt+HPPPYeDBw8iNzcXv/76K/773/9i8eLFAIDXX38dd955Jw4ePIgff/yx9OlCc+fO\nRfPmzdG1a1e3lqGmVEvQlVIdANwD4AoAnQFcq5RqXj6cSZlktaggCG6hsk2w2rdvX/rwCQDw9PRE\nVFQUAODQoUMYMGAAgoODccUVV5QK/+TJk/Haa6+5Jd+1SXUt9DYA1muti7XWNgArAdxUPpC3yVss\ndEEQzjsPPfQQAgIC0L59e7zwwgullneHDh2wZMkSZGdnY/PmzWjXrh1efPFFPPHEEwgODj7PuT53\nPKt53U4AryqlwgEUAxgOYMNpkXt4iqALwqXAxInnPR6tdaVW+tSpUzFlyhSsXLkSN998M7p27Yru\n3bvjueeew4MPPojp06fj4YcfhtlsRmJiIiZOnIgxY8YgOTkZo0aNwsMPP1ztfLmTam+fq5S6C8BD\nAAoA7AJg1lo/Uea89hnog8d6PAY/Lz/Ex8cjPj6+NvIsCIIbuZC3z7322muxevVqAHwgNIBTnu35\n888/n3bNgw8+CF9fX7z33nunHLfb7ejfvz8+/fRTfPnllwgPD8e//vUvdO3aFd9//z3atGlTq3k3\n6jUhIQEJCQmlxydNmlTt7XOra6FDaz0DwAxHxl4DcKR8mIDBAZjwyARE+EdUNxlBEIRK+eWXX0o/\nT5o0CUopvPTSS1VeY7FYEBFxuiZNmzYNvXr1Qrt27bBz5048+eST8PLyQseOHZGYmFjrgm5Q3tit\naGD3bKnJLJd6jvc4ADcCmH1a5MpDXC6CILgFrU9fyHjixAnMmTMHBQUFsNlsWLx4Mb7//vvTnjyU\nnp6OqVOnYqLD5dO0aVMsX74c+fn52LRpE5o3P23OxwVJTeah/08ptQvAzwAe0lrnlg9gUiYRdEEQ\n3EJlj3/75JNPEBsbi4iICLz44ouYOXMmrrjiilPCPf3003j55Zfh7+8PgFMdly9fjri4OFx//fUX\nzfRFlz6CLubtGGy6bxMaBDVwSRqCILieC9mHfjFz0T2CTlwugiAI7kMEXRAEoY7gUkE3eZjkIReC\nIAhuQix0QRCEOoIIuiAIQh3BtS4XmbYoCILgNlxuoctui4IgCO5BXC6CIAh1BJfPchFBFwRBcA+u\nd7nItEVBENzEnDlz0KZNG4SEhCAyMhI33XQTUlJSSs8//vjjCA8PR+/evZGcnFx6fPbs2XjssUof\nvHbRIC4XQRDqDH369MEff/yBnJwcHD58GP7+/njyyScBABs2bMCWLVuQlpaGvn374o033gAA5OTk\n4O233z7lWaMXKyLogiDUGRo1aoR69eoB4O6LJpMJMTExAPi4ub59+8LLywsDBgxAUlISAOD555/H\nhAkTEBgYeN7yXVvItEVBEOoUf/75J0JDQxEcHIwjR45g8uTJAPhs0VWrVqG4uBjLli1Dhw4dsGnT\nJuzduxe33nrrec517VDtB1ycDTJtURAuES6AR9AZ9O3bF9nZ2UhJScEdd9yBp59+Gh988AHat2+P\nkSNHomfPnmjbti3++9//YsSIEZgxYwY+/PBDzJs3D40aNcKUKVMQEhJS87KcB1y6fW6fz/vgjavf\nQN+4vi5JQxAE13Mhb5/7zTff4IEHHgAA9OvXDwsXLjzl/Pr163HNNdcgKyvrtGunTJmC5ORkjB07\nFrfeeiu2bduGN954A/n5+Xj99dddnveLbvtcmbYoCIIrGTt2LPLy8pCXl3eamAN83Jzx0IqypKWl\nYfr06XjppZewc+dOdOrUCSaTCd26dcOOHTvckXWXINMWBUGoM8yePRtHjx4FABw+fBjPP/88Ro4c\neVq4J598EpMmTYKvry+aNWuGjRs3oqCgAAkJCRfN4+YqQma5CIJQZ/jrr7/Qu3dvBAYGIj4+Hr16\n9cKbb755Spjly5cjNze39LmiV1xxBYYPH45GjRph5cqVePbZZ89H1msFl/rQn57QBYPum4xBzQe5\nJA1BEFzPhexDv5i5oHzoSqnnlFK7lFKJSqnZSimf8mHC86wyy0UQBMFNVEvQlVJNANwLoKvWuiMA\nE4DTJnKatBKXiyAIgpuo7jz0XAAWAP5KKRsAfwDJ5QN5QARdEATBXVTLQtdanwTwDoAjAFIAZGut\nl5YPZxJBFwRBcBvVstCVUs0BPA6gCYAcAN8rpcZqrb8pG27xxlQEf/gNtkRtQXx8POLj42uaX0EQ\nhDpFQkICEhISaiWuas1yUUrdAmCQ1voex/fbAPTUWj9cJoyecVsHBD87ESPbnT4PVBCEiwOZ5eIa\nLqRZLnsA9FRK+SmlFICrAfxVPlD/VcfE5SIIdQCllLxq+eUKquVy0VpvV0p9DWATADuALQCmlQ9n\nsmmZtigIFzlinV88VHu3Ra31mwDerCqMSWux0AVBENyEa5f+2yGCLgiC4CZcK+hioQuCILgNF1vo\nWnZbFARBcBMuFXRPi00sdEEQBDfhUkH3sthF0AVBENyESwVdaci0RUEQBDfhYkGXQVFBEAR3IYIu\nCIJQR3C5y0UEXRAEwT243EKXaYuCIAjuQSx0QRCEOoIIuiAIQh3B9S4XmbYoCILgFlwq6JBZLoIg\nCG7DtRY6ALsMigqCILgF1wq6XYugC4IguAnXulwA2G0i6IIgCO7A5bNctFjogiAIbsHFFrqGFgtd\nEATBLVRL0JVSrZVSW8u8cpRS/zwtnAbsMm1REATBLVTrIdFa678BdAEApZQHgGQAP5QPpwBoq7Um\n+RMEQRDOktpwuVwN4IDW+miFZ0XQBUEQ3EJtCPqtAGZXerbEUgtJCIIgCGeiWi4XA6WUN4DrADxT\n0fmJAFZ9twET90xEfHw84uPja5KcIAhCnSMhIQEJCQm1EpfSWlf/YqVGAHhQa31NBee0BvDMZ6MR\n3LI9nu/3fA2yKQiCcGmglILWWlXn2pq6XEYD+LaqAJl5qXhzzZs1TEYQBEE4E9UWdKVUADggOr+q\ncMVFeTBbzdVNRhAEQThLqu1D11oXAIg8UzhLUT7MJjO01lCqWr0IQRAE4Sxw+V4u5qICAIDFLrNd\nBEEQXInLBd1SlA8AKLYWuzopQRCESxrXW+jFFHTxowuCILgWlwu6NlvQPBMw20TQBUEQXIlLBV0r\nIMQMjNwtFrogCIKrca2ge3jA1wp4aLHQBUEQXI2LBV3BxwaY7DIoKgiC4GrcZ6GLy0UQBMGluHZQ\n1MMDPqWCLha6IAiCK3GpoNs9PBBaDNy3GTCXFLkyKUEQhEse11roJg+EmoHwIhF0QRAEV+NaH7rJ\nA/4WulxKxOUiCILgUlws6J7wswAmDZSUiKALgiC4EhcPiqoygl7o0qQEQRAudVw+bdHP8Yxoi1l8\n6IIgCK7E5dMWA0r40erYRlcQBEFwDS6f5WJY6LZicbkIgiC4Epdb6L6GoBeJy0UQBMGVuNiHboKv\njZ9t4kMXBEFwKTV5SHSoUup/SqndSqm/lFI9T4/dA752PkfUXiyCLgiC4EpqYqF/AGCR1rotgE4A\ndp8WwuQBb5tD0AsK8MrKV5BTnFODJAVBEITKqJagK6VCAFyptZ4BAFprq9b6dKX2MMHbzo+6xIyP\nN32MpKyk6udWEARBqJTqWuhNAZxQSn2hlNqilJqulPIvH8jb2xeB8AEAqOIiZBZlIr8kvwbZFQRB\nECrDswbXdQXwiNZ6o1LqfQDPAnipbKD/pGcAZk5zOfx3Mko6lSC/JB9JWUkI8w1DmF9YTfIuCIJw\n0ZOQkICEhIRaiUtprc/9IqXqA1irtW7q+N4XwLNa62vLhNH6iiuAxESguBgT7ozFW42PYe7Nc/HL\n3l/Qp1Ef3N/t/lophCAIQl1BKQWttarOtdVyuWitUwEcVUq1chy6GsCu02P3AGyct1iYnwUAyC/J\nR3ZxtrheBEEQapnqulwA4FEA3yilvAEcAHDnaSFMplJBLynk0v88cx5yzbki6IIgCLVMtQVda70d\nwBVVBjKZADunuRgrRvNL8pFjzkGBRfZ2EQRBqE1cvvTfwMsxfTG/JB85xTlioQuCINQyrhV0T2cH\nwNcK+Hv5I69EXC6CIAiuwMW7LZpKP3ragLiQOOSV5InLRRAEwQW4TdB97EDjkMbIKMyA1W4VC10Q\nBKGWcYvLRQPwsVLQk3OTAUAEXRAEoZZxj4VuMsHLBjQObYzkPBF0QRAEV+A+QbfTh55RmIFgn2AU\nlIgPXRAEoTZxj6B7esLHRpcLADQIaiAWuiAIQi3jWkH38gIAKE9PNPKvjyahTQAADYMaiqALgiDU\nMu6x0L28MKhRf0T6RwIAYoJiUGQtgl07VhtZLMCWLS7NiiAIQl3HPQuLvL0BiwW++w7Cy64Q5hsG\nX09fFFkcj6U7eRJYudKlWREEQajruEfQfXwAiwXq99/RyBaAEJ8QBHgFON0uJSW00gVBEIRq4x6X\ni48PYLUCJSUINQUi2CcYgd6Bpwp6SYlLsyIIglDXqcn2uWfGMSgKX18KusWCYJM/QnxDEOgdiBlb\nZ6DEVoK3Gt0FFBZyZ0YP17YxgiAIdRW3zHKBry9dKmYzQjwDSi30ZQeX4VDOISApCfjsM3G7CIIg\n1AD3+NANQZ82Dc1zPRF3IBNN8kzYlroNeeY8ICODFrq4XQRBEKqNa10uhqD7+1O08/PxTLt7EV4S\njth8D5htZvrRC3LFjy4IglBD3CPovr6lM1nqqUDADgTDB9BAQXEukJdHC14EXRAEodq4z4fuGBRF\nYSFgsSBIe6P7ST9cvi0dyOIDpJGb69LsCIIg1GWqbaErpQ4ByAVgA2DRWnc/PXZPzlrx9QXMZs5i\nyc0Fjh1D/UIrugW1hjXjAGDNZPjs7OpmRxAE4ZKnJi4XDSBea32y0hBeXhR0b2+guJjHsrKA5cvR\nob4JtviuWLl7J2DJ4zmx0AVBEKpNTV0uqsqzwcG00g0LHQAKCgCzGe19YjG6xY3wMttgz3MIeU5O\nDbMjCIJw6VITQdcAliqlNiml7q0wREAArXNvb+eAZ2EhxT0vD2r9eozY7wFbvsNCN3zpgiAIwjlT\nE5dLH631caVUFIDflVJ7tNarygaYOH8+hXzdOsQXFyMeAIqK6H7JywOSktAkW8Hu73jYhfjQBUG4\nxEhISEBCQkKtxFVtQddaH3e8n1BK/QCgO4BTBX3sWOCPP4DBg4G1a3mwuJi+ch8fICQE/hZAFzj2\ndMnJoQXv71/dbAmCIFxUxMfHIz4+vvT7pEmTqh1XtVwuSil/pVSQ43MAgMEAEk8LWL8+0LQpB0et\nVh4rLubnwkIgLw9+Fg17QT7sAIX+o494ztVoDZw4wc8//eSeNAVBEFxIdX3o0QBWKaW2AVgP4Bet\n9ZLTQjVoAIwaRWvcoKiIgl5cDJw8Cd8SDWtxAYq8QB96ZVsAaF27e71kZABffME4t24Fdu0CbDam\nIwiCcBFSLUHXWh/UWl/meHXQWr9eYUClOMulvKDbbBTt7Gz4WOzQJRYUegHasJgrEu7UVGDWrOpk\nt2Ly89l4HD3KfG7dCnzyCfD332d3/d69wL59tZcfQRCEGuLalaIeHtwT3dvbecxsdq4aNZth0oBn\niQX5XoA1+ySnNVYk6IWFFOHawohrzx6gZUv2DrKzablXxZ9/MuyePRR1QRCECwTXCrq/PxAe7rTQ\nPTzoajEsdMfcdN8SjXxvwF6QD0yZ4vRtl6VM+GphsTBdg/x8Wv179tD1cu21QHw8B2Z37ABWr644\nnu3bgSNHKP4yK0cQhAsI1wp6WBgwejQXFgHOFaOGP7ykBBYvE0wAsnwBe34eXTIVCaXZzGePni3l\n57QvXky3isGuXcDXX9MiX7oUWLYM8PNjGkeOUOzLsmIFty7IzmaYnBwRdEEQLijc83ggw+Xi60vB\n9vAotdbN/rTeswI9YMpyrBTNyyu99Ltd3+HjjR/TMp45k+6aZcuqHrwsLASmTj01TE4OxVtrCnNK\nCvNy+DBf899PAAAgAElEQVT9/NOnAyNGAAkJzv3ZLRb6yYuL+RDr48d5LDOTrpfVq89uENVmc259\nUPYYAGzbBqSlMU/ygA9BEGqAewTdz4/vxhYAhl/dwwNWPx/YAdgiI+CZ5/Brl/GVb0vdhh1pOyii\nhYWc2rhq1ekCWRbDD192KmJ+Pi3qxEROU0xL4/Hdu4GhQ53xHz1Kl09eHj8vWuTckmD/fuD334EN\nG/iUpQMHzm66486dwIIFpx776CP2Itav5/nVq5kvQRCEauLa/dANDB+6vz+F0tOTL6Vg9/OBTQE+\n9WLgYXf4zsu4VjILM1FgKaCQO6Y6AqBoGw1FeQyRzcmh+6RFCyA5mdeEhtI6NwQ9KQlo1owLoH79\nlW6YFStovffpwziMNA8coKhHRDA/JhPjt1iATp0qL/+JE6cOthYUUMwPHGA+jB5MSgrHCsoOIguC\nIJwl7rHQDR96UBAFy2TiYiNvb1hDgmA1ASoisjR4Tvqx0s+ZRZnIK8mjgBYWOoWxIsvYcIcUFNBV\nkptLyzg5me/LllFAMzIonkFBFGy7naLbsCEbnKwshsnL47lDh4Dvv+e74T8/eZKvJUucq2ArIzOT\ncRrumfR0vm/ezAbm+HG+YmJkKqQgCNXGvS6XkBCKrjE33csL1ugolHiboOvVKw1ekJWGH/f8iCM5\nR3Cy6CQfU2dY1CkpwDffnLrV7qpVdF3s2wf88gut5m+/ZdjCQr7n51OAk5KcA6xNmvD6+vWBTZso\nzBkZTvfL/v3AmjV00/z1FwXdanUOjGZm8r2yTcV27OA1R48Cx46xoQFYFg8P5mv1ajYmgYFAZCTw\n88+Vz7cvKGA58vOZt7rKsWPAwYPnOxeCcNHhHpeLYaGHh/Pdy4szYEpKENqwOZT/LnhF1wcAFHgC\nxTmZeG/de7jzsjuRWZQJH5MPkBfAa/fupZilpFCYW7emCBtundxcWtVaO2e1HDvm9L9v3eoU+jZt\nKNa9elGUIyMp1kVFtN43bQLWrQMaN2Y8+/cz73v38rzNRgvb2HDMKKdBYiLdTH/8QffKyZO0+lNT\ngcmTgTvuYPxdutBC/+EH4H//A6ZNc867L9PQ4Y8/WJbISJahRYvav1fnk7Vrgbg49npMJm4bIQjC\nWeNeH3pUlCNVTwqixQL/+o2AgFD4RsYAAHJ8gJLsk0jOTUFKXgoyCzMR5BME5AfyWsMyTU7mHHLD\nL19SwnRyc09dNLR7N1eC5uRQgCMiaDHbbEB0NPDAAxSRW2+lAL/6KsXUbme4vDznvPgDBxj2wAEg\nNpZhCgo4M2bMGArRvfcyPYCulaIiNhZZWcCWLZytYzYzDcPHHxrKuPbsYTni4hh26VLgn/90NoTH\njzvjKilhGUwm1947A+NpU6Ghrom/pIRjF/7+7AUZ6w7KrjIWBKFKzo+ge3vTxWC1ApddBhw6hICo\nBtAA8v0UrPk5iNh7FDn1knCyMJNP0SgM4bWHDvE9JYWClptLcff1pfhu3UoXCkDh27iRAllYSCs6\nKIhC6+kJNGrEPERG0vIODaV4NmjALQCMeeZJSRSa9HRaxRkZThGePJkP8ti1y2n5BwQ4RdvPj/Gk\np3OKYtOmHHw1mejT79GDs1y8vLjvzdSpQOfObCSUouBv2AB07EhXzfjxjHvRIrp8ylrwrmTfPgru\nAw/ULJ49e1gX/frxu9aM12SiC8xq5f06doyuNB8f4Mora5x9QbgUcO+gaAytcHh7U9zr1aOl27Mn\nAuvH4fMugKd/IIpzT6J1cgnyk3aj4+FidNntcIMAzkHRpCSn1bhoEUVy82a6MA4fZpjDh2npHT1K\nYQ0KooXfogWFo3NnhjMamtBQ+vmjopjn5GRnWs2aOcPUq8e4jEbpxAmKMuD0p2dkUKj27HH6/7Oy\ngOuvp3ANHEjXS1QU0KED0Ls3cN11wLx5tMotFor9nj0U9GXL2GAkJvK6pUspegsWUBSPH3fuaOkK\njh+nEJedK5+Tw7TN5srn0P/2G11Nv/3GXseyZRyXOHQI+PxzxrluHRuwDh2AsWPZOK5bR5fXpk38\nXpNVwoJwieAeC93Li+/R0Xz38QFuuokWtr8/4O2NsOBoHAsGvC3hKM47AV8rcDx1PxrrYOj8fOii\nIlrqhvvjyBEKg5+fc5fGgAC6W06cYKNhuGeOHeO5sDAej4ujyLZoQbEwXBohIZzpEhDAeNPS6KLJ\nzOR+Lzt3sgyFhfTdWyzsYXz9tXNxUFYWxSkwkOkaLpfGjSm4kyc7ffCRkWwg2rRhmllZzE9cHC3Y\nBQsYrnFjNirbtnGwt2FDNjarVvGabt246KpfPzaQxpYL5Tl6lOeNqZZGz2D48DPfw9RUli8tzelu\nmj4dGDSI5YmOdlrdBnl5HKzeu5f3ets2lrdhQ2DOHB5btAjo2pWv0aOZl/feYxm2bmXD+M03DPuv\nf1U+VVUQBDcJumPOeakl7OPDl93OP66PD8JD66NL/cvgAy/YDh1GY69IHMxKR3u/Rsi3FEEXF0EZ\noqeU09Wycyf92CUlFJvcXApJdDQFLDiYwtqgAT8HBFC4AwMp1k8+yfwB7PZffTXjWr2aabVq5XRt\nhIXRqm/cGGjenCITEgL8+CNnZfz8M3DNNfSx2+1Md98+5qVePYrb5s1M68EHgYcecvr1tabFm55O\nkXznHfY6HniADwixWFieyy5jeYuKaLE3aMDBRK2Z55ISzokfMoThQhyuqpQUYMYM4O672eBkZvLa\nPXs4KJyWBrRtW/k9/PtvlvHAAY4VdOtGq3ndOubZZqP4HjpEcT58mPeiTRvO6Onale8WC8MVFACX\nXw4sXMg9dHJzKfhr15a64ZCcTBfVs88Cs2cz/XbtXPMbFYQ6gHsE3cMDaN+ewqYU3Rm+vhShpk2B\n0FB4+gVgRLexyF69DD77gLFbrbDGAfWiTTBneUKbzRThtDQKqzFjJCWFQujtzbi1pmA0bUoBbNmS\nIhoUxAZFKVrAeXk8Vn4RT2gojxlPTWrcmK6CZs2A++6jWO7YwTzYbDwXGsrBV7udohoXR7Hz9aVI\nlpQw70uXUqhnzXJaua1bswyenhS4339nnr/4gj7+kBCK8NixLPvff9PS79ePoti1K7B8Od0zeXnM\n94EDzMe+ffS5//47BTIwkJZ2airw8MP8Pm8eZ9VYrcDTTzvHO6xW5unkSfZ6duzg9gctW7J+fvoJ\nuOoqptOgAePcsYONlr8/8N13bKjatQMGDOC9CA0FXnyRA8O//eYcG+jZk26o229nfTVuzJ7I9dez\n7rZuZW9q3z4RdEGoAvf40D08gJEjKU4eHhS6pk358vSk0Hp6Ak8+CZ+QCPhagZBCO1oU+uCKfYXo\nn2QDzCUUYIDW7vHj/JydTesuO9u5B4xhfQLOqX3163OJf48eFMF+/U6fZghQqOvVY+Pj4cE8BgTQ\nMvf15fmICL6HhbFMYWHOfdQPHqRl36ABX+3bM3z9+hS6++8H3niDFq7NRmFv0oT57N6d4fLzKe6v\nv05hO3iQu1A2acKpizt3si6UojVfVMRw7dsDw4axMVm/nnW0ZQsbjF69uPJ17172RAIDKZAhIZw2\n2bQpy2BsczBzJgeUFy4EvvyS9ZuZyfN33sn8/v47G7jgYMa1Zw/j/uUXWtkZGcAttwDvvw/0788d\nLWfOZP4ff5x5z8piHQQGci+dceNYnpYteU3fvoxj4ULm79NP2SDs3Xv6BmrnC63Za8nJYX0nJTFv\nGzfKA1MEt+I+C10pCprJRD9oy5YVhvMJDoefFfC129HQ7I0ws4ZHEQBridMvXL8+/9wmE/84Rlc+\nN5d+6YwMukS8vCgWAQFsNMLC2HA0bEhxMqYXlqVrV1qpEREUzQ4daM02a0aRDA2lRRkWRgvbWJQE\nUBSzsoDXXmOarVqxIWjViuk3a0Zr2M+Pg6IHD9IK79CBZQkJocvn4EEK3/z59Hc//TTw8cccVF67\n1jlLaOpU5sHDg+GLiljHAQEU97g4Wu/XXss8jBhBod2+nb7r7dtZF9u307petoyiNHo0GwFjNtCY\nMZwjHxLCvO7eTTfJuHH0a//8M/MWHU0RXruWvvCgIA58PvcccPPNjO8f/2DeP/iAs1cefxx46y02\ndO+/71yEZTKxESssZK/m00/pHuvUiQ1JSQnv8S23uOpXe3YUFrIhW7eODfG+fby/vr5sBI8epdtJ\nehaCG3CPoANOy9xkqvIh0B5BQfC3ecCzxIJ6hb4ILLFBWwFYbM5ZMrGxfG/UyOknt1ppoTdtSkFv\n0QJ49FG6NEJCKLzh4Uw7JoZWVEUYi4giIylITZrQHxwRQau6Xr1T8x8VxTDe3rx2/36WNTub6ZrN\njKtxYzYiABuh6Ghaul5epzcszZtTcC0WCmPfviyT2ey0wI8coZjfcw+t8/79Kd52O0Wvf38Omtav\nz/p57TVakeHhXPV6//2cc3/nncBjj1GQHnuM9fXTTxQhgL0Mw3rv04cN0po1dKP06QP897/O/B8+\nDEyY4Bws7dSJjcGIEWwIAgLYUOzfz4auXTv2mDp0cK4pOHmSbpe0NA4Q+/oCEyey7qxW9jQ6dKDl\nO2UKp6I6NnorbdzcRXIyXWMeHmwE585l3o0psg8/zHGBBQvYcBrjGQZa8+XOPAt1mhoJulLKBGAT\ngGNa6+uqDDxmDC1fT08KTGUEBiIQXjCVWBCR5wV/qx2q0AZl07Q4AefgXdOm9A0HBDifU1q/Prv0\nrVrR5dC6NcWlZ0/6a202NgSRkZVmAQAFLSmJafzxB/+Mw4ZVHLZrV+esnT//pEU8fjyFq3Fjljk8\nnPkbP56zcAyXSUU0b84027WjOyQxkcKelERXjFJ0OdSrR8H97DPgkUcofPXq0XXz+eccO5g2jWWJ\niQFuu40ilJZGq9/bmwOzL79MMW3ThnW7ZQut8Jtvphi9+KLTFTV7Nq3qDz7goO0bb7B+X3mFDduG\nDcC//808r1xJ0e3cmeIcEcFyZWWxHtaupeD98AOFbfhwNsQ+PnRX/PADBX3MGJ575RXWzVVXMa64\nOJY9IIBTPr/6iuX56y+WowrDoUZkZrJujx3jb6JLF/4Ohw9nmsY4jMXi3OBtwwae69bNOU6xbBnH\nQUaMcE0+hUuOmlrojwH4C0DQGUM2b873wMCqF8MEBSHMwx+wZSEcvvCwFANmG5QGhRig0BmuE09P\n5yIfu51C7ePjFP8WLejqiIlxXg+ceUFOly6Mr2FDNgZVrciMiXFuQWC1Uqz9/Chw3bvzmJcXrfKY\nGKd/vzJiY4Ebb2RZPvuM4pWYSDeV0QjExbGBMLYCOHmS6UZE0N2yejXjeftt4KmngP/8h7NcnnmG\n9+LECQpfu3Z8/+gj4N136cr4v/+jaK5YQVdMly68pmFDujjefpvbFqxYwSmGR48y3RUruDjqu+8o\nyF98wd7I7NnOlbr+/mwcZ86ki6KggPG3bOl0kwGst1ateN1XX9GK3bqVDVFwMMVy8GA2pNu20e1T\nUsJB3qQkuj6GDGHYjAzn1g4rVtDtUx3Wr2c+MjJY797edGEdOEBj4eOP+Vzali1pWHzzDWdAdezI\nBjYoiG7BqCj+trZs4XvnzuwBXXkl423Z0rkoTax34RyotqArpWIBDAPwKoAnz/pCLy9aVJURElK6\nk6KX2QIUFcMGQHsoqJgY/oliY/lq2ZJiHhlJN0RREQXc358WpeEiCQ11DqieLYGBFAMvLwpDVRi+\n/d69aZGaTHRhDBjgdOFofeq2AFVhMvFPbrcz/fh4uk/KNgTR0TwWGkoh2beP0xEtFgrI5ZezHt5+\nm8J6/DgbhYwMWq8zZ1J0UlMZtkcPumVatKDPu3lzCtVVV9GV8N57rI/rrqNAd+1KQd60iXE0akTx\nGT+eDVlkJN06iYksh7Ha08uL9ykhgeMFS5Zwdkv5fVt8fZm/+vXZIH33HV/XXENfu9lM18777/M3\nsWwZffpff03BXruW56Oj2SgUFjKP+/axTOV7aMaDTDp2dN6vxES6d06eZNlXr2bjrBRnHinF3oPR\ng3riCf7eli1j4/LSS+zlmEzsrURGsuGMiuKx/v3ZW/r6a9bZ22+z3B07siE1yh8czEbaZuPv2mrl\n9QsW8Dd38iTve+vWzJPJxPJW1EM5epSNW1XbPRsTDqxW5vsf/5AtGC4SamKhvwfgaQBV+E8qwNv7\nzIJuNjNcXh6gFMz+3vAyW/HM4c/w1vDh/NPcfjtFz7D4zWYKTJ8+9OX6+9P1AVBUz+RiKU9AQNWu\nobIYq17btHEukjH85gZKOee7ny0eHiyn8acu2yg1aMA0rrySVmCPHrT+/P2dorByJQVlzx763Bs2\nZH2FhtJFY0xb/PRT/nn37KH49ehB//rQoRQHY4fKTp0owD4+HLA1pi0aA6IRERSi227jzJjwcN6P\nyy8/vWxt2zrnrpetp4rw9aVYpaYyP089RcH99luKZkQEG5rp0ymqdjtw110M89NPrCdPT1r4HTuy\nnH5+fKWmUiSXLmV4Ly9ayU2asIxJSXQXhYbyHoSFsXdz9Cinn1qtbABHj2Z93Xsvf7sFBRT7f/6T\nA+CrV1PAH37YaXmvWEH3zYgR/C0XFDC+adN4TXExhXrPHjaAJhPv97Jl7Fnt3s1rjh3jfZ83j4ZO\n//5cuPX447w+JMRpSKxaxd9Cq1bOqcNbtrDhWrzYuSmdYRTl5nKcxGrl7zo9nXkzxrPsdt5Ho2cl\nnFeqJehKqWsBpGuttyql4s/p4pEj2Z2uDGPzp6Ag/tiDgmDx1tAlJdhQvB/odI9zPnnTphSGjh35\nA2zShOdatjzVoqjAR6m1xpGcI2gc6hSTp5Y8haEthmJgs4EU54YNz65MERFcKHT4MIWlRQv+uWsD\nIw+GC8kgPJziUFjIP6XhstCarpMNGygsjz5KX37//hSIK66gOAY6Njvr149xpaVRHGJiKEImE7/v\n3UvBaNTI6bIyFid16cJro6N5vSEQCxbwHm/ZUvFsJsD5kJPrrz+7emjVimkePMgy+Pjwvs6dy0Yh\nI4PTPOvXB55/ni6mxx6j66qoyDlQvmABhdAY9I6KooBdcQXrZ/58Ht+yhT2IdetoGOzcyTqJiaHA\ndu3KRv/vv1n+H3+ki2XfPv6GAwIo2IaIP/QQxfDbb9nImUx0WxkN6bp1dO1ZrZw5VNbVYgyebt7M\nBnX4cDYQ48ezt9SqFXDDDbx2+nSOPQQH0w114ADvUUwMwx05wv/N559T7Js3Z4O1YgV/x4WFzHNi\nIhutJ55g3Xl78zdlbPEcE8N7npPDtO65h98NoS9PYSFnU2VmcsxFqUv7QS7Gxn7n6jk4A9W10HsD\nuF4pNQyAL4BgpdTXWuvxZQNNnDix9HN8fDzi4+Ppz67K8jUK6OtbakXZvKyw5wMHrRnQXh5Qfn4U\n8Hr1aC20aeP0XYeE8NwZXBtbU7fi+m+vx7EnnQ/TSDiUAG+TNwXd25s/8LNFKafrxZiF42qUonAY\n+8MYx1q2pHiEhTm7zG3aUJj79eMf2yAqipb2X3/RmjWbnYuR/viDwn/PPRQ7Q+yDg+m7BpzT8Ro0\n4IDrDTdw0dCQIbw3tdVVN1bpxsY6Z/O0bElBfeghuouee44iftddFPSvvuKCrNWrKZqjRnFg9auv\nmP+yLoljx9gj7NWLQvf776yn3r1Zp50781xREcV14UKKodnM/Ozdyz9ocjKtVS8vivaBA2xI7rqL\njdw99/D6wkI2/idOsAdz5AiFfv16CvLUqRTWyy5jg9G2LRudjh353+jShcJ63XVMKz2d4QYOdI4T\nfPwxxzxMJoryggVs8Fq0YK8oIID3+MEHWT/GTKTCQv72Dx2iu+6++1gHw4Yx73PnssG55hrW4fff\ns7eSlMRZU+V7XFu30vpv1YrXvfce6234cKbRpg3TrisYD5Mvv/2G1pxIkJvL/9XOncCjjyJh40Yk\nJCTUStLVEnSt9f8B+D8AUEr1B/BUeTEHThV0Z4qeVXfPDLE3NtMKDkZhoB22vJPI9rKhqGE9+CvF\n7W6joyngxt7kRUUU+bKCVQ6b3QazzYy1R9ciJS8FJbYSeJtoKRzMPohNKZvOuh5Ow1h5Wnbw1R10\n7Xrq9yuvpA/3xAn+yQwr2VgsVdGgbKtWFAx/f+f+KoGBFPyy4StrjBs04L1t355CZzLV/ta+11zD\n9zZtnD2RzEy6OTw9aU1GRFDgJ0ygL/iee5jnBg3oRrrsMv7hjh+ntZmYSAEaMYJClpDAP9rll7N3\n9NVX3H/nyitpsUZHM83Bg9mY/PQTG5u77qKb6bLLnOM5N9/M3tGiRWwI69fn77RLF9athwcbm1mz\n6Ar58Ufmd+hQNtLffcc4k5P5m/LyojvmhRco1OPH8/Mvv7Dn0r07G52xY3n+xx+dPbFmzfg7ePJJ\n4M03WW6TiQ2Ece3//scG88knmde4OA7yLl/OsHfdRWt+7NhTXZhXX80yXncdxT0oiL3A48eZ56Qk\nlisykvetTx/egy++4O/ut984sN2qVdW9d2P8wl1bRp8rNht/E3/+yQb27rtpCDVuzF7hmjXMu7GF\nd/v2wNKliB8xgsaug0mTJlU7C7U1D/3sl8O1bFm15WtY6CEhpbNWrP4lsKd4ITqqMY6OuAqtAado\nhofzD+XjQ8vFGLirhG8Sv8GcnXMQ6R8JDY2UvBQ0CW2CnOIc5Jpzsfn4Zny66VOcKDyBF/q9cNr1\nO9N3okV4C/h6VrDKVCm6OGq5G3XOhIXxPSqKomT8Aby82OBV1C329OQf02rlgGKHDmf2bZfl8sud\nD+6uaAVubWKMRbRtS8EJCqJlPnUq975ZsIBz91u3pivD2Cdn8WL+Tv7zH/7hiospIgcOcODZbud3\nrVn+4cM5HnDbbXTF3H03xXbbNgprURFdiMaA7rhx/G17eTEOk4mNT3o6ffR2O+M0fv+zZ1P8v/+e\n4e+/n8cbN2Z9Gr2bo0fZMHz9NQf6R45kniZP5pTSrl1p2RvrC5Yvpxvp3XfZKAwYwEYkPp4NUMOG\nzgH6gQOZ7+uvp0vFYmHPYMMG5nn9ejYYAQFsKIODWafXXcdGISmJv6kPP3RuRa01x1JatKBh0bkz\nj2/dyp7MqFFM+4knaABlZdFd89tvdFsaU4ttNqd7SmvWl78/e1clJXytXcse5r59vHfljcWCAtZH\ndjYbq169zu73mZLC+3QuPczFi9nT69WL9/qzz9hod+7Me3PDDWxYPT1ZnpISutBqcf2E0i5amqyU\n0tWKOy+PP5oBAzh637AhMqMCodeswY3PNsZ/rvoP+jfpDwD4attXGNrkatQLjjnrCnl+2fN4Y/Ub\nqBdQD2arGT/d+hOubHwltqVuw7j545BVnIVCSyGiA6KxdPxSzNoxC8/2fRYAYNd2NHqvEV7s9yIe\n6FbDfcHPF0Z3r6r6slgujkGunByWZ9kyimZoKI/5+vJPYkyHPHbMubvmnDm0iOfOpTVtMvGcnx9/\nb5s38/o776T4HDxIKzYry7lvu8XC32hoKF0yZ6KggH/uhg0Zj7e304dqsVD09+xhI3T0KN1V+/ez\nVxERQfFTig3IH3/w/OHD7C188gkFuEsXpvXOO+yJzJxJ3/+zz/Je793L7n7v3mwE3nuPQvzZZxxM\nnTWLU1rbtqWIzZtHUX/xxVMfalJYyMHxgAAKc2QkMGkSrfepU1l/5WcsrVnDcYSsLOfU2rw85n3N\nGjY0nTuz57N/v3PramPfJquV9XXyJBtAk8l5n4OCGFdhIeNp2JD3xseH7pz169nbaNmSdZyUxHrt\n0YMNRkkJrzlyhO5Dm42TK6ZO5fFbb3U2AHY775fdzmM5OTQ8s7Mp2OvX033l58cwycnMn3GPevXi\nOEtKinPG0+rVpzUwSilorc9iOtzpXHiCXlxMwbnhBv5oGjaktXLgAP7RdCNuanMT5uyag/mj5qPb\n9G54qd9LuLHtjWcd/dj5Y/Hrvl9hsVswpPkQ3NT2JozpOAY/7P4BM7bNgIfyQEZhBvaf3I+rmlyF\n73Z9h0OPH0JcSBxWH1mNq766Cv2b9Mfvt/1+7mUTXMP+/fRp33svBfvGG2mxZWY696n39OSf1Waj\nK8RYF2G3UywXLKBgjhrltP7mzaPlunUr3Q/z5lFMbDZal+c6PqA13SDG+on69SnMf/5JS/mvv/jn\nPnaMYS0WvgcFUfRNJgrytm0UqE8/pV87KYni0ro1BVBrp/j17+8cT1qwgCIZHMxyAvxubGudk0ML\neNUqNjpeXhxLuPZaNgpt27IB1ZpxGIOaH3/MmUdjx7JncPnlvGbrVv6H33uPvYVRo/j/nj2bdfjY\nY7SqT5xgAzpwIF8GRUUUwDZtWBft2vE9PZ3lz8zkfdm6lXX65ZfMp/GKiWGjNWCAsw6MhmLpUoYp\n20Dn5LDO/P2ds8a2b2edBwbSrWTsHGtY1RERFPTWrVnuoiK6tBYtYl14ejK+PXs4YL1oEfN0+DBd\nLgMGcKZWGeqWoNts/AHffTe7kuHhtBAOHcKjUZtgsVvw6eZPkf5UOi6fdjme6v0U/tnjn5VG98rK\nV/Bw94cR7scBit6f98ZNbW9CYnoiovyjEOkfiZ///hld6neBp4cnRrQZgSDvIHyw/gN8k/gNhjQf\ngnZR7dAivAX+PPInGgU3wiebP0HSP5MQ4X8Og6aCa6nscXwHD/IPaLHQemzUyLmBmq8vxSQ01Plw\nkKAgHmvXjg2F8ZSrkydpmcbFcYBx0KDaybfdThEx9nm3250ziAzL/MgR+qOTklgOs9n5qMLjx1nu\nPn0oVHv2sEexfj1dKcZq5fr1aU3+978UnaFDGXdsLAX2iis4gG7shlpYSDHq1Yv+Xy8viqHVSkvY\nmJ+fne1cCd2gAa1Om42zeS67jNZu9+5sAG029kqMcZhNmxhPQgIt9U8+4XRas9k5OH3DDU4xfuAB\nuhFfeYXf8/LoUrrjDjbcxmZ32dlsdFq3ZlmnTaO13qMHG3NPT9aVtzc/G+6Y7Gw2QElJzGN0NO//\n9CvJalwAABf9SURBVOms73vuYTmNh7p4e7MRrl+fFve779INdvvtzN/cuexxvf8+e02PPMKeTHg4\n685mo+vMGKNwULcEXWu2eo8/TovAEPS9e/FqwyRM2zINR3KOYNdDu9BtWjc8fMXDeGvwWxVGVWQp\nQsgbIfhlzC8Y3HwwACDmnRhsvHcjYoNj8cG6D7Bg7wIsO7gMnh6eeGvQW3i85+MAgHl/zcPnWz/H\nawNfw+XTLse1ra7FttRtWDJuCf6z6j/4cc+PmBQ/CZ2jO2NW4ix8MeKLKotl13Z4KFn1d97JyqK/\nd9gwioMxQBUZSRHbtcv5YO9HHqGrolMn58NOyu/H4k6ys7kCNz6e+ezQgb799u35AthT2byZs5n+\n/NP5mMWsLOfsr549aXkHBfF4ly4UxEGDnP+3TZtYL4Y4GelrTYEDKHj+/hTAm26ii+bmm0/vueTm\nMi+G6HbvToG88krGGRHBBi0hgT2ssDCW88gRWv0lJSzD3LkU2K5dGVdYGO9L48ZM+9NP6baKiGC+\nEhO5WKuggH72hQsZV79+FFajgTFWS3fvzvoxGvTBgzlukZ3NcOWnDhtbTGdn0+33wAOsN+NhNl9+\nSTffkCEcFH/rLea3uJgCvnMn0yjn/qxbgg7wh/LSS/zBhoVxSl5mJj7PWo57FtwDAFg0ZhGGzR6G\nW9rfgjk3zzktCq01Vh1Zhf5f9sdHQz9CVACt8WHfDEPh84XwUB6Yv3s+bv7uZlzZ+EqsOrwKP9zy\nA0a0cc5ZN0Q4qygLYX5hpcetdit2pu/EoJmD0DS0KbYc34Kkx5IQFxJ3Wj4A4J0172BjysYK8ymc\nB3Jzq546qzX/eMazaS8miorYnW/Viv7ZHj2cPvsVK1im9u1ZRpuNA8LGrLCyU321dm5ZUZ6jR+mC\nMAbfly5lWob/ulkzCllaGq9fsoQi17Mn4121iu8HD1LcQkPpGoqJccYJ0Kru3Zvum6IiWsg5Oc61\nAXv2cHrmq6+ycbjzTuY5LIy9DGPW1ZdfOutg7lwK/YwZLL9SLEtBARv4EyfYuF91Fetm0yb2COrV\no5GZm8vrGjTg+eHD2Zi0bcvzw4fTBbRvHxs3Ly9+b9yY5Vm2jGnabGz4rrrK+ShMGLehrgn6Lbdw\nRL1//1MOL9y7ENd+ey2CvIMwMX4i/rXkX+gV2wtr7l5zSrjs4mz0/rw3hrUcho83fYx7u96LHWk7\nkFGYgWJrMfY+uhcAsDF5I7p/1h2fXfcZUvNTcVeXuxATVMnCiAoY9f0obDm+BQOaDkCobyiua3Ud\nrmzsfKDxxISJOJB1oNRnn/JkCvad3IfL6l9WGmbL8S1oFNwIUQFRZ0zvwMkD0NBoEd7irPMoCC6n\npITWdFwcrWCtKbaRkTwXG8tpoeUbjC1bKPw7d7IROn6criCrldfYbIzDuG7dOgr1ZY7/jzHw2KgR\nXSAffkhXzd9/013zwANMxxhjaNDA2UgvW+ZcALZrF2ftrF3LML//zp5JQADdTQMHMv758+nOUorj\nHLGxnHrZowddXrt3OxddeXmxscrKYtkXLWKD0bs3G73sbIaZP58DxmWs9Lon6O+/z25KuUeibU7Z\njG7Tu2FE6xGI8o/CvN3zEOAdgKNPHD0l3Jqja9BnRh94KA+M6zQOJwpOYO2xtcgz52FA0wFYctsS\nAEBqfipi3onB3kf2omVEJSsaq+BY7jGk5fMB0P2/ZOOz4d4NeHjRw5h89WSMmDMCA5sOROfozvhl\n3y+oF1APP+z+AYkPJiLCPwJR/lFo9VEr9Ivrh9EdR2P3id24qe1NWHtsLW5qexNS81PRIKgBNiZv\nRKuIVhg+ezjC/MKwYPSC6tSqILgPY7+Zs9m7yGDbNlqzvr6cCWK1UuBtNor38uUU6I4daUl7etJK\nNrZBGDmSjYq3t3Ol8+rVHEdo2JAWcqNG7PEbaxlsNgps2VWbJSX0u7drRwt62zZuEVHZ6m/jWbuz\nZnELiNhYxrFiBXsLv/3GnsLIkadfW1Jy2orZuifoU6aw62KsfnSQUZiBIbOGYGiLoUg4lAAvkxdW\nH1mNwucL4enhie2p2/HDnh8QFxKH2YmzsTN9J74d+S2un3M9Qn1D0Sm6E2KDYvHpdZ8CoEvl7TVv\n4+neT0Odyw+vArTWeGnFS5i7ay6O5R5DXEgcvExeSHwwEQDw1uq3MGHpBIztOBbrk9cjKSsJj3Z/\nFAv2LkB2cTY8PTwR4ReBo7lHERscixJbCQ5mHcRTvZ/ClI1TEBscC5vdhozCDOx7dB8i/SNrnGdB\nuKBJTeUWBcY4QI8etHoPHqRln5lJf/rSpRTyNWucAu3t7Rz3GDeOjYTFQis8Pd05wGu3U3RTUznd\nsqCA1xrTWJWip8DPz7npmtZ002RmcuA1KYlx9e3LdQLl2bOHLpezfMB53RP01avZrapkE68P1n2A\nf//xbwxpPgQrD6/EurvXoVFII3yw7gO8sfoN3NbpNoT6hmJCnwmw2q3wf9Uf17a6Fi/0ewE2uw29\nGvWqQckqJyUvBU3eb4L/jfofxv8wHo/3fBwT4yeWnvt+1/d4oNsDeH758+gZ2xOjvh+F1we+Di+T\nFzw9PHF3l7uRnJeMJqFNsPLQSjQIaoDBswbj3cHvIuFQAgY1H4T5u+djQ/IGhPmFYd3d63Aw+yCa\nhTVzSXkE4bxTXHz2C9UWL6awp6dTqOPjK+8h7NhBgfXxYW+gUyda8kFBFOjevSnshw7RLQLQa2Bs\nu22Mw2zfzganFp9IVRNBh9baJS9G7Rq+2fGNxkToRxY+ontM76H/PPyn1lrr+xfcrzERutPHnfS8\nv+aVhm/8XmP98oqXXZafshzPO6611vq3fb/ptPy0KsP+b9f/9MnCk1WGsdltp3zfnrpdv7PmHd1+\nSnv9wIIHNCZC/5X+ly6yFNUs44IgVMzBg1rv2OG25BzaWS3ddd8j6GqRKH8OIEb6R6JZWDPc/fPd\neP+a97E7Yzei/KOwI20H2kS2KQ3fuX5n9Ip1jVVenvqBHHQZ0uIMe6gDGNmuAp9aOcpPdewU3Qmd\nojshyj8Kd/x0B25qexMeX/w4NqVswuybZp9VuoIgnAPlXL8XMhemy+UMbEvdhi6fdsFHQz/CuE7j\n8PnWz7E0aSk2pmzE+E7j8cH6D1DwfwXw8eR8WLu2Q0HVKZ+zzW7DjrQdaBzaGM0/bI4+jfrAx9MH\n80bNq1Z8hZZC+Hs5dx80W83wNnnDru3Q0PD0OL3tzyjMQLhfONLy0+Dr6XvK1M6acCTnCIqtxWgV\nUfkmaxVhtppL77k70FrjROEJLN6/GFa7Fbd0uOWUOqyM5QeXIzU/FUuTlmLz8c2IC4lDuF84usV0\nw9bUrRjQdADeXP0m7upyV+m6iGJrMTIKMxAb7KadPIWzJr0gHf5e/vDz9EOxtRgaGuuPrUeIbwg2\nJG/ArR1uLV3YeDbUPR/6GUjOTUbse7GYM3IObulwC7KLs9HgnQbw8fTBb2N/wx0/3YHdD+92SdoX\nImarGWabGXHvxeHvR/5GdGD0KecrW9R0ouAEThSewPpj6/HIr4/gjs53YOXhlXh3yLt4cOGDuKPz\nHdiWtg2eHp6Ye/Pc0utyinPg6+mLph80xegOo7Fg7wJc3uByvD/kfew/uR+9GvXCxuSN6BHbA7nm\nXAT7nP0zUOzajr4z+uJY7jEsHLMQM7bOwG8HfsMNrW9Aw+CGGN1h9CkrdOfunIsvtn2B0R1G4//b\nO/egqK48j39+PLobmub98oU8fL+AJEpUMJoxPjIW6lbN7Lq6pRtjyi3HOJPETMWpLc1UrI0pU5NJ\nMslkM6biJpZxknGTmNUIGsRRJ/hEEHwEEFHkFYWG5tkNZ//oFvGBUQSV9nyquvr2uU3f35fv7V/f\nc8793bt8x3KOLz1OTFAMmcWZrN6zmlHhowj1DaXB3sAbT71xR//XmqYaAk2BN7T/+fCfWbV7FXUt\ndZi8TEyPm05lfSXh5nAsRgujw0ezbOwy2lQbPt4+ONoctKk2Ms5m0ORoYsm2JUyMmsiQ4CHtZzOV\n1pWSVZrFyLCRfHX6K3454pf8MeuPBJgC8DP4cebSGZodzcQFx2HwNPD0oKc5fek0C8Ys4Jsz35DU\nLwlPD0/+mvdXNs7ZSE5FDslRye0HMbYWGz5ePnh6dM+VCveV7CPQFMio8Nu/7K1SCluLDYvx6sXr\nurvgbnfRbtKL0jlbc5YAYwDPJD7D2L5j8fTwxNpkJcB0e4Vh1iYrFqOlvQ7F2mylj18fjF5GbC02\nGuwN5Fbk8uHRD/m24FuMXkZMXiaq6qvw9vRmeOhwaptriQ2KJaciB7PBzCN9HiExMhF7qx1/oz8l\n1hIa7A2cs54jwhxBbmUuf5j+ByZETXi4EnqzoxnTWhO7/m2X89rlwOSPJ9PS2sKBxQduKAR6WHg5\n/WX2FO9h27xthJvDaXI08d3Z71iybQnHlx6/5lx3R5uDyR9PJrs8G4OngQ2pG9h/fj8DAwbym52/\nYcGYBWz/YTv9/PtR3VjNkzFPUmYrI6lfEmv/vpaF8QvJr8qnxFrCpIGT2FW0i0i/SM7WnGVR/CLe\nPfQuL413nqHz0eyPALAYLBRVF1HVUMWKpBVkFGfw9OCnmbtlLm9Oe5Ntp7dxpOwI56znmNB/AhuO\nbWDpY0tJHZrKzoKdlNSWkFaYxtxhc+nv358gUxCvZr7KhAETSCtM4xcjf8GF2gvER8SzJW8L66au\nY0fBDhrtjeRV5TE9bjreHt6sSlnFx9kfk16U3p5M6u31xAbFMnvobGqaavgi/wu+LfiWOcPmEB0Y\nTU1TDTVNNVxuvExhdSE7F+wkLigOb09n4U2To4nUzakMCRnC7rO7Ka0tJcQ3hIXxC3lt72uYvEyM\nDB9JdWM166etJ3XoT9/Yw9pkpeByAfX2evpa+tLfvz+HSg/R6Gjk69NfEx0Yzbr960iITCC7PBsP\n8WBY6DAOlR4i1DcUs8FMo72RD2Z9wItpLzIoeBBhvmGU1pWyKmUVR8uOsnzc8k57rrkVuRRVFzFh\nwARe3/c6a3+2FpOXiQZ7A4PfGUy4OZwjzx1BEEqsJdjb7AwKHkRRdRGONgf/OP8PBocM5sjFI+RW\n5uJoc/BJziekDk3Fz+BHdWM1GcUZzBoyi5mDZpJflY/FYGHGoBnEBMUQ7BOMUor3D7/PrCGzrinc\nc7Q5cLQ52FeyjynRU/jy1Jd8lvcZhy8eZnHiYmKDYimuKWZT7iYu1l0kNiiW7PJsfjX2V1xqvIS3\npzcVtgqaW5uZFDWJ6MBonop7it999ztKa0vJPJfJ1Nip2FpsHCs7RqApEGuzlbigOE79eAofbx8G\nBw9m3qh5LEpYxMW6izQ5mhge5kzk4ear9yzeX7Ifk5eJtMI0ym3lGDwN1DbXEh0YjY+3D1EBUZTV\nlTEibATjB4zHx9vn4UroAAGvB7B30V7iI51VVm99/xZnLp3hvZ+/12PbfNBRSrEyfSUnKk+QGJnI\nu4fexeBpYGzfsfgb/dsTro+XD+es5wgwBrAhdQMl1hImRk1s/5zj5ccZETaC/Kp8QnxDyK3IZfOJ\nzSRGJvJ5/uc8+8izLNm2hO3/up3EPokEmgJ5J+sdDl48yOP9Huf3e3/Ppn/axLLty1g2dhmv7X0N\nf6M/Ji8TFqMFR5uDC7UXsLfaSRmYQlF1Eeet5xkRNoJ5o+Yxa8gs4oLjaLA34Gfwu0bjmUtn2P7D\ndqrqqyi3lZMclczChIWU1ZURZg5jxY4V+Bn8eGH8C9cUieVU5PDeofewtdj4PP9zZgyaweLExXiK\n84jVbDBzsPQgmecy8Tf6MzVmKnOHz+XTnE9pdjQTaAokyCeIQFMg8RHxtywEs7XYsLXY2HJiCx8e\n/ZBt87ZhMVoI9b3D2yDeBrYWG77evryd9TbVjdW8kvIKeZV5JEQmsK9kHxX1FczfOp/UoakopRAR\nAo2BfHX6K0J8Q0jql0ReVR5LH12Kv9GfrNIs9p7by/na8wiCxWjB2mTF6GVkWuw0Dlw4QJtqIz4i\nnvO15wn2Ceb0j6ept9cDtCd8s7eZ+Mh4jpYdJdwczoy4GZTZylgzeQ1phWl4iid+Bj8e6/sY6w+s\np7y+nMTIRMpt5WQUZ1BiLeHJmCcxeZnILM4kzBzGs4nPEuEXwTdnvmHrya2EmcNobWulVbUSbg7n\nuUeeY/6Y+Tf0CMtt5eRX5TM8dDgvpb9EQkQCZoOZIFMQRi8jR8uOklGcwZGLR/j1479mfP/xpAxM\n4U8H/0SEX4RzP/HwpLK+kpNVJxk/YHz7PRR6goduyAXgxZ0vsnry6nbz2lQbSqlu61L2Vuytdsb9\nZRzltnJ2zN9Bg72B4aHDmbNlDlNjpjImYgwN9gZCfENIjkq+rTHfm1F4uZDYoNgbju6UUtQ01VzT\nQ9qUs4mk/knEBcUhIpTbyjlYehCLwcKszbPI/Y9ccipyeGLgEz3es1JKkVuZy+jw0W41p3Irdvyw\ng6T+Se3juEopWlpbqKyvZGX6SuYMm8MHRz4gwBhAUr8k53BQyBCCTEE0OhrZWbCTlIEpPPrfj7Lm\niTVYjBYmR0+m2dFMelE6Y/uOJSEygVbVSnZ5NvER8e09l67OX9U117H15FaKqot4Pul53s56m9K6\nUgqrCwnxCWHd1HVcqL1AclQymecySY5Kvqsk29rWyskfT97REFJP8VAmdE3nnK0+i63FxuiI0fc7\nlJ/k+slYzYOLvsDcvUEndI1Go3ET7iah659bjUajcRN0QtdoNBo3oUsJXURMIpIlItkiki8i/9Xd\ngWk0Go3mzuhSQldKNQFTlFIJwBhgiogkd2tkDzh79uy53yH0KO6sz521gdb3MNPlIRelVINr0QB4\nApe7JaJegrvvVO6sz521gdb3MNPlhC4iHiKSDVQAGUqp/O4LS6PRaDR3yt0cobe5hlz6A5NEZHK3\nRaXRaDSaO6ZbzkMXkf8EGpVS6zu06ZPQNRqNpgt09Tz0Ll0PXURCAYdSqkZEfICngFe7IyCNRqPR\ndI2u3uCiD7BRRDxwDtt8opTa3X1haTQajeZO6bHSf41Go9HcW3qkUlREZojIKRH5QUR+2xPbuJeI\nSLGI5IjIMRE56GoLFpF0ETkjImkicuOdEB5QROQjEakQkdwObZ3qEZFXXF6eEpFp9yfq26cTfWtE\n5ILLw2MiMrPDul6jT0QGiEiGiOSJyAkRed7V7hb+3UKfu/h306LMbvOvqzcj7eyB85z0AiAa8Aay\ngeHdvZ17+QDOAsHXtb0BvOxa/i3w+v2O8w70pACJQO5P6QFGuDz0dnlaAHjcbw1d0LcaeOEm7+1V\n+oBIIMG17AecBoa7i3+30OcW/rli9nU9ewHfA8nd5V9PHKGPAwqUUsVKKTvwGTC7B7Zzr7l+kjcV\n2Oha3gjMubfhdB2l1N+B6uuaO9MzG9islLIrpYpx7lDj7kWcXaUTfXCjh9DL9CmlypVS2a5lG3AS\n6Ieb+HcLfeAG/sFNizKr6Sb/eiKh9wPOd3h9gauG9FYUsEtEDovIEldbhFKqwrVcAUTc/E97DZ3p\n6YvTwyv0Zj+Xi8hxEdnQoUvba/WJSDTOnkgWbuhfB33fu5rcwr+bFGXm0U3+9URCd8dZ1olKqURg\nJrBMRFI6rlTOvpHb6L4NPb1R6/tADJAAlAFv3uK9D7w+EfED/gasUErVdVznDv659H2BU58NN/JP\n3ViUOeW69V32rycSeikwoMPrAVz7C9PrUEqVuZ6rgP/F2eWpEJFIABHpA1Tevwi7hc70XO9nf1db\nr0IpValcAH/hare11+kTEW+cyfwTpdSXrma38a+Dvk+v6HMn/66glLIC/wc8Sjf51xMJ/TAwWESi\nRcQA/DPwdQ9s554gIr4iYnEtm4FpQC5OTQtdb1sIfHnzT+g1dKbna+BfRMQgIjHAYODgfYjvrnB9\nSa4wF6eH0Mv0iYgAG4B8pdRbHVa5hX+d6XMj/0KvDBfJ1aLMY3SXfz00izsT5+x0AfDK/Z5Vvkst\nMThnmbOBE1f0AMHALuAMkAYE3u9Y70DTZuAi0IJzvuPfb6UHWOXy8hQw/X7H3wV9zwD/A+QAx11f\nlojeqA/nGRFtrv3xmOsxw13860TfTDfybzRw1KUvB1jpau8W/3RhkUaj0bgJ+hZ0Go1G4ybohK7R\naDRugk7oGo1G4ybohK7RaDRugk7oGo1G4ybohK7RaDRugk7oGo1G4ybohK7RaDRuwv8DZCngblld\nnOcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f0a7d6610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(forest_loss, label='my', color='g')\n",
    "\n",
    "plt.plot(forest_sk_loss, label='sklearn', color='r')\n",
    "plt.plot(np.array(forest_sk_loss) * 1.03, label='+3%', color='r', alpha=0.5)\n",
    "plt.plot(np.array(forest_sk_loss) * 0.97, label='-3%', color='r', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"Log loss vs n_estimators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEKCAYAAAD6q1UVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f064f1890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(forest_f1, label='my', color='g')\n",
    "\n",
    "plt.plot(forest_sk_f1, label='sklearn', color='r')\n",
    "plt.plot(np.array(forest_sk_f1) * 1.03, label='+3%', color='r', alpha=0.5)\n",
    "plt.plot(np.array(forest_sk_f1) * 0.97, label='-3%', color='r', alpha=0.5)\n",
    "plt.legend(loc=4)\n",
    "plt.title(\"F1 score vs n_estimators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.90015910899\n",
      "precision =  0.836802001648\n",
      "recall =  0.880389379795\n",
      "f1 =  0.855529986291\n"
     ]
    }
   ],
   "source": [
    "forest_pred = forest_clf.predict(spam_X_test)\n",
    "\n",
    "print \"score: \", forest_clf.score(spam_X_test, spam_y_test)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(np.array((spam_y_test+1)/2, dtype=int),\n",
    "                                             np.array((forest_pred+1)/2, dtype=int))\n",
    "print \"precision = \", p.mean()\n",
    "print \"recall = \", r.mean()\n",
    "print \"f1 = \", f.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest_tree_pred = forest_tree.predict(spam_X_test)\n",
    "p, r, f, _ = precision_recall_fscore_support(spam_y_test, forest_tree_pred)\n",
    "\n",
    "#print \"score: \", tree.score(spam_X_test, spam_y_test)\n",
    "\n",
    "print \"precision = \", p.mean()\n",
    "print \"recall = \", r.mean()\n",
    "print \"f1 = \", f.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting gradient boosting with 300 estimators\n",
      "Fitting estimator number 1\n",
      "Score:  0.800930494854\n",
      "Loss: 6.87575037273\n",
      "diff: 6.87575037273\n",
      "MY MSE:  0.0881430506584\n",
      "SK MSE:  0.0881430506584\n",
      "Fitting estimator number 2\n",
      "Score:  0.800930494854\n",
      "Loss: 6.87575037273\n",
      "diff: 0.0\n",
      "MY MSE:  0.0865412314261\n",
      "SK MSE:  0.0865412314261\n",
      "Fitting estimator number 3\n",
      "Score:  0.891442267024\n",
      "Loss: 3.74948429524\n",
      "diff: -3.12626607749\n",
      "MY MSE:  0.0800755874712\n",
      "SK MSE:  0.0800755874712\n",
      "Fitting estimator number 4\n",
      "Score:  0.907232482729\n",
      "Loss: 3.20411091852\n",
      "diff: -0.545373376718\n",
      "MY MSE:  0.0773138921242\n",
      "SK MSE:  0.0773138921242\n",
      "Fitting estimator number 5\n",
      "Score:  0.902157056253\n",
      "Loss: 3.37939832747\n",
      "diff: 0.17528740895\n",
      "MY MSE:  0.0724988967992\n",
      "SK MSE:  0.0724988967992\n",
      "Fitting estimator number 6\n",
      "Score:  0.916960383477\n",
      "Loss: 2.86810557301\n",
      "diff: -0.511292754459\n",
      "MY MSE:  0.0673754597067\n",
      "SK MSE:  0.0673754597067\n",
      "Fitting estimator number 7\n",
      "Score:  0.916960383477\n",
      "Loss: 2.86810422025\n",
      "diff: -1.352765989e-06\n",
      "MY MSE:  0.0617608177298\n",
      "SK MSE:  0.0617608177298\n",
      "Fitting estimator number 8\n",
      "Score:  0.918229240096\n",
      "Loss: 2.82427732332\n",
      "diff: -0.0438268969273\n",
      "MY MSE:  0.0624658165611\n",
      "SK MSE:  0.0624658165611\n",
      "Fitting estimator number 9\n",
      "Score:  0.921330889609\n",
      "Loss: 2.71714867882\n",
      "diff: -0.107128644502\n",
      "MY MSE:  0.0593756801313\n",
      "SK MSE:  0.0593756801313\n",
      "Fitting estimator number 10\n",
      "Score:  0.927252220499\n",
      "Loss: 2.51263270434\n",
      "diff: -0.204515974478\n",
      "MY MSE:  0.0587435692895\n",
      "SK MSE:  0.0587435692895\n",
      "Fitting estimator number 11\n",
      "Score:  0.927252220499\n",
      "Loss: 2.51263180249\n",
      "diff: -9.01843993262e-07\n",
      "MY MSE:  0.0544444288818\n",
      "SK MSE:  0.0544444288818\n",
      "Fitting estimator number 12\n",
      "Score:  0.929085013393\n",
      "Loss: 2.44932858942\n",
      "diff: -0.0633032130715\n",
      "MY MSE:  0.0537703443981\n",
      "SK MSE:  0.0537703443981\n",
      "Fitting estimator number 13\n",
      "Score:  0.930353870013\n",
      "Loss: 2.40550338345\n",
      "diff: -0.0438252059698\n",
      "MY MSE:  0.0528591718364\n",
      "SK MSE:  0.0528591718364\n",
      "Fitting estimator number 14\n",
      "Score:  0.933737487664\n",
      "Loss: 2.28863680634\n",
      "diff: -0.116866577113\n",
      "MY MSE:  0.0507777520587\n",
      "SK MSE:  0.0507777520587\n",
      "Fitting estimator number 15\n",
      "Score:  0.935711264627\n",
      "Loss: 2.2204644015\n",
      "diff: -0.068172404838\n",
      "MY MSE:  0.0493078716765\n",
      "SK MSE:  0.0493078716765\n",
      "Fitting estimator number 16\n",
      "Score:  0.938530946003\n",
      "Loss: 2.12307526784\n",
      "diff: -0.0973891336642\n",
      "MY MSE:  0.0487160854646\n",
      "SK MSE:  0.0487160854646\n",
      "Fitting estimator number 17\n",
      "Score:  0.93895389821\n",
      "Loss: 2.10846701615\n",
      "diff: -0.0146082516826\n",
      "MY MSE:  0.045998172716\n",
      "SK MSE:  0.045998172716\n",
      "Fitting estimator number 18\n",
      "Score:  0.939517834485\n",
      "Loss: 2.08898945997\n",
      "diff: -0.0194775561796\n",
      "MY MSE:  0.0466724155268\n",
      "SK MSE:  0.0466724155268\n",
      "Fitting estimator number 19\n",
      "Score:  0.939376850416\n",
      "Loss: 2.09385853901\n",
      "diff: 0.00486907903604\n",
      "MY MSE:  0.0453497433583\n",
      "SK MSE:  0.0453497433583\n",
      "Fitting estimator number 20\n",
      "Score:  0.940504722966\n",
      "Loss: 2.05490331392\n",
      "diff: -0.0389552250898\n",
      "MY MSE:  0.0437085071333\n",
      "SK MSE:  0.0437085071333\n",
      "Fitting estimator number 21\n",
      "Score:  0.942055547723\n",
      "Loss: 2.00133949896\n",
      "diff: -0.0535638149639\n",
      "MY MSE:  0.0427943599424\n",
      "SK MSE:  0.0427943599424\n",
      "Fitting estimator number 22\n",
      "Score:  0.94473424503\n",
      "Loss: 1.90882012071\n",
      "diff: -0.0925193782452\n",
      "MY MSE:  0.0422800053305\n",
      "SK MSE:  0.0422800053305\n",
      "Fitting estimator number 23\n",
      "Score:  0.946989990131\n",
      "Loss: 1.83090944507\n",
      "diff: -0.0779106756406\n",
      "MY MSE:  0.0411736125414\n",
      "SK MSE:  0.0411736125414\n",
      "Fitting estimator number 24\n",
      "Score:  0.946708021994\n",
      "Loss: 1.84064805407\n",
      "diff: 0.00973860899407\n",
      "MY MSE:  0.0410514716765\n",
      "SK MSE:  0.0410514716765\n",
      "Fitting estimator number 25\n",
      "Score:  0.947835894544\n",
      "Loss: 1.80169260351\n",
      "diff: -0.0389554505508\n",
      "MY MSE:  0.0389920410559\n",
      "SK MSE:  0.0389920410559\n",
      "Fitting estimator number 26\n",
      "Score:  0.947976878613\n",
      "Loss: 1.79682329902\n",
      "diff: -0.00486930449704\n",
      "MY MSE:  0.0394097634701\n",
      "SK MSE:  0.0394097634701\n",
      "Fitting estimator number 27\n",
      "Score:  0.948681798957\n",
      "Loss: 1.77247632561\n",
      "diff: -0.0243469734072\n",
      "MY MSE:  0.0379094872028\n",
      "SK MSE:  0.0379094872028\n",
      "Fitting estimator number 28\n",
      "Score:  0.949245735232\n",
      "Loss: 1.7529986567\n",
      "diff: -0.0194776689101\n",
      "MY MSE:  0.0379719920269\n",
      "SK MSE:  0.0379719920269\n",
      "Fitting estimator number 29\n",
      "Score:  0.951078528126\n",
      "Loss: 1.68969600728\n",
      "diff: -0.063302649419\n",
      "MY MSE:  0.0377339062936\n",
      "SK MSE:  0.0377339062936\n",
      "Fitting estimator number 30\n",
      "Score:  0.950937544058\n",
      "Loss: 1.69456553724\n",
      "diff: 0.00486952995803\n",
      "MY MSE:  0.0361852557805\n",
      "SK MSE:  0.0361852557805\n",
      "Fitting estimator number 31\n",
      "Score:  0.953334273227\n",
      "Loss: 1.61178510618\n",
      "diff: -0.0827804310596\n",
      "MY MSE:  0.0361101491924\n",
      "SK MSE:  0.0361101491924\n",
      "Fitting estimator number 32\n",
      "Score:  0.953757225434\n",
      "Loss: 1.59717674177\n",
      "diff: -0.0146083644131\n",
      "MY MSE:  0.0358498473537\n",
      "SK MSE:  0.0358498473537\n",
      "Fitting estimator number 33\n",
      "Score:  0.95418017764\n",
      "Loss: 1.58256815189\n",
      "diff: -0.0146085898741\n",
      "MY MSE:  0.0351765269915\n",
      "SK MSE:  0.0351765269915\n",
      "Fitting estimator number 34\n",
      "Score:  0.955026082053\n",
      "Loss: 1.55335176126\n",
      "diff: -0.0292163906347\n",
      "MY MSE:  0.0348823857723\n",
      "SK MSE:  0.0348823857723\n",
      "Fitting estimator number 35\n",
      "Score:  0.955026082053\n",
      "Loss: 1.55335164853\n",
      "diff: -1.12730498936e-07\n",
      "MY MSE:  0.0340624530309\n",
      "SK MSE:  0.0340624530309\n",
      "Fitting estimator number 36\n",
      "Score:  0.956153954603\n",
      "Loss: 1.51439585979\n",
      "diff: -0.0389557887423\n",
      "MY MSE:  0.0343122684217\n",
      "SK MSE:  0.0343122684217\n",
      "Fitting estimator number 37\n",
      "Score:  0.956012970534\n",
      "Loss: 1.51926538974\n",
      "diff: 0.00486952995803\n",
      "MY MSE:  0.0337771889829\n",
      "SK MSE:  0.0337771889829\n",
      "Fitting estimator number 38\n",
      "Score:  0.957140843085\n",
      "Loss: 1.48030982646\n",
      "diff: -0.0389555632813\n",
      "MY MSE:  0.0331891947016\n",
      "SK MSE:  0.0331891947016\n",
      "Fitting estimator number 39\n",
      "Score:  0.956999859016\n",
      "Loss: 1.48517946915\n",
      "diff: 0.00486964268853\n",
      "MY MSE:  0.0325519823684\n",
      "SK MSE:  0.0325519823684\n",
      "Fitting estimator number 40\n",
      "Score:  0.95770477936\n",
      "Loss: 1.46083215755\n",
      "diff: -0.0243473115987\n",
      "MY MSE:  0.0328888585572\n",
      "SK MSE:  0.0328888585572\n",
      "Fitting estimator number 41\n",
      "Score:  0.958268715635\n",
      "Loss: 1.44135460137\n",
      "diff: -0.0194775561796\n",
      "MY MSE:  0.0319865546525\n",
      "SK MSE:  0.0319865546525\n",
      "Fitting estimator number 42\n",
      "Score:  0.958409699704\n",
      "Loss: 1.43648507141\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.030860063247\n",
      "SK MSE:  0.030860063247\n",
      "Fitting estimator number 43\n",
      "Score:  0.958127731566\n",
      "Loss: 1.44622435679\n",
      "diff: 0.00973928537707\n",
      "MY MSE:  0.0314968079833\n",
      "SK MSE:  0.0314968079833\n",
      "Fitting estimator number 44\n",
      "Score:  0.95883265191\n",
      "Loss: 1.421876707\n",
      "diff: -0.0243476497902\n",
      "MY MSE:  0.0307602096056\n",
      "SK MSE:  0.0307602096056\n",
      "Fitting estimator number 45\n",
      "Score:  0.958550683773\n",
      "Loss: 1.43161542873\n",
      "diff: 0.00973872172457\n",
      "MY MSE:  0.0306928991111\n",
      "SK MSE:  0.0306928991111\n",
      "Fitting estimator number 46\n",
      "Score:  0.95883265191\n",
      "Loss: 1.42187681973\n",
      "diff: -0.00973860899407\n",
      "MY MSE:  0.0301256436289\n",
      "SK MSE:  0.0301256436289\n",
      "Fitting estimator number 47\n",
      "Score:  0.958691667842\n",
      "Loss: 1.42674634969\n",
      "diff: 0.00486952995803\n",
      "MY MSE:  0.0299729460242\n",
      "SK MSE:  0.0299729460242\n",
      "Fitting estimator number 48\n",
      "Score:  0.959255604117\n",
      "Loss: 1.40726845532\n",
      "diff: -0.0194778943711\n",
      "MY MSE:  0.0298884815215\n",
      "SK MSE:  0.0298884815215\n",
      "Fitting estimator number 49\n",
      "Score:  0.960806428874\n",
      "Loss: 1.35370486582\n",
      "diff: -0.0535635895029\n",
      "MY MSE:  0.0294934768055\n",
      "SK MSE:  0.0294934768055\n",
      "Fitting estimator number 50\n",
      "Score:  0.959396588186\n",
      "Loss: 1.40239892536\n",
      "diff: 0.0486940595449\n",
      "MY MSE:  0.0288743813104\n",
      "SK MSE:  0.0288743813104\n",
      "Fitting estimator number 51\n",
      "Score:  0.961511349218\n",
      "Loss: 1.32935744149\n",
      "diff: -0.073041483874\n",
      "MY MSE:  0.0290472909794\n",
      "SK MSE:  0.0290472909794\n",
      "Fitting estimator number 52\n",
      "Score:  0.962639221768\n",
      "Loss: 1.29040176547\n",
      "diff: -0.0389556760118\n",
      "MY MSE:  0.0284807196973\n",
      "SK MSE:  0.0284807196973\n",
      "Fitting estimator number 53\n",
      "Score:  0.96235725363\n",
      "Loss: 1.3001404872\n",
      "diff: 0.00973872172457\n",
      "MY MSE:  0.0283850819149\n",
      "SK MSE:  0.0283850819149\n",
      "Fitting estimator number 54\n",
      "Score:  0.962639221768\n",
      "Loss: 1.29040142728\n",
      "diff: -0.00973905991607\n",
      "MY MSE:  0.0281998499744\n",
      "SK MSE:  0.0281998499744\n",
      "Fitting estimator number 55\n",
      "Score:  0.962498237699\n",
      "Loss: 1.29527084451\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0272563444461\n",
      "SK MSE:  0.0272563444461\n",
      "Fitting estimator number 56\n",
      "Score:  0.963344142112\n",
      "Loss: 1.26605422841\n",
      "diff: -0.0292166160957\n",
      "MY MSE:  0.0274415334614\n",
      "SK MSE:  0.0274415334614\n",
      "Fitting estimator number 57\n",
      "Score:  0.963344142112\n",
      "Loss: 1.26605422841\n",
      "diff: 0.0\n",
      "MY MSE:  0.027162825512\n",
      "SK MSE:  0.027162825512\n",
      "Fitting estimator number 58\n",
      "Score:  0.96362611025\n",
      "Loss: 1.25631528123\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.0271878420848\n",
      "SK MSE:  0.0271878420848\n",
      "Fitting estimator number 59\n",
      "Score:  0.964331030594\n",
      "Loss: 1.23196819509\n",
      "diff: -0.0243470861377\n",
      "MY MSE:  0.0268181322029\n",
      "SK MSE:  0.0268181322029\n",
      "Fitting estimator number 60\n",
      "Score:  0.964331030594\n",
      "Loss: 1.23196808236\n",
      "diff: -1.12730499158e-07\n",
      "MY MSE:  0.0266553659716\n",
      "SK MSE:  0.0266553659716\n",
      "Fitting estimator number 61\n",
      "Score:  0.965035950938\n",
      "Loss: 1.20762065803\n",
      "diff: -0.0243474243292\n",
      "MY MSE:  0.0258415000143\n",
      "SK MSE:  0.0258415000143\n",
      "Fitting estimator number 62\n",
      "Score:  0.965317919075\n",
      "Loss: 1.19788182358\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0259570322986\n",
      "SK MSE:  0.0259570322986\n",
      "Fitting estimator number 63\n",
      "Score:  0.965599887213\n",
      "Loss: 1.18814298912\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0256960801705\n",
      "SK MSE:  0.0256960801705\n",
      "Fitting estimator number 64\n",
      "Score:  0.96588185535\n",
      "Loss: 1.17840404194\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.025776362206\n",
      "SK MSE:  0.025776362206\n",
      "Fitting estimator number 65\n",
      "Score:  0.966304807557\n",
      "Loss: 1.16379590298\n",
      "diff: -0.0146081389521\n",
      "MY MSE:  0.0253571968694\n",
      "SK MSE:  0.0253571968694\n",
      "Fitting estimator number 66\n",
      "Score:  0.967009727901\n",
      "Loss: 1.13944836592\n",
      "diff: -0.0243475370597\n",
      "MY MSE:  0.0251929450829\n",
      "SK MSE:  0.0251929450829\n",
      "Fitting estimator number 67\n",
      "Score:  0.967291696038\n",
      "Loss: 1.12970953147\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0250038382149\n",
      "SK MSE:  0.0250038382149\n",
      "Fitting estimator number 68\n",
      "Score:  0.967432680107\n",
      "Loss: 1.12484022697\n",
      "diff: -0.00486930449704\n",
      "MY MSE:  0.0248205725666\n",
      "SK MSE:  0.0248205725666\n",
      "Fitting estimator number 69\n",
      "Score:  0.967291696038\n",
      "Loss: 1.1297096442\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0246162220975\n",
      "SK MSE:  0.0246162220975\n",
      "Fitting estimator number 70\n",
      "Score:  0.96827858452\n",
      "Loss: 1.09562361088\n",
      "diff: -0.0340860333232\n",
      "MY MSE:  0.0244695654432\n",
      "SK MSE:  0.0244695654432\n",
      "Fitting estimator number 71\n",
      "Score:  0.967996616382\n",
      "Loss: 1.1053623326\n",
      "diff: 0.00973872172457\n",
      "MY MSE:  0.0243142846069\n",
      "SK MSE:  0.0243142846069\n",
      "Fitting estimator number 72\n",
      "Score:  0.968419568589\n",
      "Loss: 1.09075396819\n",
      "diff: -0.0146083644131\n",
      "MY MSE:  0.0240386428863\n",
      "SK MSE:  0.0240386428863\n",
      "Fitting estimator number 73\n",
      "Score:  0.96827858452\n",
      "Loss: 1.09562338542\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.023803137333\n",
      "SK MSE:  0.023803137333\n",
      "Fitting estimator number 74\n",
      "Score:  0.968419568589\n",
      "Loss: 1.09075396819\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0238036133479\n",
      "SK MSE:  0.0238036133479\n",
      "Fitting estimator number 75\n",
      "Score:  0.968701536726\n",
      "Loss: 1.08101513373\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0235035368715\n",
      "SK MSE:  0.0235035368715\n",
      "Fitting estimator number 76\n",
      "Score:  0.969124488933\n",
      "Loss: 1.06640676932\n",
      "diff: -0.0146083644131\n",
      "MY MSE:  0.0234559376511\n",
      "SK MSE:  0.0234559376511\n",
      "Fitting estimator number 77\n",
      "Score:  0.96940645707\n",
      "Loss: 1.05666782213\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.0231923391908\n",
      "SK MSE:  0.0231923391908\n",
      "Fitting estimator number 78\n",
      "Score:  0.969547441139\n",
      "Loss: 1.05179840491\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0229679868668\n",
      "SK MSE:  0.0229679868668\n",
      "Fitting estimator number 79\n",
      "Score:  0.969829409277\n",
      "Loss: 1.04205957045\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0228059917109\n",
      "SK MSE:  0.0228059917109\n",
      "Fitting estimator number 80\n",
      "Score:  0.969688425208\n",
      "Loss: 1.04692887495\n",
      "diff: 0.00486930449704\n",
      "MY MSE:  0.0224877792692\n",
      "SK MSE:  0.0224877792692\n",
      "Fitting estimator number 81\n",
      "Score:  0.969970393346\n",
      "Loss: 1.03719004049\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0226844651616\n",
      "SK MSE:  0.0226844651616\n",
      "Fitting estimator number 82\n",
      "Score:  0.969829409277\n",
      "Loss: 1.04205945772\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0223418799198\n",
      "SK MSE:  0.0223418799198\n",
      "Fitting estimator number 83\n",
      "Score:  0.970111377414\n",
      "Loss: 1.03232062327\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0223575487102\n",
      "SK MSE:  0.0223575487102\n",
      "Fitting estimator number 84\n",
      "Score:  0.970111377414\n",
      "Loss: 1.03232062327\n",
      "diff: 0.0\n",
      "MY MSE:  0.0219459700914\n",
      "SK MSE:  0.0219459700914\n",
      "Fitting estimator number 85\n",
      "Score:  0.970252361483\n",
      "Loss: 1.02745120604\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0218548691801\n",
      "SK MSE:  0.0218548691801\n",
      "Fitting estimator number 86\n",
      "Score:  0.970252361483\n",
      "Loss: 1.02745120604\n",
      "diff: 0.0\n",
      "MY MSE:  0.0217332506956\n",
      "SK MSE:  0.0217332506956\n",
      "Fitting estimator number 87\n",
      "Score:  0.970393345552\n",
      "Loss: 1.02258190154\n",
      "diff: -0.00486930449704\n",
      "MY MSE:  0.0216680392954\n",
      "SK MSE:  0.0216680392954\n",
      "Fitting estimator number 88\n",
      "Score:  0.97067531369\n",
      "Loss: 1.01284306709\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0212215719702\n",
      "SK MSE:  0.0212215719702\n",
      "Fitting estimator number 89\n",
      "Score:  0.970393345552\n",
      "Loss: 1.02258201427\n",
      "diff: 0.00973894718557\n",
      "MY MSE:  0.0214125361234\n",
      "SK MSE:  0.0214125361234\n",
      "Fitting estimator number 90\n",
      "Score:  0.970816297758\n",
      "Loss: 1.00797353713\n",
      "diff: -0.0146084771436\n",
      "MY MSE:  0.0212501517535\n",
      "SK MSE:  0.0212501517535\n",
      "Fitting estimator number 91\n",
      "Score:  0.970957281827\n",
      "Loss: 1.00310434536\n",
      "diff: -0.00486919176654\n",
      "MY MSE:  0.0210583011708\n",
      "SK MSE:  0.0210583011708\n",
      "Fitting estimator number 92\n",
      "Score:  0.970957281827\n",
      "Loss: 1.00310434536\n",
      "diff: 0.0\n",
      "MY MSE:  0.020944002655\n",
      "SK MSE:  0.020944002655\n",
      "Fitting estimator number 93\n",
      "Score:  0.971521218102\n",
      "Loss: 0.983626563721\n",
      "diff: -0.0194777816406\n",
      "MY MSE:  0.0206677278346\n",
      "SK MSE:  0.0206677278346\n",
      "Fitting estimator number 94\n",
      "Score:  0.971521218102\n",
      "Loss: 0.983626676452\n",
      "diff: 1.12730499047e-07\n",
      "MY MSE:  0.0204869050152\n",
      "SK MSE:  0.0204869050152\n",
      "Fitting estimator number 95\n",
      "Score:  0.97180318624\n",
      "Loss: 0.973887841997\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0205416802125\n",
      "SK MSE:  0.0205416802125\n",
      "Fitting estimator number 96\n",
      "Score:  0.972649090653\n",
      "Loss: 0.944671338631\n",
      "diff: -0.0292165033652\n",
      "MY MSE:  0.02048363332\n",
      "SK MSE:  0.02048363332\n",
      "Fitting estimator number 97\n",
      "Score:  0.972790074722\n",
      "Loss: 0.939801808673\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.0202713740691\n",
      "SK MSE:  0.0202713740691\n",
      "Fitting estimator number 98\n",
      "Score:  0.972790074722\n",
      "Loss: 0.939801921404\n",
      "diff: 1.12730499047e-07\n",
      "MY MSE:  0.0200523714477\n",
      "SK MSE:  0.0200523714477\n",
      "Fitting estimator number 99\n",
      "Score:  0.973072042859\n",
      "Loss: 0.930062974218\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.0201354548032\n",
      "SK MSE:  0.0201354548032\n",
      "Fitting estimator number 100\n",
      "Score:  0.97293105879\n",
      "Loss: 0.934932504176\n",
      "diff: 0.00486952995803\n",
      "MY MSE:  0.0199008567733\n",
      "SK MSE:  0.0199008567733\n",
      "Fitting estimator number 101\n",
      "Score:  0.973494995066\n",
      "Loss: 0.915454497075\n",
      "diff: -0.0194780071016\n",
      "MY MSE:  0.0197117035623\n",
      "SK MSE:  0.0197117035623\n",
      "Fitting estimator number 102\n",
      "Score:  0.974340899478\n",
      "Loss: 0.886237993709\n",
      "diff: -0.0292165033652\n",
      "MY MSE:  0.0196224537382\n",
      "SK MSE:  0.0196224537382\n",
      "Fitting estimator number 103\n",
      "Score:  0.973635979134\n",
      "Loss: 0.910585192578\n",
      "diff: 0.0243471988682\n",
      "MY MSE:  0.0195546077963\n",
      "SK MSE:  0.0195546077963\n",
      "Fitting estimator number 104\n",
      "Score:  0.974340899478\n",
      "Loss: 0.886237880979\n",
      "diff: -0.0243473115987\n",
      "MY MSE:  0.0194664353093\n",
      "SK MSE:  0.0194664353093\n",
      "Fitting estimator number 105\n",
      "Score:  0.974481883547\n",
      "Loss: 0.88136823829\n",
      "diff: -0.00486964268853\n",
      "MY MSE:  0.019209823247\n",
      "SK MSE:  0.019209823247\n",
      "Fitting estimator number 106\n",
      "Score:  0.975045819822\n",
      "Loss: 0.86189056938\n",
      "diff: -0.0194776689101\n",
      "MY MSE:  0.0191234748296\n",
      "SK MSE:  0.0191234748296\n",
      "Fitting estimator number 107\n",
      "Score:  0.974904835754\n",
      "Loss: 0.866759986608\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0190234896615\n",
      "SK MSE:  0.0190234896615\n",
      "Fitting estimator number 108\n",
      "Score:  0.974904835754\n",
      "Loss: 0.866759986608\n",
      "diff: 0.0\n",
      "MY MSE:  0.0189517689521\n",
      "SK MSE:  0.0189517689521\n",
      "Fitting estimator number 109\n",
      "Score:  0.975186803891\n",
      "Loss: 0.857021039422\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.0188764898875\n",
      "SK MSE:  0.0188764898875\n",
      "Fitting estimator number 110\n",
      "Score:  0.975468772029\n",
      "Loss: 0.847282317698\n",
      "diff: -0.00973872172457\n",
      "MY MSE:  0.01868098545\n",
      "SK MSE:  0.01868098545\n",
      "Fitting estimator number 111\n",
      "Score:  0.975891724235\n",
      "Loss: 0.832673953285\n",
      "diff: -0.0146083644131\n",
      "MY MSE:  0.0186663868389\n",
      "SK MSE:  0.0186663868389\n",
      "Fitting estimator number 112\n",
      "Score:  0.975750740166\n",
      "Loss: 0.837543370512\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0184611784802\n",
      "SK MSE:  0.0184611784802\n",
      "Fitting estimator number 113\n",
      "Score:  0.976032708304\n",
      "Loss: 0.827804423327\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.0185018954067\n",
      "SK MSE:  0.0185018954067\n",
      "Fitting estimator number 114\n",
      "Score:  0.976173692373\n",
      "Loss: 0.822934893369\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.0183575222328\n",
      "SK MSE:  0.0183575222328\n",
      "Fitting estimator number 115\n",
      "Score:  0.976173692373\n",
      "Loss: 0.822934893369\n",
      "diff: 0.0\n",
      "MY MSE:  0.0182190729842\n",
      "SK MSE:  0.0182190729842\n",
      "Fitting estimator number 116\n",
      "Score:  0.976032708304\n",
      "Loss: 0.827804536057\n",
      "diff: 0.00486964268853\n",
      "MY MSE:  0.0181111494773\n",
      "SK MSE:  0.0181111494773\n",
      "Fitting estimator number 117\n",
      "Score:  0.976596644579\n",
      "Loss: 0.808326754416\n",
      "diff: -0.0194777816406\n",
      "MY MSE:  0.0180630260592\n",
      "SK MSE:  0.0180630260592\n",
      "Fitting estimator number 118\n",
      "Score:  0.976314676442\n",
      "Loss: 0.818065588871\n",
      "diff: 0.00973883445507\n",
      "MY MSE:  0.0180496628455\n",
      "SK MSE:  0.0180496628455\n",
      "Fitting estimator number 119\n",
      "Score:  0.976314676442\n",
      "Loss: 0.818065701602\n",
      "diff: 1.12730499158e-07\n",
      "MY MSE:  0.0179697298056\n",
      "SK MSE:  0.0179697298056\n",
      "Fitting estimator number 120\n",
      "Score:  0.976314676442\n",
      "Loss: 0.818065701602\n",
      "diff: 1.11022302463e-16\n",
      "MY MSE:  0.0177936245002\n",
      "SK MSE:  0.0177936245002\n",
      "Fitting estimator number 121\n",
      "Score:  0.976314676442\n",
      "Loss: 0.818065701602\n",
      "diff: -1.11022302463e-16\n",
      "MY MSE:  0.0177266725562\n",
      "SK MSE:  0.0177266725562\n",
      "Fitting estimator number 122\n",
      "Score:  0.97645566051\n",
      "Loss: 0.813196284374\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0175956764237\n",
      "SK MSE:  0.0175956764237\n",
      "Fitting estimator number 123\n",
      "Score:  0.976737628648\n",
      "Loss: 0.803457224458\n",
      "diff: -0.00973905991607\n",
      "MY MSE:  0.0174839027615\n",
      "SK MSE:  0.0174839027615\n",
      "Fitting estimator number 124\n",
      "Score:  0.977160580854\n",
      "Loss: 0.788848972776\n",
      "diff: -0.0146082516826\n",
      "MY MSE:  0.0174583061993\n",
      "SK MSE:  0.0174583061993\n",
      "Fitting estimator number 125\n",
      "Score:  0.977160580854\n",
      "Loss: 0.788848860045\n",
      "diff: -1.12730499158e-07\n",
      "MY MSE:  0.0173299772513\n",
      "SK MSE:  0.0173299772513\n",
      "Fitting estimator number 126\n",
      "Score:  0.977865501198\n",
      "Loss: 0.764501661177\n",
      "diff: -0.0243471988682\n",
      "MY MSE:  0.0172657250412\n",
      "SK MSE:  0.0172657250412\n",
      "Fitting estimator number 127\n",
      "Score:  0.97772451713\n",
      "Loss: 0.769371078405\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0171272104707\n",
      "SK MSE:  0.0171272104707\n",
      "Fitting estimator number 128\n",
      "Score:  0.97772451713\n",
      "Loss: 0.769370965674\n",
      "diff: -1.12730499158e-07\n",
      "MY MSE:  0.0170788031194\n",
      "SK MSE:  0.0170788031194\n",
      "Fitting estimator number 129\n",
      "Score:  0.978147469336\n",
      "Loss: 0.754762488531\n",
      "diff: -0.0146084771436\n",
      "MY MSE:  0.0169315501116\n",
      "SK MSE:  0.0169315501116\n",
      "Fitting estimator number 130\n",
      "Score:  0.978288453405\n",
      "Loss: 0.749893071303\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0168230233125\n",
      "SK MSE:  0.0168230233125\n",
      "Fitting estimator number 131\n",
      "Score:  0.978429437474\n",
      "Loss: 0.745023654075\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0169160505148\n",
      "SK MSE:  0.0169160505148\n",
      "Fitting estimator number 132\n",
      "Score:  0.978288453405\n",
      "Loss: 0.749893184033\n",
      "diff: 0.00486952995803\n",
      "MY MSE:  0.016811687177\n",
      "SK MSE:  0.016811687177\n",
      "Fitting estimator number 133\n",
      "Score:  0.978570421542\n",
      "Loss: 0.740154124117\n",
      "diff: -0.00973905991607\n",
      "MY MSE:  0.0166998351537\n",
      "SK MSE:  0.0166998351537\n",
      "Fitting estimator number 134\n",
      "Score:  0.97885238968\n",
      "Loss: 0.730415289662\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.016569850753\n",
      "SK MSE:  0.016569850753\n",
      "Fitting estimator number 135\n",
      "Score:  0.978570421542\n",
      "Loss: 0.740154124117\n",
      "diff: 0.00973883445507\n",
      "MY MSE:  0.0166534725668\n",
      "SK MSE:  0.0166534725668\n",
      "Fitting estimator number 136\n",
      "Score:  0.978570421542\n",
      "Loss: 0.740154124117\n",
      "diff: 0.0\n",
      "MY MSE:  0.01649182927\n",
      "SK MSE:  0.01649182927\n",
      "Fitting estimator number 137\n",
      "Score:  0.978570421542\n",
      "Loss: 0.740154124117\n",
      "diff: 0.0\n",
      "MY MSE:  0.016307335364\n",
      "SK MSE:  0.016307335364\n",
      "Fitting estimator number 138\n",
      "Score:  0.97885238968\n",
      "Loss: 0.730415289662\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0163183096285\n",
      "SK MSE:  0.0163183096285\n",
      "Fitting estimator number 139\n",
      "Score:  0.979275341886\n",
      "Loss: 0.71580703798\n",
      "diff: -0.0146082516826\n",
      "MY MSE:  0.0161397970862\n",
      "SK MSE:  0.0161397970862\n",
      "Fitting estimator number 140\n",
      "Score:  0.978711405611\n",
      "Loss: 0.73528470689\n",
      "diff: 0.0194776689101\n",
      "MY MSE:  0.0161316408416\n",
      "SK MSE:  0.0161316408416\n",
      "Fitting estimator number 141\n",
      "Score:  0.978993373749\n",
      "Loss: 0.725545872435\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0160303713633\n",
      "SK MSE:  0.0160303713633\n",
      "Fitting estimator number 142\n",
      "Score:  0.979275341886\n",
      "Loss: 0.71580703798\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0159243223713\n",
      "SK MSE:  0.0159243223713\n",
      "Fitting estimator number 143\n",
      "Score:  0.979557310024\n",
      "Loss: 0.706068090794\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.0158588468907\n",
      "SK MSE:  0.0158588468907\n",
      "Fitting estimator number 144\n",
      "Score:  0.979557310024\n",
      "Loss: 0.706068203525\n",
      "diff: 1.12730499158e-07\n",
      "MY MSE:  0.0158264609159\n",
      "SK MSE:  0.0158264609159\n",
      "Fitting estimator number 145\n",
      "Score:  0.979839278162\n",
      "Loss: 0.69632936907\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.015811132265\n",
      "SK MSE:  0.015811132265\n",
      "Fitting estimator number 146\n",
      "Score:  0.97998026223\n",
      "Loss: 0.691460064573\n",
      "diff: -0.00486930449704\n",
      "MY MSE:  0.015728375081\n",
      "SK MSE:  0.015728375081\n",
      "Fitting estimator number 147\n",
      "Score:  0.97998026223\n",
      "Loss: 0.691460064573\n",
      "diff: 0.0\n",
      "MY MSE:  0.0155444248655\n",
      "SK MSE:  0.0155444248655\n",
      "Fitting estimator number 148\n",
      "Score:  0.980403214437\n",
      "Loss: 0.67685181289\n",
      "diff: -0.0146082516826\n",
      "MY MSE:  0.0154876523919\n",
      "SK MSE:  0.0154876523919\n",
      "Fitting estimator number 149\n",
      "Score:  0.980403214437\n",
      "Loss: 0.67685181289\n",
      "diff: 0.0\n",
      "MY MSE:  0.0154605363652\n",
      "SK MSE:  0.0154605363652\n",
      "Fitting estimator number 150\n",
      "Score:  0.980826166643\n",
      "Loss: 0.662243561207\n",
      "diff: -0.0146082516826\n",
      "MY MSE:  0.0154308869727\n",
      "SK MSE:  0.0154308869727\n",
      "Fitting estimator number 151\n",
      "Score:  0.981108134781\n",
      "Loss: 0.652504614022\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.0152633965593\n",
      "SK MSE:  0.0152633965593\n",
      "Fitting estimator number 152\n",
      "Score:  0.981108134781\n",
      "Loss: 0.652504501291\n",
      "diff: -1.12730499047e-07\n",
      "MY MSE:  0.015264513964\n",
      "SK MSE:  0.015264513964\n",
      "Fitting estimator number 153\n",
      "Score:  0.98124911885\n",
      "Loss: 0.647635084064\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0152546816543\n",
      "SK MSE:  0.0152546816543\n",
      "Fitting estimator number 154\n",
      "Score:  0.98124911885\n",
      "Loss: 0.647634971333\n",
      "diff: -1.12730499158e-07\n",
      "MY MSE:  0.0151312886497\n",
      "SK MSE:  0.0151312886497\n",
      "Fitting estimator number 155\n",
      "Score:  0.981672071056\n",
      "Loss: 0.63302660692\n",
      "diff: -0.0146083644131\n",
      "MY MSE:  0.0151528475666\n",
      "SK MSE:  0.0151528475666\n",
      "Fitting estimator number 156\n",
      "Score:  0.981672071056\n",
      "Loss: 0.63302660692\n",
      "diff: 0.0\n",
      "MY MSE:  0.0149854831499\n",
      "SK MSE:  0.0149854831499\n",
      "Fitting estimator number 157\n",
      "Score:  0.981672071056\n",
      "Loss: 0.63302660692\n",
      "diff: 0.0\n",
      "MY MSE:  0.0148227862692\n",
      "SK MSE:  0.0148227862692\n",
      "Fitting estimator number 158\n",
      "Score:  0.981672071056\n",
      "Loss: 0.63302660692\n",
      "diff: 0.0\n",
      "MY MSE:  0.0149050645057\n",
      "SK MSE:  0.0149050645057\n",
      "Fitting estimator number 159\n",
      "Score:  0.981531086987\n",
      "Loss: 0.637896024148\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.014893336434\n",
      "SK MSE:  0.014893336434\n",
      "Fitting estimator number 160\n",
      "Score:  0.981531086987\n",
      "Loss: 0.637896024148\n",
      "diff: 0.0\n",
      "MY MSE:  0.0147359744569\n",
      "SK MSE:  0.0147359744569\n",
      "Fitting estimator number 161\n",
      "Score:  0.981531086987\n",
      "Loss: 0.637896024148\n",
      "diff: 0.0\n",
      "MY MSE:  0.0147406592131\n",
      "SK MSE:  0.0147406592131\n",
      "Fitting estimator number 162\n",
      "Score:  0.981813055125\n",
      "Loss: 0.628157189693\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.014591602151\n",
      "SK MSE:  0.014591602151\n",
      "Fitting estimator number 163\n",
      "Score:  0.981813055125\n",
      "Loss: 0.628157189693\n",
      "diff: 0.0\n",
      "MY MSE:  0.0145865775598\n",
      "SK MSE:  0.0145865775598\n",
      "Fitting estimator number 164\n",
      "Score:  0.981672071056\n",
      "Loss: 0.63302660692\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.014495203851\n",
      "SK MSE:  0.014495203851\n",
      "Fitting estimator number 165\n",
      "Score:  0.981672071056\n",
      "Loss: 0.63302660692\n",
      "diff: 0.0\n",
      "MY MSE:  0.014439177326\n",
      "SK MSE:  0.014439177326\n",
      "Fitting estimator number 166\n",
      "Score:  0.981954039194\n",
      "Loss: 0.623287772465\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0144441508443\n",
      "SK MSE:  0.0144441508443\n",
      "Fitting estimator number 167\n",
      "Score:  0.981954039194\n",
      "Loss: 0.623287772465\n",
      "diff: 0.0\n",
      "MY MSE:  0.0143526570135\n",
      "SK MSE:  0.0143526570135\n",
      "Fitting estimator number 168\n",
      "Score:  0.982095023262\n",
      "Loss: 0.618418355238\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.014196016163\n",
      "SK MSE:  0.014196016163\n",
      "Fitting estimator number 169\n",
      "Score:  0.982236007331\n",
      "Loss: 0.613548825279\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.0141527471613\n",
      "SK MSE:  0.0141527471613\n",
      "Fitting estimator number 170\n",
      "Score:  0.982236007331\n",
      "Loss: 0.613548825279\n",
      "diff: 0.0\n",
      "MY MSE:  0.0141555354265\n",
      "SK MSE:  0.0141555354265\n",
      "Fitting estimator number 171\n",
      "Score:  0.982658959538\n",
      "Loss: 0.598940348136\n",
      "diff: -0.0146084771436\n",
      "MY MSE:  0.0140790150877\n",
      "SK MSE:  0.0140790150877\n",
      "Fitting estimator number 172\n",
      "Score:  0.982799943606\n",
      "Loss: 0.594070930908\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0139215783081\n",
      "SK MSE:  0.0139215783081\n",
      "Fitting estimator number 173\n",
      "Score:  0.982517975469\n",
      "Loss: 0.603809878094\n",
      "diff: 0.00973894718557\n",
      "MY MSE:  0.013964727677\n",
      "SK MSE:  0.013964727677\n",
      "Fitting estimator number 174\n",
      "Score:  0.982517975469\n",
      "Loss: 0.603809878094\n",
      "diff: 0.0\n",
      "MY MSE:  0.013924729509\n",
      "SK MSE:  0.013924729509\n",
      "Fitting estimator number 175\n",
      "Score:  0.982799943606\n",
      "Loss: 0.594070930908\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.0138027332213\n",
      "SK MSE:  0.0138027332213\n",
      "Fitting estimator number 176\n",
      "Score:  0.983081911744\n",
      "Loss: 0.584332096453\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0137616312693\n",
      "SK MSE:  0.0137616312693\n",
      "Fitting estimator number 177\n",
      "Score:  0.982940927675\n",
      "Loss: 0.589201513681\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0137207216794\n",
      "SK MSE:  0.0137207216794\n",
      "Fitting estimator number 178\n",
      "Score:  0.982940927675\n",
      "Loss: 0.589201513681\n",
      "diff: 0.0\n",
      "MY MSE:  0.0137308953374\n",
      "SK MSE:  0.0137308953374\n",
      "Fitting estimator number 179\n",
      "Score:  0.983081911744\n",
      "Loss: 0.584332209184\n",
      "diff: -0.00486930449704\n",
      "MY MSE:  0.013616431405\n",
      "SK MSE:  0.013616431405\n",
      "Fitting estimator number 180\n",
      "Score:  0.983081911744\n",
      "Loss: 0.584332096453\n",
      "diff: -1.12730499047e-07\n",
      "MY MSE:  0.0135750613148\n",
      "SK MSE:  0.0135750613148\n",
      "Fitting estimator number 181\n",
      "Score:  0.982658959538\n",
      "Loss: 0.598940573597\n",
      "diff: 0.0146084771436\n",
      "MY MSE:  0.0135481279038\n",
      "SK MSE:  0.0135481279038\n",
      "Fitting estimator number 182\n",
      "Score:  0.982940927675\n",
      "Loss: 0.589201626411\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.0134136041271\n",
      "SK MSE:  0.0134136041271\n",
      "Fitting estimator number 183\n",
      "Score:  0.982799943606\n",
      "Loss: 0.594071156369\n",
      "diff: 0.00486952995803\n",
      "MY MSE:  0.0133515420925\n",
      "SK MSE:  0.0133515420925\n",
      "Fitting estimator number 184\n",
      "Score:  0.983363879882\n",
      "Loss: 0.574593374729\n",
      "diff: -0.0194777816406\n",
      "MY MSE:  0.0132470654047\n",
      "SK MSE:  0.0132470654047\n",
      "Fitting estimator number 185\n",
      "Score:  0.983222895813\n",
      "Loss: 0.579462791956\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0132181839389\n",
      "SK MSE:  0.0132181839389\n",
      "Fitting estimator number 186\n",
      "Score:  0.983786832088\n",
      "Loss: 0.559985123046\n",
      "diff: -0.0194776689101\n",
      "MY MSE:  0.0131551633132\n",
      "SK MSE:  0.0131551633132\n",
      "Fitting estimator number 187\n",
      "Score:  0.983786832088\n",
      "Loss: 0.559985010316\n",
      "diff: -1.12730499047e-07\n",
      "MY MSE:  0.013247842783\n",
      "SK MSE:  0.013247842783\n",
      "Fitting estimator number 188\n",
      "Score:  0.983645848019\n",
      "Loss: 0.564854540274\n",
      "diff: 0.00486952995803\n",
      "MY MSE:  0.013056867338\n",
      "SK MSE:  0.013056867338\n",
      "Fitting estimator number 189\n",
      "Score:  0.983786832088\n",
      "Loss: 0.559985123046\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0129226696127\n",
      "SK MSE:  0.0129226696127\n",
      "Fitting estimator number 190\n",
      "Score:  0.984209784294\n",
      "Loss: 0.545376871363\n",
      "diff: -0.0146082516826\n",
      "MY MSE:  0.0130338594174\n",
      "SK MSE:  0.0130338594174\n",
      "Fitting estimator number 191\n",
      "Score:  0.984209784294\n",
      "Loss: 0.545376871363\n",
      "diff: 0.0\n",
      "MY MSE:  0.0129499673515\n",
      "SK MSE:  0.0129499673515\n",
      "Fitting estimator number 192\n",
      "Score:  0.984350768363\n",
      "Loss: 0.540507454136\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0128818866198\n",
      "SK MSE:  0.0128818866198\n",
      "Fitting estimator number 193\n",
      "Score:  0.984350768363\n",
      "Loss: 0.540507454136\n",
      "diff: 0.0\n",
      "MY MSE:  0.0128408357836\n",
      "SK MSE:  0.0128408357836\n",
      "Fitting estimator number 194\n",
      "Score:  0.98477372057\n",
      "Loss: 0.525899089723\n",
      "diff: -0.0146083644131\n",
      "MY MSE:  0.0127539064727\n",
      "SK MSE:  0.0127539064727\n",
      "Fitting estimator number 195\n",
      "Score:  0.98477372057\n",
      "Loss: 0.525899089723\n",
      "diff: 0.0\n",
      "MY MSE:  0.0127114929354\n",
      "SK MSE:  0.0127114929354\n",
      "Fitting estimator number 196\n",
      "Score:  0.985196672776\n",
      "Loss: 0.51129083804\n",
      "diff: -0.0146082516826\n",
      "MY MSE:  0.0127260221348\n",
      "SK MSE:  0.0127260221348\n",
      "Fitting estimator number 197\n",
      "Score:  0.985055688707\n",
      "Loss: 0.516160255268\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0126536096711\n",
      "SK MSE:  0.0126536096711\n",
      "Fitting estimator number 198\n",
      "Score:  0.985055688707\n",
      "Loss: 0.516160255268\n",
      "diff: 0.0\n",
      "MY MSE:  0.0125147092494\n",
      "SK MSE:  0.0125147092494\n",
      "Fitting estimator number 199\n",
      "Score:  0.985196672776\n",
      "Loss: 0.51129083804\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.012552855359\n",
      "SK MSE:  0.012552855359\n",
      "Fitting estimator number 200\n",
      "Score:  0.985196672776\n",
      "Loss: 0.51129083804\n",
      "diff: 0.0\n",
      "MY MSE:  0.0125242056714\n",
      "SK MSE:  0.0125242056714\n",
      "Fitting estimator number 201\n",
      "Score:  0.985337656845\n",
      "Loss: 0.506421308082\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.012473666979\n",
      "SK MSE:  0.012473666979\n",
      "Fitting estimator number 202\n",
      "Score:  0.985478640914\n",
      "Loss: 0.501551778124\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.012304660288\n",
      "SK MSE:  0.012304660288\n",
      "Fitting estimator number 203\n",
      "Score:  0.985619624982\n",
      "Loss: 0.496682248166\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.0123898427093\n",
      "SK MSE:  0.0123898427093\n",
      "Fitting estimator number 204\n",
      "Score:  0.985478640914\n",
      "Loss: 0.501551778124\n",
      "diff: 0.00486952995803\n",
      "MY MSE:  0.0122623034785\n",
      "SK MSE:  0.0122623034785\n",
      "Fitting estimator number 205\n",
      "Score:  0.985478640914\n",
      "Loss: 0.501551778124\n",
      "diff: 0.0\n",
      "MY MSE:  0.0123014261373\n",
      "SK MSE:  0.0123014261373\n",
      "Fitting estimator number 206\n",
      "Score:  0.985337656845\n",
      "Loss: 0.506421195352\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0122197619552\n",
      "SK MSE:  0.0122197619552\n",
      "Fitting estimator number 207\n",
      "Score:  0.985337656845\n",
      "Loss: 0.506421195352\n",
      "diff: 0.0\n",
      "MY MSE:  0.0121620772629\n",
      "SK MSE:  0.0121620772629\n",
      "Fitting estimator number 208\n",
      "Score:  0.985478640914\n",
      "Loss: 0.501551665394\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.0121000747405\n",
      "SK MSE:  0.0121000747405\n",
      "Fitting estimator number 209\n",
      "Score:  0.985478640914\n",
      "Loss: 0.501551665394\n",
      "diff: 0.0\n",
      "MY MSE:  0.0121394495011\n",
      "SK MSE:  0.0121394495011\n",
      "Fitting estimator number 210\n",
      "Score:  0.985478640914\n",
      "Loss: 0.501551665394\n",
      "diff: 0.0\n",
      "MY MSE:  0.0119478293323\n",
      "SK MSE:  0.0119478293323\n",
      "Fitting estimator number 211\n",
      "Score:  0.985760609051\n",
      "Loss: 0.491812830939\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0119480692903\n",
      "SK MSE:  0.0119480692903\n",
      "Fitting estimator number 212\n",
      "Score:  0.985760609051\n",
      "Loss: 0.491812830939\n",
      "diff: 0.0\n",
      "MY MSE:  0.0119402970596\n",
      "SK MSE:  0.0119402970596\n",
      "Fitting estimator number 213\n",
      "Score:  0.985760609051\n",
      "Loss: 0.491812830939\n",
      "diff: 0.0\n",
      "MY MSE:  0.0118192309595\n",
      "SK MSE:  0.0118192309595\n",
      "Fitting estimator number 214\n",
      "Score:  0.98590159312\n",
      "Loss: 0.486943413711\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0118367150079\n",
      "SK MSE:  0.0118367150079\n",
      "Fitting estimator number 215\n",
      "Score:  0.98590159312\n",
      "Loss: 0.486943413711\n",
      "diff: 0.0\n",
      "MY MSE:  0.0117393275464\n",
      "SK MSE:  0.0117393275464\n",
      "Fitting estimator number 216\n",
      "Score:  0.986183561258\n",
      "Loss: 0.477204466525\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.0117828599211\n",
      "SK MSE:  0.0117828599211\n",
      "Fitting estimator number 217\n",
      "Score:  0.986042577189\n",
      "Loss: 0.482073996484\n",
      "diff: 0.00486952995803\n",
      "MY MSE:  0.0117307611718\n",
      "SK MSE:  0.0117307611718\n",
      "Fitting estimator number 218\n",
      "Score:  0.986042577189\n",
      "Loss: 0.482073996484\n",
      "diff: 0.0\n",
      "MY MSE:  0.0117053405155\n",
      "SK MSE:  0.0117053405155\n",
      "Fitting estimator number 219\n",
      "Score:  0.98590159312\n",
      "Loss: 0.486943413711\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.011570895569\n",
      "SK MSE:  0.011570895569\n",
      "Fitting estimator number 220\n",
      "Score:  0.98590159312\n",
      "Loss: 0.486943413711\n",
      "diff: 0.0\n",
      "MY MSE:  0.0115828641307\n",
      "SK MSE:  0.0115828641307\n",
      "Fitting estimator number 221\n",
      "Score:  0.986183561258\n",
      "Loss: 0.477204579256\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0115215050831\n",
      "SK MSE:  0.0115215050831\n",
      "Fitting estimator number 222\n",
      "Score:  0.986324545326\n",
      "Loss: 0.472335162028\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0115496587032\n",
      "SK MSE:  0.0115496587032\n",
      "Fitting estimator number 223\n",
      "Score:  0.986324545326\n",
      "Loss: 0.472335162028\n",
      "diff: 0.0\n",
      "MY MSE:  0.0114874756712\n",
      "SK MSE:  0.0114874756712\n",
      "Fitting estimator number 224\n",
      "Score:  0.986324545326\n",
      "Loss: 0.472335162028\n",
      "diff: 0.0\n",
      "MY MSE:  0.0114807183729\n",
      "SK MSE:  0.0114807183729\n",
      "Fitting estimator number 225\n",
      "Score:  0.986324545326\n",
      "Loss: 0.472335162028\n",
      "diff: 0.0\n",
      "MY MSE:  0.0113184746801\n",
      "SK MSE:  0.0113184746801\n",
      "Fitting estimator number 226\n",
      "Score:  0.986324545326\n",
      "Loss: 0.472335162028\n",
      "diff: 0.0\n",
      "MY MSE:  0.0113621656326\n",
      "SK MSE:  0.0113621656326\n",
      "Fitting estimator number 227\n",
      "Score:  0.986324545326\n",
      "Loss: 0.472335162028\n",
      "diff: 0.0\n",
      "MY MSE:  0.0113163081301\n",
      "SK MSE:  0.0113163081301\n",
      "Fitting estimator number 228\n",
      "Score:  0.986888481602\n",
      "Loss: 0.452857493118\n",
      "diff: -0.0194776689101\n",
      "MY MSE:  0.0113028421038\n",
      "SK MSE:  0.0113028421038\n",
      "Fitting estimator number 229\n",
      "Score:  0.986747497533\n",
      "Loss: 0.457726910346\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.011266515958\n",
      "SK MSE:  0.011266515958\n",
      "Fitting estimator number 230\n",
      "Score:  0.986747497533\n",
      "Loss: 0.457726910346\n",
      "diff: 0.0\n",
      "MY MSE:  0.0112462227906\n",
      "SK MSE:  0.0112462227906\n",
      "Fitting estimator number 231\n",
      "Score:  0.986888481602\n",
      "Loss: 0.452857493118\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0111533018263\n",
      "SK MSE:  0.0111533018263\n",
      "Fitting estimator number 232\n",
      "Score:  0.986888481602\n",
      "Loss: 0.452857493118\n",
      "diff: 0.0\n",
      "MY MSE:  0.0109221130486\n",
      "SK MSE:  0.0109221130486\n",
      "Fitting estimator number 233\n",
      "Score:  0.986888481602\n",
      "Loss: 0.452857493118\n",
      "diff: 0.0\n",
      "MY MSE:  0.0110485579158\n",
      "SK MSE:  0.0110485579158\n",
      "Fitting estimator number 234\n",
      "Score:  0.986888481602\n",
      "Loss: 0.452857493118\n",
      "diff: 0.0\n",
      "MY MSE:  0.010965855509\n",
      "SK MSE:  0.010965855509\n",
      "Fitting estimator number 235\n",
      "Score:  0.986747497533\n",
      "Loss: 0.457726910346\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0110398119345\n",
      "SK MSE:  0.0110398119345\n",
      "Fitting estimator number 236\n",
      "Score:  0.986888481602\n",
      "Loss: 0.452857493118\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0109596732338\n",
      "SK MSE:  0.0109596732338\n",
      "Fitting estimator number 237\n",
      "Score:  0.98702946567\n",
      "Loss: 0.447988075891\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0108627266187\n",
      "SK MSE:  0.0108627266187\n",
      "Fitting estimator number 238\n",
      "Score:  0.987311433808\n",
      "Loss: 0.438249015975\n",
      "diff: -0.00973905991607\n",
      "MY MSE:  0.0108954228984\n",
      "SK MSE:  0.0108954228984\n",
      "Fitting estimator number 239\n",
      "Score:  0.98702946567\n",
      "Loss: 0.447988075891\n",
      "diff: 0.00973905991607\n",
      "MY MSE:  0.0108541774903\n",
      "SK MSE:  0.0108541774903\n",
      "Fitting estimator number 240\n",
      "Score:  0.987311433808\n",
      "Loss: 0.438249015975\n",
      "diff: -0.00973905991607\n",
      "MY MSE:  0.0108523603014\n",
      "SK MSE:  0.0108523603014\n",
      "Fitting estimator number 241\n",
      "Score:  0.987170449739\n",
      "Loss: 0.443118658663\n",
      "diff: 0.00486964268853\n",
      "MY MSE:  0.0108044990124\n",
      "SK MSE:  0.0108044990124\n",
      "Fitting estimator number 242\n",
      "Score:  0.987311433808\n",
      "Loss: 0.438249128705\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.0106891022231\n",
      "SK MSE:  0.0106891022231\n",
      "Fitting estimator number 243\n",
      "Score:  0.987452417877\n",
      "Loss: 0.433379598747\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.0107069424368\n",
      "SK MSE:  0.0107069424368\n",
      "Fitting estimator number 244\n",
      "Score:  0.987452417877\n",
      "Loss: 0.433379598747\n",
      "diff: 0.0\n",
      "MY MSE:  0.0106814184904\n",
      "SK MSE:  0.0106814184904\n",
      "Fitting estimator number 245\n",
      "Score:  0.987311433808\n",
      "Loss: 0.438249015975\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0106540139229\n",
      "SK MSE:  0.0106540139229\n",
      "Fitting estimator number 246\n",
      "Score:  0.987311433808\n",
      "Loss: 0.438249015975\n",
      "diff: 0.0\n",
      "MY MSE:  0.0105470025531\n",
      "SK MSE:  0.0105470025531\n",
      "Fitting estimator number 247\n",
      "Score:  0.987311433808\n",
      "Loss: 0.438249015975\n",
      "diff: 0.0\n",
      "MY MSE:  0.0106279620379\n",
      "SK MSE:  0.0106279620379\n",
      "Fitting estimator number 248\n",
      "Score:  0.987311433808\n",
      "Loss: 0.438249015975\n",
      "diff: 0.0\n",
      "MY MSE:  0.0105428598664\n",
      "SK MSE:  0.0105428598664\n",
      "Fitting estimator number 249\n",
      "Score:  0.987311433808\n",
      "Loss: 0.438249128705\n",
      "diff: 1.12730499158e-07\n",
      "MY MSE:  0.0104238844793\n",
      "SK MSE:  0.0104238844793\n",
      "Fitting estimator number 250\n",
      "Score:  0.987452417877\n",
      "Loss: 0.433379598747\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.0104691359915\n",
      "SK MSE:  0.0104691359915\n",
      "Fitting estimator number 251\n",
      "Score:  0.987452417877\n",
      "Loss: 0.433379598747\n",
      "diff: 0.0\n",
      "MY MSE:  0.010379808073\n",
      "SK MSE:  0.010379808073\n",
      "Fitting estimator number 252\n",
      "Score:  0.987311433808\n",
      "Loss: 0.438249015975\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0103925893596\n",
      "SK MSE:  0.0103925893596\n",
      "Fitting estimator number 253\n",
      "Score:  0.987452417877\n",
      "Loss: 0.433379598747\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0103045665147\n",
      "SK MSE:  0.0103045665147\n",
      "Fitting estimator number 254\n",
      "Score:  0.987452417877\n",
      "Loss: 0.433379598747\n",
      "diff: 0.0\n",
      "MY MSE:  0.0103327984847\n",
      "SK MSE:  0.0103327984847\n",
      "Fitting estimator number 255\n",
      "Score:  0.987593401946\n",
      "Loss: 0.42851018152\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0102853218969\n",
      "SK MSE:  0.0102853218969\n",
      "Fitting estimator number 256\n",
      "Score:  0.987452417877\n",
      "Loss: 0.433379598747\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.0102500377882\n",
      "SK MSE:  0.0102500377882\n",
      "Fitting estimator number 257\n",
      "Score:  0.987452417877\n",
      "Loss: 0.433379598747\n",
      "diff: 0.0\n",
      "MY MSE:  0.0102556173366\n",
      "SK MSE:  0.0102556173366\n",
      "Fitting estimator number 258\n",
      "Score:  0.987452417877\n",
      "Loss: 0.433379598747\n",
      "diff: 0.0\n",
      "MY MSE:  0.0101656858097\n",
      "SK MSE:  0.0101656858097\n",
      "Fitting estimator number 259\n",
      "Score:  0.987593401946\n",
      "Loss: 0.42851018152\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0101908428974\n",
      "SK MSE:  0.0101908428974\n",
      "Fitting estimator number 260\n",
      "Score:  0.987593401946\n",
      "Loss: 0.42851018152\n",
      "diff: 0.0\n",
      "MY MSE:  0.0101192975327\n",
      "SK MSE:  0.0101192975327\n",
      "Fitting estimator number 261\n",
      "Score:  0.987734386014\n",
      "Loss: 0.423640651562\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.0100538202399\n",
      "SK MSE:  0.0100538202399\n",
      "Fitting estimator number 262\n",
      "Score:  0.987875370083\n",
      "Loss: 0.418771234334\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.0101032664993\n",
      "SK MSE:  0.0101032664993\n",
      "Fitting estimator number 263\n",
      "Score:  0.987875370083\n",
      "Loss: 0.418771234334\n",
      "diff: 0.0\n",
      "MY MSE:  0.0100270424171\n",
      "SK MSE:  0.0100270424171\n",
      "Fitting estimator number 264\n",
      "Score:  0.988016354152\n",
      "Loss: 0.413901817107\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.00994882343503\n",
      "SK MSE:  0.00994882343503\n",
      "Fitting estimator number 265\n",
      "Score:  0.988157338221\n",
      "Loss: 0.409032287148\n",
      "diff: -0.00486952995803\n",
      "MY MSE:  0.00999391011961\n",
      "SK MSE:  0.00999391011961\n",
      "Fitting estimator number 266\n",
      "Score:  0.988157338221\n",
      "Loss: 0.409032287148\n",
      "diff: 0.0\n",
      "MY MSE:  0.00987639990418\n",
      "SK MSE:  0.00987639990418\n",
      "Fitting estimator number 267\n",
      "Score:  0.988439306358\n",
      "Loss: 0.399293452693\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.00988420660698\n",
      "SK MSE:  0.00988420660698\n",
      "Fitting estimator number 268\n",
      "Score:  0.988580290427\n",
      "Loss: 0.394424035466\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.00975362272308\n",
      "SK MSE:  0.00975362272308\n",
      "Fitting estimator number 269\n",
      "Score:  0.988580290427\n",
      "Loss: 0.394424035466\n",
      "diff: 0.0\n",
      "MY MSE:  0.00978632344344\n",
      "SK MSE:  0.00978632344344\n",
      "Fitting estimator number 270\n",
      "Score:  0.988439306358\n",
      "Loss: 0.399293452693\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.00979090729366\n",
      "SK MSE:  0.00979090729366\n",
      "Fitting estimator number 271\n",
      "Score:  0.988439306358\n",
      "Loss: 0.399293452693\n",
      "diff: 0.0\n",
      "MY MSE:  0.00972820448872\n",
      "SK MSE:  0.00972820448872\n",
      "Fitting estimator number 272\n",
      "Score:  0.988721274496\n",
      "Loss: 0.389554618238\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.0096760691117\n",
      "SK MSE:  0.0096760691117\n",
      "Fitting estimator number 273\n",
      "Score:  0.988862258565\n",
      "Loss: 0.384685201011\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.00969166527521\n",
      "SK MSE:  0.00969166527521\n",
      "Fitting estimator number 274\n",
      "Score:  0.988721274496\n",
      "Loss: 0.389554618238\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.00960153426292\n",
      "SK MSE:  0.00960153426292\n",
      "Fitting estimator number 275\n",
      "Score:  0.988721274496\n",
      "Loss: 0.389554618238\n",
      "diff: 0.0\n",
      "MY MSE:  0.00943225994197\n",
      "SK MSE:  0.00943225994197\n",
      "Fitting estimator number 276\n",
      "Score:  0.988721274496\n",
      "Loss: 0.389554618238\n",
      "diff: 0.0\n",
      "MY MSE:  0.00949377550582\n",
      "SK MSE:  0.00949377550582\n",
      "Fitting estimator number 277\n",
      "Score:  0.988862258565\n",
      "Loss: 0.384685201011\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.00951727989062\n",
      "SK MSE:  0.00951727989062\n",
      "Fitting estimator number 278\n",
      "Score:  0.988862258565\n",
      "Loss: 0.384685201011\n",
      "diff: 0.0\n",
      "MY MSE:  0.00946903028183\n",
      "SK MSE:  0.00946903028183\n",
      "Fitting estimator number 279\n",
      "Score:  0.988721274496\n",
      "Loss: 0.389554618238\n",
      "diff: 0.00486941722754\n",
      "MY MSE:  0.00946213433539\n",
      "SK MSE:  0.00946213433539\n",
      "Fitting estimator number 280\n",
      "Score:  0.988862258565\n",
      "Loss: 0.384685201011\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.00938916391202\n",
      "SK MSE:  0.00938916391202\n",
      "Fitting estimator number 281\n",
      "Score:  0.988862258565\n",
      "Loss: 0.384685201011\n",
      "diff: 0.0\n",
      "MY MSE:  0.00939707929794\n",
      "SK MSE:  0.00939707929794\n",
      "Fitting estimator number 282\n",
      "Score:  0.988862258565\n",
      "Loss: 0.384685201011\n",
      "diff: 0.0\n",
      "MY MSE:  0.00937937093714\n",
      "SK MSE:  0.00937937093714\n",
      "Fitting estimator number 283\n",
      "Score:  0.988862258565\n",
      "Loss: 0.384685201011\n",
      "diff: 0.0\n",
      "MY MSE:  0.00931056139828\n",
      "SK MSE:  0.00931056139828\n",
      "Fitting estimator number 284\n",
      "Score:  0.988862258565\n",
      "Loss: 0.384685201011\n",
      "diff: 0.0\n",
      "MY MSE:  0.00933022678933\n",
      "SK MSE:  0.00933022678933\n",
      "Fitting estimator number 285\n",
      "Score:  0.988862258565\n",
      "Loss: 0.384685201011\n",
      "diff: 0.0\n",
      "MY MSE:  0.00925754110074\n",
      "SK MSE:  0.00925754110074\n",
      "Fitting estimator number 286\n",
      "Score:  0.988862258565\n",
      "Loss: 0.384685201011\n",
      "diff: 0.0\n",
      "MY MSE:  0.00927823022154\n",
      "SK MSE:  0.00927823022154\n",
      "Fitting estimator number 287\n",
      "Score:  0.988862258565\n",
      "Loss: 0.384685201011\n",
      "diff: 0.0\n",
      "MY MSE:  0.00925332407679\n",
      "SK MSE:  0.00925332407679\n",
      "Fitting estimator number 288\n",
      "Score:  0.989003242634\n",
      "Loss: 0.379815783783\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.00912257528274\n",
      "SK MSE:  0.00912257528274\n",
      "Fitting estimator number 289\n",
      "Score:  0.989003242634\n",
      "Loss: 0.379815783783\n",
      "diff: 0.0\n",
      "MY MSE:  0.00912781463852\n",
      "SK MSE:  0.00912781463852\n",
      "Fitting estimator number 290\n",
      "Score:  0.989003242634\n",
      "Loss: 0.379815783783\n",
      "diff: 0.0\n",
      "MY MSE:  0.00907997917273\n",
      "SK MSE:  0.00907997917273\n",
      "Fitting estimator number 291\n",
      "Score:  0.989003242634\n",
      "Loss: 0.379815783783\n",
      "diff: 0.0\n",
      "MY MSE:  0.00909265829937\n",
      "SK MSE:  0.00909265829937\n",
      "Fitting estimator number 292\n",
      "Score:  0.989144226702\n",
      "Loss: 0.374946366556\n",
      "diff: -0.00486941722754\n",
      "MY MSE:  0.00907268456683\n",
      "SK MSE:  0.00907268456683\n",
      "Fitting estimator number 293\n",
      "Score:  0.989144226702\n",
      "Loss: 0.374946366556\n",
      "diff: 0.0\n",
      "MY MSE:  0.00905569560569\n",
      "SK MSE:  0.00905569560569\n",
      "Fitting estimator number 294\n",
      "Score:  0.989144226702\n",
      "Loss: 0.374946366556\n",
      "diff: 0.0\n",
      "MY MSE:  0.00903786210148\n",
      "SK MSE:  0.00903786210148\n",
      "Fitting estimator number 295\n",
      "Score:  0.989144226702\n",
      "Loss: 0.374946366556\n",
      "diff: 0.0\n",
      "MY MSE:  0.00899024590527\n",
      "SK MSE:  0.00899024590527\n",
      "Fitting estimator number 296\n",
      "Score:  0.989144226702\n",
      "Loss: 0.374946366556\n",
      "diff: 0.0\n",
      "MY MSE:  0.00895398211217\n",
      "SK MSE:  0.00895398211217\n",
      "Fitting estimator number 297\n",
      "Score:  0.989144226702\n",
      "Loss: 0.374946366556\n",
      "diff: 0.0\n",
      "MY MSE:  0.00886520544621\n",
      "SK MSE:  0.00886520544621\n",
      "Fitting estimator number 298\n",
      "Score:  0.98942619484\n",
      "Loss: 0.365207532101\n",
      "diff: -0.00973883445507\n",
      "MY MSE:  0.00881719387997\n",
      "SK MSE:  0.00881719387997\n",
      "Fitting estimator number 299\n",
      "Score:  0.98942619484\n",
      "Loss: 0.36520741937\n",
      "diff: -1.12730499102e-07\n",
      "MY MSE:  0.00886711639992\n",
      "SK MSE:  0.00886711639992\n",
      "Fitting estimator number 300\n",
      "Score:  0.989708162978\n",
      "Loss: 0.355468472185\n",
      "diff: -0.00973894718557\n",
      "MY MSE:  0.00882228862747\n",
      "SK MSE:  0.00882228862747\n"
     ]
    }
   ],
   "source": [
    "grad_clf = GradientBoosting(n_estimators=300, max_depth=3, mu=0.01)\n",
    "grad_clf.fit(spam_X_train, spam_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98970816297758357"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clf.score(spam_X_train, spam_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.953758949881\n",
      "precision =  0.923807808776\n",
      "recall =  0.935031073661\n",
      "f1 =  0.970888374131\n",
      "Log loss =  1.59712487517\n"
     ]
    }
   ],
   "source": [
    "grad_pred = grad_clf.predict(spam_X_test)\n",
    "grad_pred = np.sign(grad_pred)\n",
    "\n",
    "print \"score: \", grad_clf.score(spam_X_test, spam_y_test)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support((spam_y_test+1)/2., (grad_pred+1)/2.)\n",
    "print \"precision = \", p.mean()\n",
    "print \"recall = \", r.mean()\n",
    "print \"f1 = \", f1_score(np.array((spam_y_test+1)/2, dtype=int),\n",
    "                        np.array((grad_pred+1)/2, dtype=int))\n",
    "\n",
    "print \"Log loss = \", log_loss((spam_y_test+1)/2., (grad_pred+1)/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.01, loss='deviance',\n",
       "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "              presort=False, random_state=1, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_tree = GradientBoostingClassifier(max_depth=3, n_estimators=300, learning_rate=0.01, random_state=1, presort=False)\n",
    "grad_tree.fit(spam_X_train, spam_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.947195704057\n",
      "precision =  0.916523674459\n",
      "recall =  0.920635115754\n",
      "f1 =  0.966851863412\n",
      "Log loss =  1.8238158079\n"
     ]
    }
   ],
   "source": [
    "grad_tree_pred = grad_tree.predict(spam_X_test)\n",
    "p, r, f, _ = precision_recall_fscore_support((spam_y_test+1)/2., (grad_tree_pred+1)/2.)\n",
    "\n",
    "print \"score: \", grad_tree.score(spam_X_test, spam_y_test)\n",
    "\n",
    "print \"precision = \", p.mean()\n",
    "print \"recall = \", r.mean()\n",
    "#print \"f1 = \", f.mean()\n",
    "print \"f1 = \", f1_score((spam_y_test+1)/2, (grad_tree_pred+1)/2)\n",
    "\n",
    "print \"Log loss = \", log_loss((spam_y_test+1)/2., (grad_tree_pred+1)/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = 0\n",
    "\n",
    "loss = []\n",
    "\n",
    "for b, e in zip(grad_clf.weigths, grad_clf.estimators):\n",
    "    s += b * e.predict(spam_X_test)    \n",
    "    p = np.sign(s)\n",
    "    loss.append(log_loss((spam_y_test+1)/2., (p+1)/2.))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss, label='my', color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = 0\n",
    "\n",
    "loss = []\n",
    "f1 = []\n",
    "\n",
    "for b, e in zip(grad_clf.weigths, grad_clf.estimators):\n",
    "    s += b * e.predict(spam_X_test)\n",
    "    #p = sigmoid(s)\n",
    "    p = np.sign(s)\n",
    "    loss.append(log_loss((spam_y_test+1)/2., (p+1)/2.))\n",
    "    f1.append(f1_score(spam_y_test, p))\n",
    "\n",
    "grad_tree = GradientBoostingClassifier(max_depth=3, n_estimators=grad_clf.n_estimators, learning_rate=0.01, random_state=1)\n",
    "grad_tree.fit(spam_X_train, spam_y_train)\n",
    "\n",
    "stage_pred = grad_tree.staged_predict(spam_X_test)\n",
    "sk_loss = map(lambda p: log_loss( (spam_y_test+1)/2., (p+1)/2. ), stage_pred)\n",
    "\n",
    "stage_pred = grad_tree.staged_predict(spam_X_test) # Generator\n",
    "sk_f1 = map(lambda p: f1_score(spam_y_test, p), stage_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEKCAYAAADdBdT9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f022e0610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss, label='my', color='g')\n",
    " \n",
    "plt.plot(sk_loss, label='sklearn', color='r')\n",
    "plt.plot(np.array(sk_loss) * 1.03, label='+3%', color='r', alpha=0.5)\n",
    "plt.plot(np.array(sk_loss) * 0.97, label='-3%', color='r', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"Log loss vs n_estimators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEKCAYAAAD6q1UVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBqTJXymlbkCa/JVS6gb0/4+rOnqQwPRiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f02261710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(f1, label='my', color='g')\n",
    "\n",
    "plt.plot(sk_f1, label='sklearn', color='r')\n",
    "plt.plot(np.array(sk_f1) * 1.03, label='+3%', color='r', alpha=0.5)\n",
    "plt.plot(np.array(sk_f1) * 0.97, label='-3%', color='r', alpha=0.5)\n",
    "plt.legend(loc=4)\n",
    "plt.title(\"F1 score vs n_estimators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting voting classifier\n",
      "Final F1 score:  0.971120716657\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier([forest_clf, grad_clf])\n",
    "voting_clf.fit(spam_X_train, spam_y_train)\n",
    "voting_pred = voting_clf.predict(spam_X_test)\n",
    "\n",
    "print \"Final F1 score: \", f1_score(np.array((spam_y_test+1)/2, dtype=int), np.array((voting_pred+1)/2, dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00018359  0.0015128 ]\n"
     ]
    }
   ],
   "source": [
    "print voting_clf.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest_file = open('forest.pkl', 'wb')\n",
    "pickle.dump(forest_clf, forest_file)\n",
    "forest_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_file = open('grad.pkl', 'wb')\n",
    "pickle.dump(grad_clf, grad_file)\n",
    "grad_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voting_file = open('voting.pkl', 'wb')\n",
    "pickle.dump(voting_clf, voting_file)\n",
    "voting_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97112071665726996"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_file = open('voting.pkl', 'rb')\n",
    "ololo_clf = pickle.load(voting_file)\n",
    "voting_file.close()\n",
    "\n",
    "ololo_pred = ololo_clf.predict(spam_X_test)\n",
    "f1_score(np.array((spam_y_test+1)/2, dtype=int), np.array((ololo_pred+1)/2, dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foo(node):\n",
    "    if node.is_leaf:\n",
    "        return Counter([])\n",
    "\n",
    "    left_scores = node.left.get_feature_scores()\n",
    "    right_scores = node.right.get_feature_scores()\n",
    "\n",
    "    #print left_scores, right_scores\n",
    "    \n",
    "    scores = left_scores + right_scores\n",
    "    scores[node.feature] += 1. - np.sum(1. - node.var_importance)\n",
    "\n",
    "    return scores\n",
    "\n",
    "def bar(forest):\n",
    "    feature_scores = Counter([])\n",
    "    for e, ef in zip(forest.estimators, forest.estimators_features):\n",
    "        scores = e.get_feature_scores()\n",
    "        for f, s in scores.items():\n",
    "            feature_scores[ef[f]] += s\n",
    "        #scores = Counter(map(lambda (f, s): (ef[f], s), scores.items()))\n",
    "        #feature_scores += scores\n",
    "    return feature_scores\n",
    "\n",
    "def baz(grad):\n",
    "    feature_scores = Counter([])\n",
    "    for w, e in zip(grad.weights, grad.estimators):\n",
    "        scores = e.get_feature_scores()\n",
    "        for k in scores.keys():\n",
    "            scores[k] *= w\n",
    "        feature_scores += scores\n",
    "    return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest_clf.estimators[0].root.__class__.get_feature_scores = foo\n",
    "forest_clf.__class__.get_feature_scores = bar\n",
    "grad_clf.__class__.get_feature_scores = baz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " 'depth',\n",
       " 'feature',\n",
       " 'fit',\n",
       " 'get_answer',\n",
       " 'get_feature_scores',\n",
       " 'get_impurity_change',\n",
       " 'get_predicate',\n",
       " 'impurity',\n",
       " 'is_leaf',\n",
       " 'left',\n",
       " 'max_depth',\n",
       " 'predict',\n",
       " 'right',\n",
       " 'stopping_criteria',\n",
       " 'threshold',\n",
       " 'var_importance']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(forest_clf.estimators[0].root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grad_clf.weigths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voting_clf.estimators[1].weights = grad_clf.weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
